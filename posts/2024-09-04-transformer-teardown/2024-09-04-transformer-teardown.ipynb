{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7628c47-f092-445a-a823-5d419668693f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "source": [
    "# Transformer Teardown: DistilBERT\n",
    "\n",
    "> Tracing an inference from raw data to prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5ba7e-2021-4bdd-8d1a-4777fdb9e87e",
   "metadata": {},
   "source": [
    "<img src=\"banner-circuit-trail.png\" class=\"stickshift-figure\" width=\"800\">\n",
    "\n",
    "I loved taking things apart as a kid. Especially discarded electronics. I used to keep a pile of the circuit boards I scavenged in my closet. If I stacked them together the right way, I was convinced I could build my own C-3PO.\n",
    "\n",
    "As an adult, I still like taking things apart. Methodically dissecting, cataloging, and rebuilding helps me wrap my brain around new technology. Especially the hardcore stuff like LLMs and the Transformers that power them.\n",
    "\n",
    "While there are a million papers, blogs, and tutorials written on Transformers, I still find it challenging to map the abstract ideas from the research literature into concrete, actionable steps you can experiment with. My engineer's brain wants to \"see the code\" behind high level concepts like embeddings, residuals, and multi-head self-attention. Yes, it's easy to find open source Transformer implementations, but they're often overloaded with configuration settings to the point that the main ideas are completely obscured.\n",
    "\n",
    "The goal of this post is to give you a stronger sense of the Transformer machinery powering the AI revolution. We'll dissect Hugging Face's default text-classification model, lay all the pieces on the table, and then trace a single inference through the stack from raw data to final prediction. We'll illustrate the main ideas from the Transformer literature with minimal, straightforward, working Python code. You may be surprised by how few steps are required!\n",
    "\n",
    "All of the code for this post is available in [GitHub](https://github.com/stickshift/stickshift.github.io/blob/main/posts/2024-09-04-transformer-teardown/2024-09-04-transformer-teardown.ipynb) but it's a lot easier to [read with nbviewer](https://nbviewer.org/github/stickshift/stickshift.github.io/blob/main/posts/2024-09-04-transformer-teardown/2024-09-04-transformer-teardown.ipynb)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f5496-2f6a-46b5-bd9b-f874e68ac4f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [
     "skip-publish"
    ]
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b52cfc-e02e-4b70-aecf-c0962354fee8",
   "metadata": {
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pytest import approx\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu, softmax\n",
    "import transformers\n",
    "\n",
    "from stickshift import default_arg, take\n",
    "from stickshift.models import distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f68eaaf-2117-48d2-8623-f90d5a035452",
   "metadata": {
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure gpu\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae509a70-4087-47a7-b587-3aaef0ab9b71",
   "metadata": {},
   "source": [
    "# Text Classification with DistilBERT\n",
    "\n",
    "If you've worked with Transformers at all, I'm sure you're familiar with Hugging Face's collection of Python libraries as well as their endless repository of models and datasets. Throughout the post, we'll be working with Hugging Face's default text classification model DistilBERT. DistilBERT is a smaller, faster, lighter-weight version of the original BERT model that's easier to experiment with. We'll use the pre-trained model parameters from Hugging Face, but we'll implement the model's logic step-by-step in Jupyter using a slightly modified version of the actual DistilBERT PyTorch implementation from Hugging Face's `transformers` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a45264-ad56-4656-bdc2-bcc55836ebd4",
   "metadata": {},
   "source": [
    "Before we get into the implementation, let's start by running the entire process end-to-end using Hugging Face's high level `pipeline` API. The following cells create a complete text classification pipeline and then apply it to the sentence \"I love ice cream\". As you might expect, the model classifies the sentence as overwhelmingly positive. Over the rest of the post, we'll break this prediction down and recreate it one step at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14efe2e6-d089-4abb-8726-5fe4edb8d451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# Create off-the-shelf text classification transformer\n",
    "transformer = transformers.pipeline(\"text-classification\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78cc7338-7f84-45fa-a659-5f3bd8cc2169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998118281364441}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer(\"I love ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd36e702-ffcf-4069-bd13-64f6c18ee8ee",
   "metadata": {
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "# Load model config and pre-trained parameters\n",
    "config = distilbert.config(transformer.model)\n",
    "parameters = transformer.model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc2126e9-eec2-458a-9215-b6fb927d7e83",
   "metadata": {
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "def load_state(*args, layer=None):\n",
    "    # Defaults\n",
    "    layer = default_arg(layer, lambda: 0)\n",
    "\n",
    "    for module, key in take(2, args):\n",
    "        match key:\n",
    "            case \"value_embeddings\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[\"distilbert.embeddings.word_embeddings.weight\"],\n",
    "                })\n",
    "            case \"position_embeddings\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[\"distilbert.embeddings.position_embeddings.weight\"],\n",
    "                })\n",
    "            case \"normalize_embeddings\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[\"distilbert.embeddings.LayerNorm.weight\"], \n",
    "                    \"bias\": parameters[\"distilbert.embeddings.LayerNorm.bias\"],\n",
    "                })\n",
    "            case \"queries\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"distilbert.transformer.layer.{layer}.attention.q_lin.weight\"],\n",
    "                    \"bias\": parameters[f\"distilbert.transformer.layer.{layer}.attention.q_lin.bias\"],\n",
    "                })\n",
    "            case \"keys\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"distilbert.transformer.layer.{layer}.attention.k_lin.weight\"],\n",
    "                    \"bias\": parameters[f\"distilbert.transformer.layer.{layer}.attention.k_lin.bias\"],\n",
    "                })\n",
    "            case \"values\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"distilbert.transformer.layer.{layer}.attention.v_lin.weight\"],\n",
    "                    \"bias\": parameters[f\"distilbert.transformer.layer.{layer}.attention.v_lin.bias\"],\n",
    "                })\n",
    "            case \"outputs\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"distilbert.transformer.layer.{layer}.attention.out_lin.weight\"],\n",
    "                    \"bias\": parameters[f\"distilbert.transformer.layer.{layer}.attention.out_lin.bias\"],\n",
    "                })\n",
    "            case \"normalize_attention\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"distilbert.transformer.layer.{layer}.sa_layer_norm.weight\"], \n",
    "                    \"bias\": parameters[f\"distilbert.transformer.layer.{layer}.sa_layer_norm.bias\"],\n",
    "                })\n",
    "            case \"ffn\":\n",
    "                module.load_state_dict({\n",
    "                    \"0.weight\": parameters[f\"distilbert.transformer.layer.{layer}.ffn.lin1.weight\"], \n",
    "                    \"0.bias\": parameters[f\"distilbert.transformer.layer.{layer}.ffn.lin1.bias\"],\n",
    "                    \"2.weight\": parameters[f\"distilbert.transformer.layer.{layer}.ffn.lin2.weight\"], \n",
    "                    \"2.bias\": parameters[f\"distilbert.transformer.layer.{layer}.ffn.lin2.bias\"],\n",
    "                })\n",
    "            case \"normalize_ffn\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"distilbert.transformer.layer.{layer}.output_layer_norm.weight\"], \n",
    "                    \"bias\": parameters[f\"distilbert.transformer.layer.{layer}.output_layer_norm.bias\"],\n",
    "                })\n",
    "            case \"classifier\":\n",
    "                module.load_state_dict({\n",
    "                    \"0.weight\": parameters[\"pre_classifier.weight\"], \n",
    "                    \"0.bias\": parameters[\"pre_classifier.bias\"],\n",
    "                    \"2.weight\": parameters[\"classifier.weight\"], \n",
    "                    \"2.bias\": parameters[\"classifier.bias\"],\n",
    "                })                \n",
    "\n",
    "\n",
    "def load_pretrained_state(layer):    \n",
    "    # Load pre-trained state\n",
    "    load_state(\n",
    "        queries, \"queries\", \n",
    "        keys, \"keys\", \n",
    "        values, \"values\", \n",
    "        outputs, \"outputs\", \n",
    "        normalize_attention, \"normalize_attention\",\n",
    "        ffn, \"ffn\",\n",
    "        normalize_ffn, \"normalize_ffn\",\n",
    "        layer=layer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b4579-fb46-481c-8c62-abf66dd022b2",
   "metadata": {},
   "source": [
    "# Transformer Pipeline\n",
    "\n",
    "The following diagram depicts a Transformer as a multi-stage pipeline. The Context stage at the center of the pipeline is where most of the magic happens. The stages before and after Context provide the extra machinery required to convert raw data into input embeddings and output embeddings into task-specific outputs. While we'll focus on text data, it's worth noting that the same stages can be applied to all data modalities including audio and images (Xu et al. 2023).\n",
    "\n",
    "<img src=\"transformer-pipeline.svg\" class=\"stickshift-figure\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796e5d2-ca36-4b34-a48d-de2a4e475e8f",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f495b6e-a916-4e1d-9a29-90adf2548d1b",
   "metadata": {},
   "source": [
    "The Tokenize stage is responsible for breaking raw data into a sequence of \"tokens\". While the word \"token\" is often associated with text processing, the Transformer literature extends this to other data modalities as well. Examples include patches of an image or segments of an audio recording. In fact, tokenization is seen as a core strength of the Transformer architecture because it allows Transformers to process different types of data using a single, universal approach (Xu et al. 2023).\n",
    "\n",
    "While tokenization is a general concept, the specific algorithms used are modality-specific. In this case, our transformer uses an algorithm known as \"word-piece\" (Devlin et al. 2019) to split raw text into a sequence of tokens. Next, special tokens are injected to mark the beginning and end of the sequence. Each token is then converted into an integer-encoded categorical value using a fixed token vocabulary, producing the final sequence of \"input_ids\" that are passed to the next stage.\n",
    "\n",
    "Since our primary interest is in the Transformer layers that come later, we'll use Hugging Face's off-the-shelf tokenizer implementation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ae8a9e-db57-4d0f-96d4-17c8e0175823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokenizer from transformer\n",
    "tokenizer = transformer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c73c7451-d942-4645-abb3-f77dbac14db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2293, 3256, 6949,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize sentence\n",
    "batch = tokenizer(\"I love ice cream\", return_tensors=\"pt\")\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c372b2-d7c8-44b3-ba78-4fbc78b785ad",
   "metadata": {},
   "source": [
    "Tokenizing \"I love ice cream\" generates the token sequence: `[101, 1045, 2293, 3256, 6949, 102]`. If we decode the integer-encoded values to see what each one represents, we can see the four words are represented by values `1045` to `6949`. The values `101` and `102` represent special tokens `[CLS]` and `[SEP]` that were added to mark the beginning and end of the sequence respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "516cc2da-dae9-4746-8132-2b7f16cfab8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'i', 'love', 'ice', 'cream', '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(input_id) for input_id in batch.input_ids[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfacce5-17f9-46ee-8414-053274f8f2df",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df54d4b-9cd9-4044-a6aa-5d08ae5582a0",
   "metadata": {},
   "source": [
    "The second stage in the Transformer pipeline converts each of the integer-encoded categorical values into an \"embedding\". Embeddings (Bengio et al. 2000) are the fundamental data structure of the Transformer architecture. The Transformer layers we'll look at in the next stage take embeddings as input, *transform* them, and produce embeddings as output. Embeddings predated Transformers by almost 2 decades and are a fascinating topic in their own right. But we'll save the embeddings deep dive for another post. For now, all we need to know is embeddings represent each token as a unique point in an n-dimensional vector space. The vector space coordinates are initialized randomly and then learned during training.\n",
    "\n",
    "Similar to tokenization, the steps required to convert tokens into embeddings depend on the data modality. In text transformers, the Embeddings stage is typically implemented using 2 lookup tables. The first lookup table maps the value of each token to a unique embedding vector. The second lookup table maps the position of each token to a unique embedding vector. The value and position embeddings are then added together to create the initial token embeddings.\n",
    "\n",
    "<img src=\"embeddings.svg\" class=\"stickshift-figure\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f05cb-1525-48aa-a001-dddba54d5659",
   "metadata": {},
   "source": [
    "Let's start with value embeddings. First, we initialize the value embeddings lookup table. Next, we read the values from the tokenizer output. Finally, we pass the token values to the lookup table to get unique embeddings for each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a17a31c-f505-4191-9fab-d718216474a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize value embeddings lookup table\n",
    "value_embeddings = nn.Embedding(\n",
    "    num_embeddings=config.vocab_size, \n",
    "    embedding_dim=config.d_model,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(value_embeddings, \"value_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acbd0034-07f6-4e20-bae1-139310c8c18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'i', 'love', 'ice', 'cream', '[SEP]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate token values\n",
    "values = torch.squeeze(batch.input_ids)\n",
    "\n",
    "[tokenizer.decode(input_id) for input_id in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18970299-0558-4419-b89e-9879595e2390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map token values to embeddings\n",
    "v = value_embeddings(values)\n",
    "\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a5fd210-4c1e-4cc1-923c-4af984a8f8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.9925e-02, -1.0171e-02, -2.0390e-02,  ...,  6.1588e-02,\n",
       "          2.1959e-02,  2.2732e-02],\n",
       "        [-1.2794e-02,  4.9879e-03, -2.6270e-02,  ..., -7.2300e-05,\n",
       "          5.3657e-03,  1.1908e-02],\n",
       "        [ 5.9359e-02, -2.3563e-02, -2.0560e-03,  ..., -1.0420e-02,\n",
       "          1.4846e-02, -1.2815e-02],\n",
       "        [-2.4101e-02, -2.4911e-02, -2.2601e-02,  ..., -2.5139e-02,\n",
       "          1.1392e-02,  3.2655e-02],\n",
       "        [-8.5466e-02, -5.9276e-02, -5.6659e-02,  ..., -1.7192e-02,\n",
       "         -8.6179e-02, -4.5105e-02],\n",
       "        [-2.1060e-02, -6.4941e-03, -1.0682e-02,  ..., -2.3401e-02,\n",
       "          6.1463e-03, -6.4845e-03]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show sample of value embeddings\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c203d1a-fd6d-43aa-83a0-82e4a3544dd1",
   "metadata": {},
   "source": [
    "Next, we'll follow a similar set of steps for the position embeddings. We'll start by initializing the position embeddings lookup table. Next, we'll calculate the positions from the tokenizer output. Finally, we pass the token positions to the lookup table to get unique embeddings for each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "740a3ecc-d180-445f-a1fe-f2e157e9b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure position embeddings lookup table\n",
    "position_embeddings = nn.Embedding(\n",
    "    num_embeddings=config.max_sequence_length, \n",
    "    embedding_dim=config.d_model,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(position_embeddings, \"position_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "547facad-a7b5-480b-bd9e-22a21650a838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate token positions\n",
    "positions = torch.arange(values.size(0))\n",
    "\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49cbeef1-f5e4-4e5c-8e15-562c585f289c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map token positions to embeddings\n",
    "p = position_embeddings(positions)\n",
    "\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f4f9448-ef27-46a0-964d-c7e80d367947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8007e-02, -2.3798e-02, -3.5982e-02,  ...,  4.5726e-04,\n",
       "          5.1363e-05,  1.5002e-02],\n",
       "        [ 7.8592e-03,  4.8144e-03, -1.6093e-02,  ...,  2.9312e-02,\n",
       "          2.7634e-02, -8.5431e-03],\n",
       "        [-1.1663e-02, -3.1590e-03, -9.4000e-03,  ...,  1.4870e-02,\n",
       "          2.1609e-02, -7.4069e-03],\n",
       "        [-4.0848e-03, -1.1123e-02, -2.1704e-02,  ...,  1.8962e-02,\n",
       "          4.6763e-03, -1.0220e-03],\n",
       "        [-8.2666e-03, -4.1641e-03, -7.5136e-03,  ...,  1.9757e-02,\n",
       "         -2.2192e-03,  3.8681e-03],\n",
       "        [ 4.6293e-04, -1.8499e-02, -1.9709e-02,  ...,  5.4042e-03,\n",
       "          1.8076e-02,  2.9490e-03]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show sample of position embeddings\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f8e9d-b332-4672-9aee-8ebf472e7076",
   "metadata": {},
   "source": [
    "Now that we have value and position embeddings, we add and normalize them to get the final \"position-encoded token embeddings\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21482619-56f4-4352-9842-1fd1441dc3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure embeddings normalization\n",
    "normalize_embeddings = nn.LayerNorm(\n",
    "    normalized_shape=config.d_model, \n",
    "    eps=1e-12,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(normalize_embeddings, \"normalize_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a2a90c3-c770-4a33-8603-a0fa1cdfcbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add and normalize value and position embeddings\n",
    "x = normalize_embeddings(v + p)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d963258-fc89-4326-984e-8cd40a31b131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3549, -0.1386, -0.2253,  ...,  0.1536,  0.0748,  0.1310],\n",
       "        [ 0.2282,  0.5511, -0.5092,  ...,  0.6421,  0.9541,  0.3192],\n",
       "        [ 1.4511, -0.0794,  0.2168,  ...,  0.2851,  1.0723, -0.0919],\n",
       "        [-0.0564, -0.1761, -0.2870,  ...,  0.1442,  0.6767,  1.0396],\n",
       "        [-1.1349, -0.5135, -0.4714,  ...,  0.3874, -1.0348, -0.2812],\n",
       "        [-0.2980, -0.3332, -0.3742,  ..., -0.3392,  0.3764, -0.1298]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show sample of token embeddings\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c934b-a53a-4c6f-8eb1-c2810aeb27fc",
   "metadata": {},
   "source": [
    "Congrats! You've converted the raw text \"I love ice cream\" into embeddings that encode both the token values and positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef34552-3fc7-4a00-856c-705a78e0f0d5",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc8de72-c9b6-4c23-bc27-b525a043cd18",
   "metadata": {},
   "source": [
    "In the previous stage, we mapped the token values and positions to embeddings. But these embeddings represent the tokens in *isolation*. The Context stage is responsible for infusing each embedding with contextual signals drawn from the entire sequence. At a conceptual level, this should be intuitive. For example, the meaning of the word \"ice\" changes when you add \"cream\" after it.\n",
    "\n",
    "<img src=\"contextualized-embeddings.svg\" class=\"stickshift-figure\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41880d9-cc37-4d32-a8a6-cb0ad60b74a6",
   "metadata": {},
   "source": [
    "## Layers of Attention and FFNs\n",
    "\n",
    "The Context stage works by passing the token embeddings through multiple layers of attention and feedforward blocks. The attention blocks focus on relationships between tokens, augmenting each embedding with information drawn from the surrounding embeddings. The feedforward blocks focus on individual tokens, transforming the contextual clues added by attention with the non-linear transformation magic neural networks are famous for.\n",
    "\n",
    "The following diagram illustrates the stack of Transformer layers in the Context stage. The contents of each layer are identical. By arranging the layers in a stack, the model builds context in small increments similar to the hierarchical features in a CNN. The main differences between popular Transformer models such as BERT and GPT come down to how these layers are configured.\n",
    "\n",
    "<img src=\"transformer-layers.svg\" class=\"stickshift-figure\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510dc3aa-2911-46f9-b5fa-1dbea77335ea",
   "metadata": {},
   "source": [
    "As illustrated above, given input embeddings $X$, we can define the output embeddings $Z$ for a single layer as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Y &= Normalize(X + Attention(X)) \\\\\n",
    "Z &= Normalize(Y + FFN(Y))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcb2ce-05ff-4837-a40d-b320ff65dc5d",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "The Attention block is the signature component of the Transformer architecture. It's also one of the most complicated and likely the least familiar when you're first learning about Transformers. We'll walk through the core attention algorithm described in the original \"All You Need is Attention\" paper by Vaswani et al. one step at a time. At the end of the Context section, we'll put all the pieces together.\n",
    "\n",
    "Vaswani et al. described their attention algorithm as Scaled Dot-Product Attention (SDPA) and defined the standard attention equation everyone cites:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_K}})V\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c22fe94-4120-4161-8359-451572eec2b7",
   "metadata": {},
   "source": [
    "## Queries, Keys, Values\n",
    "\n",
    "The $Q$, $K$, and $V$ terms in the SDPA equation are \"query\", \"key\", and \"value\" matrices respectively. Each row in $Q$, $K$, and $V$ represents a token embedding that has been projected to distinct representation subspaces. Query embeddings represent selection criteria for the surrounding tokens that would add context to the current token definition. Key embeddings represent characteristics that satisfy the selection criteria. Value embeddings represent the contextual information one token transfers to another. Together, queries, keys, and values allow the attention mechanism to refine the representation of each token based on the surrounding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38d246a9-bdfc-45e8-97b1-dd2efcf3de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure query, key, value projections\n",
    "queries = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.d_model,\n",
    ")\n",
    "keys = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_model,\n",
    ")\n",
    "values = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.d_model,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(queries, \"queries\", keys, \"keys\", values, \"values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33e69dfc-ccc1-4f52-b6b9-65a9207f5285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 768]), torch.Size([6, 768]), torch.Size([6, 768]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project token embeddings to query, key, and value spaces\n",
    "q = queries(x)\n",
    "k = keys(x)\n",
    "v = values(x)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a8d05c-3e01-454f-a6d8-51d7df87c452",
   "metadata": {},
   "source": [
    "We can see the projections generated unique query, key, and value embeddings for each of the 6 tokens `['[CLS]', 'i', 'love', 'ice', 'cream', '[SEP]']`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27d541e-2284-4599-90d3-d0854005054f",
   "metadata": {},
   "source": [
    "## Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab030dc-bcd5-494a-9aae-341945664a42",
   "metadata": {},
   "source": [
    "Now that we have $Q$, $K$, and $V$, we can delve into the SDPA equation itself. For each input embedding, SDPA calculates a weighted sum of the value projections for all the tokens in the sequence. We already saw the value projections are represented by $V$. The weights are represented by the softmax term:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "softmax(\\frac{QK^T}{\\sqrt{d_K}})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "I wouldn't hold it against you if it's not immediately obvious what we get here. To see what's happening, let's break this down even further.\n",
    "\n",
    "First, the $QK^T$ term calculates a $d_Q \\times d_K$ matrix of the dot products of each query embedding with every key embedding. To see why, imagine we have 2 token embeddings of length 3. Using matrix multiplication, we end up with a $2 \\times 2$ matrix where each element $w_{ij}$ represents the dot product of query $i$ with key $j$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "QK^T\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "q_{00} & q_{01} & q_{02} \\\\\n",
    "q_{10} & q_{11} & q_{12}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "k_{00} & k_{10} \\\\\n",
    "k_{01} & k_{11} \\\\\n",
    "k_{02} & k_{12}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "w_{00} & w_{01} \\\\\n",
    "w_{10} & w_{11}\n",
    "\\end{bmatrix} \\\\\n",
    "\\text{where } w_{ij} &= row(Q, i) \\cdot row(K, j)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7db10512-59cb-42fb-8dd1-334e6dec0c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate similarity between Q and K\n",
    "w = q @ k.transpose(-2, -1)\n",
    "\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c069a27-6b05-42ba-9448-d53e2c149745",
   "metadata": {},
   "source": [
    "Second, the $1/\\sqrt{d_K}$ term scales the dot products down to avoid pushing the softmax function into regions with very small gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c4d8df1-ae31-4d2c-a0a1-158d7b823950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w /= np.sqrt(config.d_head)\n",
    "\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835d7d85-1457-4994-8714-7dbd370a0487",
   "metadata": {},
   "source": [
    "Finally, the softmax function normalizes the weights across the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6b71776-7b28-434a-9437-62769469dea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 2.5216e-18, 7.7336e-19, 1.0305e-17, 3.1062e-17, 7.4088e-13],\n",
       "        [1.0000e+00, 1.1983e-12, 4.7904e-09, 5.7942e-14, 1.9032e-13, 3.4724e-06],\n",
       "        [1.0000e+00, 1.6518e-10, 1.0182e-12, 1.0676e-12, 6.3056e-11, 5.2538e-08],\n",
       "        [9.8410e-01, 1.3746e-15, 1.3196e-10, 1.9757e-14, 1.5893e-02, 7.7626e-06],\n",
       "        [9.8189e-01, 9.0107e-18, 8.6323e-13, 1.8114e-02, 9.2891e-17, 4.1733e-07],\n",
       "        [9.9863e-01, 3.8286e-12, 1.6438e-15, 1.0339e-14, 1.4698e-10, 1.3678e-03]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize weights across keys\n",
    "w = softmax(w, dim=-1)\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f3fffa5-44e4-4064-b749-1128710360bb",
   "metadata": {
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot weights for each query\n",
    "_, axs = plt.subplots(nrows=2, ncols=3, figsize=(10,5), gridspec_kw={\"wspace\": 0.5, \"hspace\": 0.5})\n",
    "for i in range(6):\n",
    "    ax = axs[i//3][i%3]\n",
    "    sns.scatterplot(x=np.arange(len(w[i])), y=w[i].detach(), ax=ax)\n",
    "    ax.set_title(f\"Query {i}\")\n",
    "    ax.set_xlabel(f\"Keys\")\n",
    "    ax.set_ylabel(f\"Weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b8f79-8434-47e5-a01c-088a587af36b",
   "metadata": {},
   "source": [
    "## Attention Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61da97b-8bd5-43ab-b7d5-37f6fd98947f",
   "metadata": {},
   "source": [
    "Now that we have the attention weights, we can apply them to the values. This will give us a weighted sum of contextual information. However, the answer is still in \"value space\". Before we combine them with the token embeddings, we'll project them back to \"model space\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a8d1c74-fc5e-4ea7-a263-be6eccb208a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute weighted combination of values\n",
    "a = w @ v\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21b505b4-b94c-408d-8e9e-5b5a6d0980cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure output projection\n",
    "outputs = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.d_model,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(outputs, \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65c9b39f-6b40-40a4-bbb8-273821bc21aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project attention embeddings back to model space\n",
    "a = outputs(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3075e1fd-8477-4d2f-857e-250abe9eba04",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "At this point, we've walked through the core SDPA algorithm step-by-step. However, we're not quite done. Vaswani et al. realized there are more than one set of relationships involved in transferring context across tokens. A single application of SDPA would effectively water these down by averaging them together. The solution is to apply SDPA multiple times on separate query, key, and value embeddings. Each of these is referred to as an \"attention head\". Each head is isolated, leaving it free to learn distinct relational structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b7eb210-6157-4f7d-bb16-80054f99d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x):\n",
    "    return x.view(-1, config.n_heads, config.d_head).transpose(-3, -2)\n",
    "\n",
    "def combine_heads(x):\n",
    "    return x.transpose(-3, -2).contiguous().view(-1, int(config.n_heads * config.d_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "566f79ae-4445-4a92-a7d0-5b15bb1327f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 768]), torch.Size([6, 768]), torch.Size([6, 768]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Render query, key, value dimensions before we split\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efa0cabc-9903-4e1f-a188-8fe9ef2c3647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 6, 64]), torch.Size([12, 6, 64]), torch.Size([12, 6, 64]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split queries, keys, values into separate heads\n",
    "q = split_heads(q)\n",
    "k = split_heads(k)\n",
    "v = split_heads(v)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8eab-37ea-42ea-b466-d237b499d87c",
   "metadata": {},
   "source": [
    "We can see that the queries, keys, and values have been split into 12 heads. Each of the original 768-element query, key, and value embeddings is now 64 elements long.\n",
    "\n",
    "Next, let's recompute the attention embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1539a494-99dc-477f-bfb0-1dfbafeafce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 6, 64])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute attention for all heads in parallel\n",
    "a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head), dim=-1) @ v\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cc6ac3-16b4-4ebb-8b21-b650d3b63d4f",
   "metadata": {},
   "source": [
    "While the attention code is the same, you can see the attention values are still split into heads. Next, we'll recombine them before applying the final output projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5df05cde-1886-49b1-b2df-9c24b689391e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recombine heads\n",
    "a = combine_heads(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be91d403-0580-4519-aaf1-d432ef0f8e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project attention embeddings back to model space\n",
    "a = outputs(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d8a6f8-deb0-4e68-bffa-149e3ca3c2c5",
   "metadata": {},
   "source": [
    "## Add and Normalize\n",
    "\n",
    "Before we get to the FFN, we combine the attention embeddings with input embeddings the same way we combined the value and position embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd4453d3-0ee2-42e1-8b26-f0a8d8e8298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention normalization\n",
    "normalize_attention = nn.LayerNorm(\n",
    "    normalized_shape=config.d_model, \n",
    "    eps=1e-12,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(normalize_attention, \"normalize_attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5dfbb320-be66-4456-81c2-92d63c7129a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine attention with input embeddings\n",
    "y = normalize_attention(x + a)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a8da5b-78fa-4528-9a74-f0cedc33c49b",
   "metadata": {},
   "source": [
    "## FFN\n",
    "\n",
    "The FFN block is a straightforward fully connected multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2661b91-b813-44af-83e1-7870e4c1fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FFN\n",
    "ffn = nn.Sequential(\n",
    "    nn.Linear(in_features=config.d_model, out_features=config.d_ffn),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(in_features=config.d_ffn, out_features=config.d_model),\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(ffn, \"ffn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f90b3b4f-98ff-4683-9302-ed08bab7440e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform attention outputs\n",
    "f = ffn(y)\n",
    "\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4252c07b-de67-4220-ac2d-415790ba906b",
   "metadata": {},
   "source": [
    "## Add and Normalize\n",
    "\n",
    "Next, we combine the transformed embeddings with the attention embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d4b356b-a9e3-4081-bf36-dbb2a6a1c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention normalization\n",
    "normalize_ffn = nn.LayerNorm(\n",
    "    normalized_shape=config.d_model,\n",
    "    eps=1e-12,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(normalize_ffn, \"normalize_ffn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45b46002-94be-4fc0-b9cb-6caa9127a422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = normalize_ffn(y + f)\n",
    "\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93630fa8-50d2-48fb-9df2-a339ef1aaf25",
   "metadata": {},
   "source": [
    "Quick recap... We just finished going through a single Transformer layer. Given input embeddings $X$, we calculated and added attention embeddings to get $Y$, and then calculated and added transformed embeddings to get $Z$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Y &= Normalize(X + Attention(X)) \\\\\n",
    "Z &= Normalize(Y + FFN(Y))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0917d31-aa8c-495e-b6eb-cfaaa845bfd2",
   "metadata": {},
   "source": [
    "## Stacking the Layers\n",
    "\n",
    "Next, we combine all of the steps and repeat for each layer in the stack. While you would normally create a stack of torch modules, instead we run the layers in a loop to make it easier to see what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4dbc9b44-2fb2-4d2e-98ee-4778f243b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loop w/ initial input embeddings\n",
    "z_i = x\n",
    "\n",
    "# Apply layer logic in a loop\n",
    "for layer in range(config.n_layers):\n",
    "    \n",
    "    # Use previous layer's outputs as inputs\n",
    "    x_i = z_i\n",
    "\n",
    "    # Load pre-trained state for layer\n",
    "    load_pretrained_state(layer)\n",
    "\n",
    "    #\n",
    "    # Attention\n",
    "    #\n",
    "    \n",
    "    # Project x_i to query, key, and value spaces\n",
    "    q_i = queries(x_i)\n",
    "    k_i = keys(x_i)\n",
    "    v_i = values(x_i)\n",
    "    \n",
    "    # Split q, k, v into separate attention heads\n",
    "    q_i = split_heads(q_i)\n",
    "    k_i = split_heads(k_i)\n",
    "    v_i = split_heads(v_i)\n",
    "\n",
    "    # Compute attention for all heads in parallel\n",
    "    w_i = softmax(\n",
    "        q_i @ k_i.transpose(-2, -1) / np.sqrt(config.d_head), \n",
    "        dim=-1,\n",
    "    )\n",
    "    a_i = w_i @ v_i\n",
    "    \n",
    "    # Recombine attention heads\n",
    "    a_i = combine_heads(a_i)\n",
    "    \n",
    "    # Project attention embeddings back to model space\n",
    "    a_i = outputs(a_i)\n",
    "\n",
    "    # Add and normalize\n",
    "    y_i = normalize_attention(x_i + a_i)\n",
    "\n",
    "    #\n",
    "    # FFN\n",
    "    #\n",
    "\n",
    "    # Transform attention\n",
    "    f_i = ffn(y_i)\n",
    "\n",
    "    # Add and normalize\n",
    "    z_i = normalize_ffn(y_i + f_i)\n",
    "\n",
    "# Save outputs from last layer\n",
    "z = z_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e46166a-a427-41fb-958d-0deb84d46682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.6173e-01, -1.3168e-01,  3.5340e-02,  ...,  4.4015e-01,\n",
       "          1.0666e+00, -1.9293e-01],\n",
       "        [ 7.3341e-01,  4.9823e-02, -1.7590e-02,  ...,  5.0063e-01,\n",
       "          1.1480e+00, -1.2997e-01],\n",
       "        [ 1.1230e+00,  2.7603e-01,  3.2096e-01,  ...,  1.8820e-01,\n",
       "          1.0586e+00, -1.2496e-01],\n",
       "        [ 4.8728e-01,  1.4863e-02,  4.2930e-01,  ...,  4.8993e-01,\n",
       "          7.9435e-01,  1.2331e-01],\n",
       "        [ 1.0595e-03, -1.4508e-01,  2.8892e-01,  ...,  5.5342e-01,\n",
       "          7.9370e-01, -9.0899e-02],\n",
       "        [ 1.1021e+00,  8.6115e-02,  5.7461e-01,  ...,  6.8800e-01,\n",
       "          5.6345e-01, -6.6278e-01]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9f577c2-82dc-4187-856e-2d64f34630fa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "hiddens = transformer.model.distilbert(input_ids=batch.input_ids.to(device)).last_hidden_state.squeeze().to(\"cpu\")\n",
    "assert torch.allclose(z, hiddens, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f318a24-9dc4-4ba3-a369-3dcc231c4a3f",
   "metadata": {},
   "source": [
    "# Head\n",
    "\n",
    "As the final stage in the Transformer pipeline, Head maps the contextualized embeddings to task-specific predictions. In our case, the Head stage is responsible for turning the contextualized embeddings into a binary classifier that predicts whether the original text contains positive or negative sentiments. This sounds like a straightforward neural network output layer until you realize that instead of one set of features, we have a sequence of features. And the length of the sequence is arbitrary. How do you connect an arbitrary length sequence of feature vectors to an output layer?\n",
    "\n",
    "The trick is hiding in our contextualized embeddings. Each input embedding represents a single token in isolation. But the output embeddings have been infused with information from all of the tokens. This is why it's common practice to simply take the first output embedding and drop the rest. The first embedding represents the start of sequence marker `[CLS]`. Since the `[CLS]` marker token is added to every sequence, the first *input* embedding is always the same. In contrast, the first *output* embedding uniquely represents this specific sequence.\n",
    "\n",
    "If we take the first output embedding to represent the entire sequence, then we have a single feature vector that's easy to connect to any task-specific output layer we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0cf5b65b-28b7-41be-9d48-a98f56d35e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use [CLS] embedding to represent the entire sequence\n",
    "features = z[0]\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "babf146e-10c0-4a57-9ee2-6499e18ec6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure classifier head\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=config.d_model, out_features=config.d_model),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=config.d_model, out_features=config.n_labels),\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(classifier, \"classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "875ada3a-42e6-42b2-9c6f-7b611a5252c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998118281364441"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classify features\n",
    "prediction1 = torch.softmax(classifier(features), dim=-1)[1].item()\n",
    "prediction1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3209240f-d3c7-4445-a9f7-242043dc8830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998118281364441"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify custom results match off-the-shelf ones\n",
    "prediction2 = transformer(\"I love ice cream\")[0][\"score\"]\n",
    "prediction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3f405e1-9b83-45b7-b318-36560da32f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert prediction1 == approx(prediction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1266662a-c54e-438e-95ef-cb13eeaf463b",
   "metadata": {},
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a8c4e5-a16a-41b6-b515-52f1eb063cbe",
   "metadata": {},
   "source": [
    "We did it! We dissected an off-the-shelf, production-grade Transformer model, cataloged all the parts, and reassembled everything, tracing a single inference from the raw text \"I love ice cream\" to a positive sentiment prediction of `0.99`. Not only did we get the same positive prediction, we got the exact same answer as Hugging Face's production PyTorch code! That should give you confidence at least that we didn't leave any steps out.\n",
    "\n",
    "I hope you learned something about Transformers. In the very least, I hope they're a little less intimidating. Go take something else apart!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce44040-c06c-4bc2-afe3-7edccf5943db",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Bengio, Yoshua, Réjean Ducharme, and Pascal Vincent. “A Neural Probabilistic Language Model.” In Advances in Neural Information Processing Systems, Vol. 13. MIT Press, 2000. <https://proceedings.neurips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html>.\n",
    "\n",
    "Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv, May 24, 2019. <https://doi.org/10.48550/arXiv.1810.04805>.\n",
    "\n",
    "“Transformer Explainer: LLM Transformer Model Visually Explained.” Accessed August 29, 2024. <https://poloclub.github.io/transformer-explainer/>.\n",
    "\n",
    "Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” arXiv, August 1, 2023. <https://doi.org/10.48550/arXiv.1706.03762>\n",
    "\n",
    "Xu, Peng, Xiatian Zhu, and David A. Clifton. “Multimodal Learning with Transformers: A Survey.” arXiv, May 9, 2023. <https://doi.org/10.48550/arXiv.2206.06488>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "stickshift": {
   "description": "Tracing an inference from raw data to prediction",
   "title": "Transformer Teardown: DistilBERT"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
