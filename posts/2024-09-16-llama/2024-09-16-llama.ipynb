{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f5fa8b-9842-4be3-8cdc-3421d5019662",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "source": [
    "# Transformer Teardown: Llama 3.1\n",
    "\n",
    "> Trace an Inference Through Each Layer of the SOTA Llama 3.1 Foundation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924b74d-61d7-4cfc-a3c0-5c333bec5659",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In [the last Transformer Teardown](https://stickshift.github.io/2024/09/04/transformer-teardown.html), we dissected a DistilBERT text classification pipeline, tracing a single inference through the entire stack from raw data to final prediction. We learned about the main stages of a Transformer pipeline as well as fundamental Transformer concepts such as token embeddings and Multi-Head Self Attention. Studying BERT-based text classification models is a fantastic way to see the basic Transformer machinery in action. But BERT was published in 2018! It would be another 4 years before ChatGPT launched and Generative AI exploded onto the scene.\n",
    "\n",
    "In this Transformer Teardown, we're going to fast forward to present day. We'll use the same teardown process to unpack the state-of-the-art [Llama 3.1](https://llama.meta.com/) open source foundation models released by Meta in July. We'll walk through each step of a text generation pipeline one cell at a time, tracing an inference from raw text to the first output token. We'll illustrate the main ideas from the latest Transformer literature with minimal, straightforward, working Python code, giving you a close-up view of the core mechanisms driving the Generative AI revolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c3be3e-78e2-47f4-8714-d6613bea7a4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "040ef74f-fccb-4a91-9748-b3eba0398a0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sys import stdout\n",
    "from textwrap import dedent\n",
    "import warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas import Series\n",
    "from pydantic import BaseModel, validate_call\n",
    "from pytest import approx\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import relu, silu, softmax\n",
    "\n",
    "from llama_models.llama3.reference_impl.model import RMSNorm\n",
    "\n",
    "import stickshift as ss\n",
    "from stickshift import default_arg, take\n",
    "from stickshift.torch import device as torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8523438f-2c81-474b-bab5-e9a707fcc572",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure gpu\n",
    "device = torch_device()\n",
    "\n",
    "# Torch display options\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca4d3dd0-eff4-4090-8673-9810d87b1b74",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "figure > img {\n",
       "    display:block;\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "figcaption {\n",
       "    text-align: center;\n",
       "}\n",
       "blockquote {\n",
       "    margin-top: 2.0rem !important;\n",
       "    margin-bottom: 2.0rem !important;\n",
       "    margin-left: 0 !important;\n",
       "    margin-right: 0 !important;\n",
       "    padding: 1.0rem !important;\n",
       "    background-color: rgba(0,0,0,0.05) !important;\n",
       "    border: 1px solid rgba(0,0,0,0.1) !important;\n",
       "    font-style: italic !important;\n",
       "}\n",
       "blockquote p {\n",
       "    margin: 0 !important;\n",
       "    padding: 0 !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "figure > img {\n",
    "    display:block;\n",
    "    margin-left: auto !important;\n",
    "    margin-right: auto !important;\n",
    "}\n",
    "figcaption {\n",
    "    text-align: center;\n",
    "}\n",
    "blockquote {\n",
    "    margin-top: 2.0rem !important;\n",
    "    margin-bottom: 2.0rem !important;\n",
    "    margin-left: 0 !important;\n",
    "    margin-right: 0 !important;\n",
    "    padding: 1.0rem !important;\n",
    "    background-color: rgba(0,0,0,0.05) !important;\n",
    "    border: 1px solid rgba(0,0,0,0.1) !important;\n",
    "    font-style: italic !important;\n",
    "}\n",
    "blockquote p {\n",
    "    margin: 0 !important;\n",
    "    padding: 0 !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58287922-dc47-44d9-a2c6-77bd0c8abb7f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61da974-fb00-438a-bb44-f3a432182478",
   "metadata": {},
   "source": [
    "# Llama Foundation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d95b5-8ee7-45bd-9d2d-7908012ac587",
   "metadata": {},
   "source": [
    "[Llama](https://llama.meta.com/) is a family of general purpose, state-of-the-art open source foundation models from Meta. According to the 3.1 technical report, the latest models can \"answer questions in at least 8 languages, write high quality code, solve complex reasoning problems, and use tools in a zero-shot way.\" (Dubey et al. 2024) The Llama 3.1 release includes 8B, 70B, and 405B sizes. While you need a multi-GPU cluster to run the 70B and 405B sizes, the 8B model is small enough to experiment with on a laptop. Not only did Meta release the pre-trained model checkpoints for all 3 sizes, they also published a fantastically detailed, [70 page technical report](https://arxiv.org/abs/2407.21783v2) as well as a complete [reference implementation](https://github.com/meta-llama/llama-models). Together, Llama 3.1 represents both a tremendous contribution to the AI community as well as an incredible learning opportunity to study the inner workings of a modern frontier model.\n",
    "\n",
    "Over the course of this post, we'll implement a complete text generation pipeline using only the research literature, pre-trained weights from the `Meta-Llama3.1-8B-Instruct` checkpoint, and Meta's reference implementation as a guide. After we load the 8B checkpoint, we'll review the stages of an end-to-end, text generation pipeline. In the sections that follow, we'll walk through a detailed teardown of each stage—tracing an inference from raw data to the first output token. In the last section, we'll put all the pieces together into a complete generative Transformer capable of producing long form content.\n",
    "\n",
    "Let the teardown begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a3d63-e556-4845-9c2f-cea2d0712d27",
   "metadata": {},
   "source": [
    "# Model Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb04ecc-6f9a-4e2a-9c23-7ae89a702f31",
   "metadata": {},
   "source": [
    "We'll start by loading the configuration and pre-trained weights for the `Meta-Llama3.1-8B-Instruct` checkpoint. The \"instruct\" versions of the Llama models include the raw pre-training and substantial post-training to support user and assistant interactions and complex tool-calling scenarios. The weights for all Llama checkpoints can be downloaded directly from [Meta](https://llama.meta.com/), [Hugging Face](https://huggingface.co/meta-llama), and [Kaggle](https://www.kaggle.com/organizations/metaresearch/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07362fc6-41cf-45b4-8d88-e8c55aa9ce9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checkpoint_path': PosixPath('/Users/andrewyoung/.llama/checkpoints/Meta-Llama3.1-8B-Instruct'), 'vocab_size': 128256, 'd_model': 4096, 'd_head': 128, 'd_ffn': 14336, 'n_layers': 32, 'n_heads': 32, 'n_kv_heads': 8, 'rms_norm_eps': 1e-05, 'rope_theta': 500000.0, 'max_seq_len': 8192, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9, 'max_output_tokens': 500}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import custom utilities\n",
    "from stickshift.models import llama\n",
    "\n",
    "# Load model config\n",
    "config = llama.config(\"Meta-Llama3.1-8B-Instruct\")\n",
    "\n",
    "# Load pre-trained model parameters\n",
    "checkpoint = torch.load(\n",
    "    config.checkpoint_path / \"consolidated.00.pth\", \n",
    "    weights_only=True, \n",
    "    map_location=device,\n",
    ")\n",
    "\n",
    "config.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bc3a1e-91c4-4fec-896f-2f5ee6977915",
   "metadata": {},
   "source": [
    "We'll reference a number of the settings in `config` throughout the teardown. For now, a few interesting ones to note are `d_model`, `d_ffn`, `n_layers`, and `n_heads`. These represent the primary differences between the 8B, 70B, and 405B sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6acaf5d-b853-46fa-86e1-d63030074dbd",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "def load_pretrained_state(layer):    \n",
    "    # Load pre-trained state\n",
    "    llama.load_state(\n",
    "        normalize_attention, \"normalize_attention\", \n",
    "        normalize_ffn, \"normalize_ffn\", \n",
    "        W_q, \"w_q\", \n",
    "        W_k, \"w_k\", \n",
    "        W_v, \"w_v\", \n",
    "        attention_outputs, \"attention_outputs\",\n",
    "        ffn_gates, \"ffn_gates\",\n",
    "        ffn_inputs, \"ffn_inputs\",\n",
    "        ffn_outputs, \"ffn_outputs\",\n",
    "        checkpoint=checkpoint,\n",
    "        layer=layer,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb0698a-439d-44ad-959c-33b2a19a9593",
   "metadata": {},
   "source": [
    "# Text Generation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c8069-1148-48a7-bb07-a91f60b43a3f",
   "metadata": {},
   "source": [
    "In [the last teardown](https://stickshift.github.io/2024/09/04/transformer-teardown.html), we looked at a text classification Transformer. This time we're going to dissect a *text generation* Transformer. Instead of simply applying a label to the input text, the Head stage will be responsible for *generating* new content. But don't worry! It's not as complicated as it sounds.\n",
    "\n",
    "Figure 1 illustrates the stages in a text generation pipeline. It's very similar to the text classification pipeline we looked at last time. The Tokenize stage splits raw text into a sequence of tokens. The Embeddings stage maps the sequence of tokens to a sequence of embedding vectors. The Context Layers augment the embeddings with contextual signals drawn from the surrounding tokens, transforming individual token embeddings into contextualized \"semantic embeddings\". Finally, the Head stage converts the semantic embeddings into predictions. The key difference is, instead of predicting a label for the raw text, text generation Transformers *predict the next token*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc06ac-b312-491b-98b6-ed82d237b6cd",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"transformer-pipeline.svg\" width=\"940\">\n",
    "<figcaption>Figure 1: Text Generation Pipeline</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe17de4c-55b7-48d3-8f78-43874b3c4bb4",
   "metadata": {},
   "source": [
    "But one token is just the beginning! The magical powers of Generative AI are manifested by simply running the token predictions in a loop. The predicted token in each iteration is appended to the end of the input sequence, and the process repeats. Over and over again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95117d8e-7ea3-4c2a-ad4a-1bd535771a5f",
   "metadata": {},
   "source": [
    "# Raw Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacc9c84-f587-49e4-85d5-e89ed78fa9db",
   "metadata": {},
   "source": [
    "Before we can tear anything down, we need a prompt. Since our goal is to trace an inference from raw text to the first output token, we want to start with a prompt that's specific enough to generate a consistent, one-word answer. If we do everything right, the first output token we predict should be \"Boston\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9832bc2-033e-4c48-903f-f05544c135eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "prompt += \"What is the capital of Massachusetts? Answer in one word.\"\n",
    "prompt += \"<|eot_id|>\"\n",
    "prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5ae4e-3b6d-4b71-9eed-a2dd512cf8f9",
   "metadata": {},
   "source": [
    "You can see `prompt` includes a number of special tokens. These would usually be injected by a framework like Hugging Face's `transformers`. We need to manually inject them because we're working with the model directly. You can find more information on the Llama 3.1 prompt syntax in the [Llama Prompting Guide](https://www.llama.com/docs/how-to-guides/prompting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0e94d-a969-43b8-bfad-2ceaa10ca7aa",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f879aa-2352-48a8-bc02-60adcd1d058a",
   "metadata": {},
   "source": [
    "The Tokenize stage splits raw text into a sequence of tokens using a fixed vocabulary. Llama uses a vocabulary of 128k tokens built on top of OpenAI's tiktoken tokenizer. We'll dig into the gory details in the later stages, but here we'll simply use the off-the-shelf Tokenizer from Meta's llama-models reference implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9605a1b6-beb8-4f7f-8ad2-67351929a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_models.llama3.api.tokenizer import Tokenizer\n",
    "\n",
    "# Load tokenizer model from checkpoint\n",
    "tokenizer = Tokenizer(str(config.checkpoint_path / \"tokenizer.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07c6b94b-36dc-43b0-8dc8-e99c28d26d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 128006, 882, 128007, 271, 3923, 374, 279, 6864, 315, 22108, 30, 22559, 304, 832, 3492, 13, 128009, 128006, 78191, 128007, 271]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split raw text into tokens\n",
    "token_ids = tokenizer.encode(prompt, bos=True, eos=False, allowed_special=\"all\")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bfa2427-7eba-4c68-b161-fc8238d4c2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e067e-95ba-4c7c-aef6-0e6c031057fa",
   "metadata": {},
   "source": [
    "We see `tokenizer.encode` split our prompt into 22 token ids. These ids represent the index of each token in Llama's 128k token vocabulary. We can always reverse the process with `tokenizer.decode`. If you look closely at the cell output below, you'll notice the tokenizer injected another special token `(128000, '<|begin_of_text|>')` to mark the beginning of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73c0e958-3d09-4f4e-8269-2cf7a1a39bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the capital of Massachusetts? Answer in one word.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode token ids back into raw text\n",
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "040cffac-921e-43ad-ac2b-069895233360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load token_ids into a tensor\n",
    "X = torch.tensor(token_ids, device=device)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f60316-a506-4912-b8ac-8bbaa83a4d7f",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccd09b-1a58-40e7-98f4-4794ea681c98",
   "metadata": {},
   "source": [
    "Embeddings are a key component of the Transformer architecture. They're also abstract mathematical structures that can be difficult to wrap your head around. To illustrate the crucial role embeddings play, let's use a quick metaphor.\n",
    "\n",
    "> If a Transformer was a brain, then embeddings would be the electrical signals carrying information through the brain.\n",
    "\n",
    "Continuing with the metaphor, the Embeddings stage of the pipeline would be your sensory organs where light rays and air vibrations are translated into electrical impulses. Token embeddings would be the fresh sensory percepts. Semantic embeddings would be the abstract thoughts at the top of the cortical stack. The idea of percepts traveling up the cortical stack is a perfect analogy for token embeddings traveling through the Transformer layers.\n",
    "\n",
    "Implementing Llama's Embeddings stage is relatively straightforward. We'll use a lookup table with a unique embedding for each of the 128k tokens in the vocabulary. Each embedding is a vector with $d_{model}$ elements that were randomly generated and then learned during training. Given a sequence of token ids, the lookup table returns their embeddings as row vectors stacked in an $n \\times d_{model}$ tensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811d333-dc1c-4177-950a-56ced36ab246",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"embeddings.svg\" width=\"940\">\n",
    "<figcaption>Figure 2: Learned Token Embeddings</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43f02c0c-6073-401e-9f8e-b6f5159e7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings lookup table\n",
    "embeddings = nn.Embedding(\n",
    "    num_embeddings=config.vocab_size, \n",
    "    embedding_dim=config.d_model,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "llama.load_state(embeddings, \"embeddings\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd113f37-d587-45e0-aecd-5ddc7b82144a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map token ids to embeddings\n",
    "X = embeddings(X)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46771ed3-4483-4a7b-ade5-25179d13feba",
   "metadata": {},
   "source": [
    "We can see from `X.shape` that we successfully mapped the 22 token ids to 22 token embeddings stacked in an $n \\times d_{model}$ tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5da73e42-c82a-4469-bdf5-fe04e51bae6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,  6.3419e-05,  1.1902e-03],\n",
       "        [-1.6499e-04, -2.4319e-04,  1.6403e-04,  ..., -1.5163e-04,  3.5095e-04,  7.3242e-04],\n",
       "        [ 3.5095e-03,  7.2021e-03,  5.3406e-05,  ..., -7.2479e-04, -1.0620e-02,  8.2779e-04],\n",
       "        ...,\n",
       "        [-9.7656e-03, -3.4637e-03,  1.8616e-03,  ..., -7.1411e-03, -4.3030e-03,  8.6060e-03],\n",
       "        [-4.6158e-04, -3.9291e-04, -6.5863e-06,  ..., -6.2561e-04, -5.0354e-04,  6.6757e-04],\n",
       "        [-2.8687e-03,  3.8910e-03, -1.7357e-04,  ...,  8.0872e-04,  5.0354e-04,  2.3041e-03]], device='mps:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show sample\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc36c605-3d13-4d36-98a0-021005ad0f75",
   "metadata": {},
   "source": [
    "Before we move on, a quick note on terminology. If you've used cloud-based LLM APIs like OpenAI or LangChain, you're likely familiar with the term \"embedding model\". An *embedding model* is really a combination of a tokenizer and embeddings table. These are often bundled together to give you everything you need to convert raw text into embedding vectors and can be used for a number of things independent of the LLM.\n",
    "\n",
    "Now that we've converted our raw text into token embeddings, it's time to start transforming!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab57f0-610c-46e5-b372-532c41699d43",
   "metadata": {},
   "source": [
    "# Context Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f094441-b5d6-48be-a3ff-63af16a8d9a6",
   "metadata": {},
   "source": [
    "Context layers are where the Transformer magic happens. Collectively, the Context Layers are responsible for transforming a sequence of token embeddings into a sequence of semantic embeddings. The mechanism works by passing the embeddings through multiple layers of attention and feedforward blocks. The attention blocks focus on relationships between embeddings, augmenting each one with a weighted combination of the surrounding embeddings. The feedforward blocks capitalize on the extra context, transforming each augmented embedding with the non-linear magic of a fully-connected multilayer perceptron. By stacking multiple layers together, Transformers repeat the pattern of attention and transformation, gradually converting representations of individual words into representations of abstract semantic concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714c9ef-7079-48f8-ad54-22dc6b45ff50",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"transformer-layers.svg\" width=\"940\">\n",
    "<figcaption>Figure 3: Context Layers</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a17731-7c4a-48a4-b675-727aba824ec2",
   "metadata": {},
   "source": [
    "Figure 3 illustrates the flow of information through a single layer. Embeddings are first passed to the Attention block. The attention outputs are added to the attention inputs before being passed to the FFN block. Similarly, the FFN outputs are added to the FFN inputs before being passed to the next layer. Adding the inputs and outputs of each block is known as \"residual learning\" and is critical for providing a stable path for gradient flow during training (He et al. 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aadd18f-5a1a-4c70-a461-5f07eecba08f",
   "metadata": {},
   "source": [
    "## Decoder-Only Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67512e8-b4bb-4c90-ade5-7f000d323a66",
   "metadata": {},
   "source": [
    "Like most of today's generative models, Llama uses a \"decoder-only\" model architecture. Instead of using the fully connected self attention we saw in [the DistilBERT teardown](https://stickshift.github.io/2024/09/04/transformer-teardown.html), the context layers in Llama use *masked self attention*. The \"decoder-only\" term comes from the \"Attention is All You Need\" paper, where Vaswani et al. described layers of self attention as \"encoder layers\" and layers of masked self attention as \"decoder layers\". While Vaswani et al.'s *Vanilla* Transformer architecture processed inputs and outputs with encoder layers and decoder layers respectively, later researchers showed that by adding more compute you could achieve the same goals using a single stack of decoder layers. For a fascinating discussion of how decoders became the dominant architecture, I highly recommend watching [Hyung Won Chung's guest lecture at Stanford on the Future of AI](https://youtu.be/orDKvo8h71o?si=J2sxhYtL9LCd6IRk) from April of this year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5839ec-b94a-4209-b6a4-a1143b29ac87",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc610a3b-7ecd-41d1-a4af-f24394455f07",
   "metadata": {},
   "source": [
    "Attention is the signature component in the Transformer architecture. In the 7 years since Vaswani et al. published \"Attention is All You Need\", researchers have experimented with numerous attention variations of all shapes and sizes. Before we jump into the code, we'll quickly review the fundamental concepts behind attention followed by details on the specific approach chosen by the Llama authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddaf2b1-7bc4-493e-a27d-f947f942d975",
   "metadata": {},
   "source": [
    "### What is Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e68a4-c57a-44c9-88d1-b9cc3577e405",
   "metadata": {},
   "source": [
    "Given our input embeddings are stacked in an $n \\times d_{model}$ tensor $\\mathbf{X}$, the goal of attention is to map each embedding $\\set{\\mathbf{x}_m \\mid \\mathbf{x}_m \\in \\mathbf{X}}$ to an attention representation $\\mathbf{a}_m$ that includes relevant contextual signals drawn from the rest of the embeddings $\\set{\\mathbf{x}_n \\mid \\mathbf{x}_n \\in \\mathbf{X}, n \\neq m}$. \n",
    "\n",
    "For example, let's imagine we've mapped the sentence `I love New York` to the sequence of token embeddings $\\mathbf{x} = [E_{I}, E_{love}, E_{New}, E_{York}]$. The embedding $\\mathbf{x}_2$ represents the word \"New\" *in isolation*. The word \"New\" can mean a lot of things; many of which have nothing to do with this sentence. Our goal would be to generate an attention representation $\\mathbf{a}_2$ containing signals from the other embeddings $\\set{E_{I}, E_{love}, E_{York}}$ that would help us create a *better* version of $\\mathbf{x}_2$:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{2*} = \\mathbf{x}_{2} + \\mathbf{a}_{2}\n",
    "$$\n",
    "\n",
    "Let's assume each embedding contributes \"something\" to $\\mathbf{a}_m$. Even though we can't quantify \"something\" yet, we can write $\\mathbf{a}_m$ as an unknown function $f_A$ of the two embeddings $\\mathbf{x}_m$, $\\mathbf{x}_n$:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_m = \\sum_{\\mathbf{x}_n \\in \\mathbf{X}} f_A(\\mathbf{x}_m, \\mathbf{x}_n)\n",
    "$$\n",
    "\n",
    "All of the attention variations in the Transformer literature—e.g. Self Attention, Multi-Head Self Attention, Linear Attention, Grouped Query Attention—are different approaches to implement $f_A$. In practice, the authors of a new Transformer model start with Vaswani et al.'s attention definition and then select from the large, à la carte menu of improvements that have been published since, resulting in their own unique variation of attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a4fbf6-26c3-48b0-b154-2cf72108cca3",
   "metadata": {},
   "source": [
    "### Attention in Llama 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809be56b-99a1-490f-8f10-17b590a4cc66",
   "metadata": {},
   "source": [
    "Defining attention in Llama 3.1 requires a bit of backtracking. Most of the details can be found in Llama 1 (Touvron et al. 2023) with a few changes in Llama 2 (Touvron et al. 2023) and only minor adjustments in Llama 3 (Dubey et al. 2024).\n",
    "\n",
    "Starting with the standard Masked Self Attention definition from Vaswani et al. (2017), Llama adopts the following improvements that affect attention:\n",
    "\n",
    "* Pre-normalization with RMSNorm: improves training stability and inference speed.\n",
    "* Grouped Query Attention (GQA) with 8 key/value heads: improves inference speed and reduce memory overhead of key-value caches.\n",
    "* Rotary Position Embedding (RoPE) with $\\Theta = 500,000$: relative position encoding improves performance on longer context windows.\n",
    "\n",
    "We'll start by describing the standard Masked Self Attention and then describe how these improvements modify the final attention calculation in Llama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f40ac23-8fa7-4c3b-a48b-bb5936c42e01",
   "metadata": {},
   "source": [
    "### Masked Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a440c1-a833-44da-aa69-7e327c1b30f0",
   "metadata": {},
   "source": [
    "Given $n$ input embeddings of length $d_{model}$ stacked in an $n \\times d_{model}$ tensor $\\mathbf{X}$, the standard masked self attention algorithm from Vaswani et al. can be expressed using the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{A} = softmax\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_K}} + \\mathbf{M}\\right)\\mathbf{V} \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ are all linear projections of $\\mathbf{X}$.\n",
    "* $\\mathbf{Q}$ is an $n \\times d_K$ tensor of *queries* that represent selection criteria for surrounding embeddings that would add valuable context to the current representation.\n",
    "* $\\mathbf{K}$ is an $n \\times d_K$ tensor of *keys* that represent characteristics that satisfy the selection criteria in the queries.\n",
    "* $\\mathbf{V}$ is an $n \\times d_{model}$ tensor of *values* that represent the contextual signals to transfer from one embedding to another.\n",
    "* The $\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_K}}$ term is an $n \\times n$ tensor of \"attention weights\" that define how much of each embedding's values to include from $\\mathbf{V}$.\n",
    "* $\\mathbf{M}$ is an $n \\times n$ attention mask that prevents earlier embeddings from attending to later ones by adding a $-\\infty$ bias to the later embeddings' scores.\n",
    "* $\\mathbf{A}$ is an $n \\times d_{model}$ tensor of attention representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c80f9-2fcf-493e-b365-ae2b70db6ed3",
   "metadata": {},
   "source": [
    "The mechanism starts with the $\\mathbf{Q}\\mathbf{K}^T$ term that calculates the *angular distance* between each query vector $\\mathbf{q}_m$ and key vector $\\mathbf{k}_n$ by taking their dot product. The smaller the angle, the closer the vectors, and the better the match between $\\mathbf{q}_m$ and $\\mathbf{k}_n$. The result of the $\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_K}}$ term is an $n \\times n$ tensor of raw scores where row $i$ represents query $\\mathbf{q}_m$ and column $j$ represents how well key $\\mathbf{k}_n$ matches $\\mathbf{q}_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6a701-a49e-4d35-878d-0d6d26ec5e04",
   "metadata": {},
   "source": [
    "The next step is to add the attention mask $\\mathbf{M}$ to prevent earlier embeddings from attending to later embeddings. Imagine $\\mathbf{M}$ as an $n \\times n$ diagonal mask with the upper right half set to $-\\infty$ and the lower left set to $0$. Illustrated below, you can see that $\\mathbf{q}_0$ can only attend to $\\set{\\mathbf{k}_0}$, while $\\mathbf{q}_1$ can attend to $\\set{\\mathbf{k}_0, \\mathbf{k}_1}$, and $\\mathbf{q}_2$ can attend to $\\set{\\mathbf{k}_0, \\mathbf{k}_1, \\mathbf{k}_2}$.\n",
    "\n",
    "<figure>\n",
    "<img src=\"attention-mask.svg\" width=\"940\">\n",
    "<figcaption>Figure 4: Attention Mask</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a742bda-0cf7-452d-96e8-de1abe52ed7a",
   "metadata": {},
   "source": [
    "Next, the $softmax$ term normalizes the attention weights across the keys before they're applied to $\\mathbf{V}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeec1f2-63cd-4bf9-8c11-75fa57379d7e",
   "metadata": {},
   "source": [
    "### Grouped Query Attention (GQA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed5ec8-c601-460e-8582-d868c0937d66",
   "metadata": {},
   "source": [
    "Each new token the model generates is compared against every key and value that came before. As context windows grow larger, the memory used by the key and value caches becomes a serious bottleneck. (Ainslie et al. 2023)\n",
    "\n",
    "To address this bottleneck, Llama 2 replaced the Multi-Head Attention (MHA) mechanism in Llama 1 with technique called Grouped Query Attention (GQA) (Ainslie et al. 2023). In MHA, each attention head has it's own set of queries, keys, and values. Earlier models such as PaLM (Chowdhery et al. 2022) tried replacing MHA with Multi-Query Attention (MQA) which shares a single set of keys and values across all the attention heads. But MQA didn't perform as well as hoped. GQA was designed as a trade-off between the two extremes of MHA and MQA where keys and values *are* shared across attention heads like MQA, but, instead of one group, Llama 3.1 uses 8 key / value heads.\n",
    "\n",
    "We'll see how GQA is implemented in the upcoming sections on splitting and combining attention heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb7fd7-e85f-4a0b-b3c3-acdd92fdfe73",
   "metadata": {},
   "source": [
    "### Rotary Position Embedding (RoPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271e2677-b9c2-43bf-aa20-841b6d7427e4",
   "metadata": {},
   "source": [
    "The relevance of one embedding to another is heavily influenced by the distance between them. This makes the embedding positions in the sequence critically important. However, if we recall our unknown attention function $f_A(\\mathbf{x}_m, \\mathbf{x}_n)$, you may notice the positions are conspicuously missing. This worked for early Transformers like Vanilla and BERT because they encoded the positions directly into the token embeddings $\\mathbf{x}_m$, $\\mathbf{x}_n$.\n",
    "\n",
    "More recent models including Llama have adopted *relative* position encoding schemes that have been shown to perform better especially on much longer sequences. Instead of baking the positions into the token embeddings, the idea is to explicitly add the positions $m$, $n$, to our attention function:\n",
    "\n",
    "$$\n",
    "f_A(\\mathbf{x}_m, \\mathbf{x}_n, m, n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf707b-b964-4f8c-b386-975c5d964c6f",
   "metadata": {},
   "source": [
    "To put this into practice, Llama uses an approach known as Rotary Position Embedding (RoPE) from Su et al. (2021). As we saw earlier, the attention mechanism relies on the angular distance between query and key vectors as a measure of fitness. RoPE intentionally takes advantage of this, converting distance between embedding positions into angular distance between embedding vectors.\n",
    "\n",
    "This is straightforward to visualize in 2-dimensions. The following diagram shows 2 embeddings $\\mathbf{x}_m$ and $\\mathbf{x}_n$ with positions $m$ and $n$ respectively. The idea of RoPE is to rotate $\\mathbf{x}_m$ a distance of $m \\theta$ and $\\mathbf{x}_n$ a distance of $n \\theta$, directly translating the distance in sequence space $(n - m)$ to a distance in vector space $(\\measuredangle{n} - \\measuredangle{m})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22d429-c695-4315-9d67-4278cb60564a",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"rope.svg\" width=\"940\">\n",
    "<figcaption>Figure 5: RoPE Concept in 2D</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88680fd7-41cd-47dd-8ef1-2ddff78ffe97",
   "metadata": {},
   "source": [
    "While the 2-dimensional concept is intuitive, implementing RoPE with $d_{model}$-dimensional vectors is a little more complicated. The complete RoPE algorithm involves several steps. To see what's happening, it helps to unpack and walk through them one at a time.\n",
    "\n",
    "Let's start by rotating a single embedding. Given an embedding $\\mathbf{x}$ with $d$ elements and position $m$, our goal is to rotate $\\mathbf{x}$ an angular distance of $m \\theta$. Unfortunately, there isn't an exact solution for this when $d$ > 2. Instead, RoPE approximates the idea by splitting $\\mathbf{x}$ into pairs $\\set{(x_0, x_1), (x_2, x_3), \\dots, (x_{d-2}, x_{d-1})}$ and then rotating each pair in 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3325fa-200d-45a0-aa85-a37d0229d106",
   "metadata": {},
   "source": [
    "Given $\\mathbf{x}$, $d$, $m$, $\\theta$, we can rotate $\\mathbf{x}$ an angular distance of $m \\theta$ by calculating $\\mathbf{R}\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{R} \\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta) & -sin(m \\theta) & 0 & 0 \\\\\n",
    "sin(m \\theta) & cos(m \\theta) & 0 & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & cos(m \\theta) & -sin(m \\theta) \\\\\n",
    "0 & 0 & sin(m \\theta) & cos(m \\theta) \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_{d-2} \\\\\n",
    "x_{d-1} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_0 \\cdot cos(m \\theta) - x_1 \\cdot sin(m \\theta) \\\\\n",
    "x_0 \\cdot sin(m \\theta) + x_1 \\cdot cos(m \\theta) \\\\\n",
    "\\vdots \\\\\n",
    "x_{d-2} \\cdot cos(m \\theta) - x_{d-1} \\cdot sin(m \\theta) \\\\\n",
    "x_{d-2} \\cdot sin(m \\theta) + x_{d-1} \\cdot cos(m \\theta) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463e9747-ad02-448d-8d31-b4f78e3a5988",
   "metadata": {},
   "source": [
    "Given the sparsity of $\\mathbf{R}$, we'll use the compact form recommended by Su et al. (2021). If you compare the compact form below with the right hand side above, you can see they achieve the same result.\n",
    "\n",
    "Given $\\mathbf{x}$, $d$, $m$, $\\theta$:\n",
    "\n",
    "$$\n",
    "\\mathbf{R} \\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_{d-2} \\\\\n",
    "x_{d-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta) \\\\\n",
    "cos(m \\theta) \\\\\n",
    "\\vdots \\\\\n",
    "cos(m \\theta) \\\\\n",
    "cos(m \\theta) \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "-x_1 \\\\\n",
    "x_0 \\\\\n",
    "\\vdots \\\\\n",
    "-x_{d-1} \\\\\n",
    "x_{d-2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix}\n",
    "sin(m \\theta) \\\\\n",
    "sin(m \\theta) \\\\\n",
    "\\vdots \\\\\n",
    "sin(m \\theta) \\\\\n",
    "sin(m \\theta) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad6debc-0bc1-4d7c-ae61-dcb15ef00233",
   "metadata": {},
   "source": [
    "Great. We've succeeded at rotating each of the pairs $\\set{(x_0, x_1), \\dots, (x_{d-2}, x_{d-1})}$ an angular distance $m \\theta$. However, RoPE takes position encoding a step further by varying the angular offset $\\theta$ across the pairs. The actual equation RoPE uses to rotate a single embedding is the following:\n",
    "\n",
    "Given $\\mathbf{x}$, $d$, $m$, $\\Theta$:\n",
    "\n",
    "$$\n",
    "\\mathbf{R} \\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_{d-2} \\\\\n",
    "x_{d-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_0) \\\\\n",
    "\\vdots \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "-x_1 \\\\\n",
    "x_0 \\\\\n",
    "\\vdots \\\\\n",
    "-x_{d-1} \\\\\n",
    "x_{d-2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix}\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_0) \\\\\n",
    "\\vdots \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\theta_i = \\frac{1}{\\Theta^{2i/d}}, i \\in [0, 1, \\dots, d/2-1]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f460ecbb-ad53-4e8c-bbb7-154921591710",
   "metadata": {},
   "source": [
    "Let's visualize $\\theta_i$ for $i = [0, d_{head}/2)$ to get a better sense for what's happening here. The following cell plots $\\theta_i$ using both the original $\\Theta = 10000$ from Su et al. (2021) and the $\\Theta = 500000$ from Llama 3.1. Surely the Llama authors must have had a reason to change it, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f504e292-3c66-4b1a-b5de-88110e33323b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'theta_i')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = torch.arange(config.d_head//2)\n",
    "\n",
    "thetas = 10000 ** (-2*i / config.d_head)\n",
    "sns.lineplot(x=i, y=thetas, label=f\"$\\Theta=10k$\")\n",
    "\n",
    "thetas = 500000 ** (-2*i / config.d_head)\n",
    "sns.lineplot(x=i, y=thetas, label=f\"$\\Theta=500k$\")\n",
    "\n",
    "plt.xlabel(\"i\")\n",
    "plt.ylabel(\"theta_i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d4d833-d245-40f8-9d15-768258ad9f9a",
   "metadata": {},
   "source": [
    "I'll admit this baffled me for a while. I could understand the rotation idea. *But why would you go through the trouble of varying $\\theta$ by what seems like such an arbitrary amount?*\n",
    "\n",
    "To understand the rationale, we need to step back for a moment and think about the big picture. Why are we using embeddings in the first place? At the end of the day, embeddings are feature vectors. Just like rows in a classical ML master table. Except they literally have thousands of columns. We can't, nor do we want to, know what each one represents. There are simply too many of them. But we do want to support them. Like seeds in a garden, we want to give them nutrient rich soil in which to flourish.\n",
    "\n",
    "If we rotate all the features by a uniform amount, we're restricting the nutrients in their diet. Some features will be sensitive to distance. The effect of these \"short-range\" or \"local\" features may taper off rapidly as the embeddings move apart. Other \"long-range\" or \"global\" features will be much less sensitive to distance.\n",
    "\n",
    "> Varying $\\theta$ establishes an environment that encourages a diverse population of short, medium, and long-range features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4706c2-6776-4808-baef-2007767b6d40",
   "metadata": {},
   "source": [
    "### Attention Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724fe600-802f-4465-a298-a11d6b6d3b74",
   "metadata": {},
   "source": [
    "Now that we've gone through all of the background, let's rewrite the attention equation one more time with all of the elements we need to calculate.\n",
    "    \n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{A} &= softmax\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}} + \\mathbf{M}\\right)\\mathbf{V} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "expands to\n",
    "\n",
    "<div class=\"small-math\">\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{A} &= softmax\\left(\\frac{(\\mathbf{R}\\mathbf{W}_Q\\mathbf{X})(\\mathbf{R}\\mathbf{W}_K\\mathbf{X})^T}{\\sqrt{d}} + \\mathbf{M}\\right)\\mathbf{W}_V\\mathbf{X} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "where\n",
    "\n",
    "* $\\mathbf{W}_Q$, $\\mathbf{W}_K$, $\\mathbf{W}_V$ are the linear projections for queries, keys, and values respectively.\n",
    "* $\\mathbf{R}\\mathbf{W}_Q\\mathbf{X}$, $\\mathbf{R}\\mathbf{W}_K\\mathbf{X}$ encode positions by rotating queries and keys respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c8c1b-4176-4c2c-8891-34869eefaece",
   "metadata": {},
   "source": [
    "The flowchart below enumerates the steps required to calculate $\\mathbf{A}$. Yes, there are a lot of steps but each one is tiny. My goal was to break the calculation down into small enough pieces that each one is only a few lines of code.\n",
    "\n",
    "<figure>\n",
    "<img src=\"attention-flow.svg\" width=\"700\">\n",
    "<figcaption>Figure 6: Attention Workflow</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ec00f-903d-451a-9eaf-20d703c44878",
   "metadata": {},
   "source": [
    "### Normalize Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fbfb22-ec40-4abe-bbb6-0bd535317b67",
   "metadata": {},
   "source": [
    "In the original Transformer architecture, the Attention and FFN blocks normalized the outputs of each sub-layer. In contrast, Llama normalizes the *inputs* to each sub-layer to improve training stability. Furthermore, Llama also replaces the standard LayerNorm algorithm with the RMSNorm algorithm designed by Zhang and Sennrich (2019) to be less computationally expensive and scale better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c78ae6a-358f-4c39-bacd-8ced26179ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention normalization\n",
    "normalize_attention = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(normalize_attention, \"normalize_attention\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ba28ad6-ee8d-456d-9906-b899f4fba272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preserve residuals\n",
    "residual = X\n",
    "\n",
    "# Normalize attention inputs\n",
    "X = normalize_attention(X)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbfb7ec-4bf4-43c2-8fa5-c5a83a71abf7",
   "metadata": {},
   "source": [
    "### Project Queries, Keys, Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74990d-7f9e-4ad5-8fc3-b9f8b4737b4c",
   "metadata": {},
   "source": [
    "Next, we'll configure and then apply the linear projections $\\mathbf{W}_Q$, $\\mathbf{W}_K$, $\\mathbf{W}_V$ that map input embeddings $\\mathbf{X}$ to query, key, and value subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5b9b235-9849-417d-97ab-ee9c467a4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure query, key, value projections\n",
    "W_q = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "W_k = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "W_v = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(W_q, \"w_q\", W_k, \"w_k\", W_v, \"w_v\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03a5a60e-f1d9-465a-ac74-5463fbb4b5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([22, 4096]), torch.Size([22, 1024]), torch.Size([22, 1024]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project embeddings to query, key, value spaces\n",
    "Q = W_q(X)\n",
    "K = W_k(X)\n",
    "V = W_v(X)\n",
    "\n",
    "Q.shape, K.shape, V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13001ff3-f5d0-47b9-813d-5afdf09bb4fe",
   "metadata": {},
   "source": [
    "### Split Attention Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2563ccd6-1df8-48eb-9634-c8b1e9c4a25e",
   "metadata": {},
   "source": [
    "Now that we've projected our embeddings to $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$, it's time to split up the attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba71ca0f-fce8-4f4f-8736-738d96b38af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, n_heads):    \n",
    "    return x.view(-1, n_heads, config.d_head).transpose(-3, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5106a5d7-071a-4b79-802d-485ca673fdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 22, 128]), torch.Size([8, 22, 128]), torch.Size([8, 22, 128]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split attention heads\n",
    "Q = split_heads(Q, config.n_heads)\n",
    "K = split_heads(K, config.n_kv_heads)\n",
    "V = split_heads(V, config.n_kv_heads)\n",
    "\n",
    "Q.shape, K.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "900097f7-416f-4b2b-bd61-bdb7b29c2b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 22, 128]), torch.Size([32, 22, 128]), torch.Size([32, 22, 128]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand key/value groups\n",
    "reps = config.n_heads // config.n_kv_heads\n",
    "K = K.repeat_interleave(reps, dim=0)\n",
    "V = V.repeat_interleave(reps, dim=0)\n",
    "\n",
    "Q.shape, K.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f90ed405-6816-45d9-b399-ab53a7ff19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert Q.shape == K.shape == V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b5210-9a86-4b85-8bda-e86d891b7cde",
   "metadata": {},
   "source": [
    "Let's take a quick moment to walk through what just happened. Starting with $\\mathbf{Q}$, we see the shape changed from $22 \\times 4096$ to $32 \\times 22 \\times 128$. The shape of $\\mathbf{K}$ and $\\mathbf{V}$ changed from $22 \\times 1024$ to $8 \\times 22 \\times 128$ and then changed again to $32 \\times 22 \\times 128$. But why? What is happening here?\n",
    "\n",
    "The goal of GQA is to split each embedding's query, key, and value representation into chunks, expand the key / value representations to match the queries, distribute each chunk to an attention head, and then eventually recombine everything on the other side. Figures 7 to 9 illustrate the GQA process. In the first step, we have a query representation with a dimension of 16, a key representation with a dimension of 4, and a value representation also with dimension 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea0b6b8-c079-4968-9f73-de8aacdae3c4",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"gqa_step1.svg\" width=\"940\">\n",
    "<figcaption>Figure 7: GQA - Step 1</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db50cc8-a1fa-4255-b60c-a44eb43f482b",
   "metadata": {},
   "source": [
    "Step 2 shows the representations after `split_heads`. Each color represents an attention head. We can see we have 8 query heads and 2 key/value heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defebf22-7b04-4231-9bfa-44c9a381e63c",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"gqa_step2.svg\" width=\"940\">\n",
    "<figcaption>Figure 8: GQA - Step 2</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1237433-69e7-47ac-97af-1e8afc70d7ad",
   "metadata": {},
   "source": [
    "Step 3 shows the representations after the key / value groups have been expanded. All of the attention heads have a full set of queries, keys, and values. If you look closely, you'll see the first group of attention heads all share the same keys and values $\\set{k_0, k_1, v_0, v_1}$. Similarly, the second group of attention heads all share the same keys and values $\\set{k_2, k_3, v_2, v_3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317d806-75b9-47e0-bbfa-c7f3a692abcf",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"gqa_step3.svg\" width=\"940\">\n",
    "<figcaption>Figure 9: GQA - Step 3</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c20ee16-c3d8-4f95-8c3c-55af9017f411",
   "metadata": {},
   "source": [
    "### Encode Positions\n",
    "\n",
    "Now that we've split queries, keys, and values into attention heads, the next step is to rotate the queries and keys using RoPE. Recall the compact, single embedding form of the RoPE transformation we looked at earlier:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1a332-cea9-4a8b-9359-682bd74d80bc",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{R} \\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_{d-2} \\\\\n",
    "x_{d-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_0) \\\\\n",
    "\\vdots \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "-x_1 \\\\\n",
    "x_0 \\\\\n",
    "\\vdots \\\\\n",
    "-x_{d-1} \\\\\n",
    "x_{d-2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix}\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_0) \\\\\n",
    "\\vdots \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d522b-704a-42de-abd8-922606120e72",
   "metadata": {},
   "source": [
    "We'll calculate the $cos$ and $sin$ vectors for every position $m$ and stack them in $n \\times d_{head}$ matrices $\\mathbf{R}_{cos}$ and $\\mathbf{R}_{sin}$.\n",
    "\n",
    "Given $\\Theta$, $d=d_{head}$, $n=\\text{sequence length}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{R}_{cos} &=\n",
    "\\begin{bmatrix}\n",
    "\\cos(0 \\theta_0) & \\cos(0 \\theta_0) & \\dots & \\cos(0 \\theta_{d/2-1}) & \\cos(0 \\theta_{d/2-1}) \\\\\n",
    "\\vdots & \\vdots & \\dots & \\vdots & \\vdots \\\\\n",
    "\\cos((n-1) \\theta_0) & \\cos((n-1) \\theta_0) & \\dots & \\cos((n-1) \\theta_{d/2-1}) & \\cos((n-1) \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\quad \\\\\n",
    "\\mathbf{R}_{sin} &=\n",
    "\\begin{bmatrix}\n",
    "\\sin(0 \\theta_0) & \\sin(0 \\theta_0) & \\dots & \\sin(0 \\theta_{d/2-1}) & \\sin(0 \\theta_{d/2-1}) \\\\\n",
    "\\vdots & \\vdots & \\dots & \\vdots & \\vdots \\\\\n",
    "\\sin((n-1) \\theta_0) & \\sin((n-1) \\theta_0) & \\dots & \\sin((n-1) \\theta_{d/2-1}) & \\sin((n-1) \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\theta_i = \\frac{1}{\\Theta^{2i/d}}, i \\in [0, 1, \\dots, d/2-1]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34ed9d03-7243-4637-a676-433d8615821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "base = config.rope_theta\n",
    "d = config.d_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d76f9178-eb5a-4931-8673-62ee0f15fada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate thetas\n",
    "i = torch.arange(d // 2, device=device)\n",
    "thetas = base ** (-2 * i / d)\n",
    "\n",
    "thetas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "880de2c7-62d6-411d-bb06-29437c604269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01, 2.9223e-01, 2.3805e-01, 1.9392e-01,\n",
       "        1.5797e-01, 1.2869e-01, 1.0483e-01, 8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,\n",
       "        2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03, 7.2927e-03, 5.9407e-03, 4.8394e-03,\n",
       "        3.9423e-03, 3.2114e-03, 2.6161e-03, 2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,\n",
       "        6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04, 1.8199e-04, 1.4825e-04, 1.2077e-04,\n",
       "        9.8381e-05, 8.0143e-05, 6.5286e-05, 5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,\n",
       "        1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06, 4.5417e-06, 3.6997e-06, 3.0139e-06,\n",
       "        2.4551e-06], device='mps:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad125a62-02d7-4b5e-a503-a5692644b9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate each theta, e.g. [theta_0, theta_1] -> [theta_0, theta_0, theta_1, theta_1]\n",
    "thetas = thetas.repeat_interleave(2)\n",
    "\n",
    "thetas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "092a7141-4f6e-4c86-a87c-3ebe27c75006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.0000e+00, 8.1462e-01, 8.1462e-01, 6.6360e-01, 6.6360e-01, 5.4058e-01, 5.4058e-01, 4.4037e-01,\n",
       "        4.4037e-01, 3.5873e-01, 3.5873e-01, 2.9223e-01, 2.9223e-01, 2.3805e-01, 2.3805e-01, 1.9392e-01, 1.9392e-01,\n",
       "        1.5797e-01, 1.5797e-01, 1.2869e-01, 1.2869e-01, 1.0483e-01, 1.0483e-01, 8.5397e-02, 8.5397e-02, 6.9566e-02,\n",
       "        6.9566e-02, 5.6670e-02, 5.6670e-02, 4.6164e-02, 4.6164e-02, 3.7606e-02, 3.7606e-02, 3.0635e-02, 3.0635e-02,\n",
       "        2.4955e-02, 2.4955e-02, 2.0329e-02, 2.0329e-02, 1.6560e-02, 1.6560e-02, 1.3490e-02, 1.3490e-02, 1.0990e-02,\n",
       "        1.0990e-02, 8.9523e-03, 8.9523e-03, 7.2927e-03, 7.2927e-03, 5.9407e-03, 5.9407e-03, 4.8394e-03, 4.8394e-03,\n",
       "        3.9423e-03, 3.9423e-03, 3.2114e-03, 3.2114e-03, 2.6161e-03, 2.6161e-03, 2.1311e-03, 2.1311e-03, 1.7360e-03,\n",
       "        1.7360e-03, 1.4142e-03, 1.4142e-03, 1.1520e-03, 1.1520e-03, 9.3847e-04, 9.3847e-04, 7.6450e-04, 7.6450e-04,\n",
       "        6.2277e-04, 6.2277e-04, 5.0732e-04, 5.0732e-04, 4.1327e-04, 4.1327e-04, 3.3666e-04, 3.3666e-04, 2.7425e-04,\n",
       "        2.7425e-04, 2.2341e-04, 2.2341e-04, 1.8199e-04, 1.8199e-04, 1.4825e-04, 1.4825e-04, 1.2077e-04, 1.2077e-04,\n",
       "        9.8381e-05, 9.8381e-05, 8.0143e-05, 8.0143e-05, 6.5286e-05, 6.5286e-05, 5.3183e-05, 5.3183e-05, 4.3324e-05,\n",
       "        4.3324e-05, 3.5292e-05, 3.5292e-05, 2.8750e-05, 2.8750e-05, 2.3420e-05, 2.3420e-05, 1.9078e-05, 1.9078e-05,\n",
       "        1.5542e-05, 1.5542e-05, 1.2660e-05, 1.2660e-05, 1.0313e-05, 1.0313e-05, 8.4015e-06, 8.4015e-06, 6.8440e-06,\n",
       "        6.8440e-06, 5.5752e-06, 5.5752e-06, 4.5417e-06, 4.5417e-06, 3.6997e-06, 3.6997e-06, 3.0139e-06, 3.0139e-06,\n",
       "        2.4551e-06, 2.4551e-06], device='mps:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "576a2edc-6bbe-4478-9e88-ae79446a89c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([22, 128]), torch.Size([22, 128]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rope_frequencies(n):\n",
    "    \"\"\"Compute RoPE cos and sin rotation matrices.\"\"\"\n",
    "    \n",
    "    # Repeat thetas for each position from 0 to n and stack in an (n, d_head) matrix\n",
    "    theta_stack = torch.stack([m*thetas for m in range(n)])\n",
    "    \n",
    "    # Apply cos, sin\n",
    "    r_cos = torch.cos(theta_stack)\n",
    "    r_sin = torch.sin(theta_stack)\n",
    "    \n",
    "    # Sanity check\n",
    "    assert r_cos.shape[0] == n and r_cos.shape[1] == config.d_head\n",
    "    assert r_sin.shape[0] == n and r_sin.shape[1] == config.d_head\n",
    "\n",
    "    return r_cos, r_sin\n",
    "\n",
    "# Compute cos and sin rotation matrices\n",
    "r_cos, r_sin = rope_frequencies(len(X))\n",
    "\n",
    "r_cos.shape, r_sin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30354ff0-d059-4913-bfe8-c24f2e365628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "        [ 0.5403,  0.5403,  0.6861,  ...,  1.0000,  1.0000,  1.0000],\n",
       "        [-0.4161, -0.4161, -0.0584,  ...,  1.0000,  1.0000,  1.0000],\n",
       "        ...,\n",
       "        [ 0.9887,  0.9887, -0.9736,  ...,  1.0000,  1.0000,  1.0000],\n",
       "        [ 0.4081,  0.4081, -0.8341,  ...,  1.0000,  1.0000,  1.0000],\n",
       "        [-0.5477, -0.5477, -0.1710,  ...,  1.0000,  1.0000,  1.0000]], device='mps:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0b91f94-ab1b-4d3d-bc9d-8e9a9d36c348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 8.4147e-01,  8.4147e-01,  7.2746e-01,  ...,  3.0139e-06,  2.4551e-06,  2.4551e-06],\n",
       "        [ 9.0930e-01,  9.0930e-01,  9.9829e-01,  ...,  6.0277e-06,  4.9103e-06,  4.9103e-06],\n",
       "        ...,\n",
       "        [ 1.4988e-01,  1.4988e-01,  2.2821e-01,  ...,  5.7263e-05,  4.6648e-05,  4.6648e-05],\n",
       "        [ 9.1295e-01,  9.1295e-01, -5.5168e-01,  ...,  6.0277e-05,  4.9103e-05,  4.9103e-05],\n",
       "        [ 8.3666e-01,  8.3666e-01, -9.8528e-01,  ...,  6.3291e-05,  5.1558e-05,  5.1558e-05]], device='mps:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_sin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf7ace-9da7-4b0c-b374-f29b60087129",
   "metadata": {},
   "source": [
    "Next, we'll define `rope_swap` to transform the embedding pairs to match the second column of $\\mathbf{R}$.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_{d-2} \\\\\n",
    "x_{d-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\mapsto\n",
    "\\begin{bmatrix}\n",
    "-x_1 \\\\\n",
    "x_0 \\\\\n",
    "\\vdots \\\\\n",
    "-x_{d-1} \\\\\n",
    "x_{d-2} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "238052a8-2d51-4012-87d2-22bd3bba65ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rope_swap(x):\n",
    "    \"\"\"Maps [x0, x1, x2, x3] -> [-x1, x0, -x3, x2].\"\"\"\n",
    "        \n",
    "    # Preserve original shape\n",
    "    s = x.shape\n",
    "\n",
    "    # Split into pairs, swap, and restore shape\n",
    "    x = x.reshape(-1, 2).flip(-1).view(s)\n",
    "\n",
    "    # Multiply every even index along the last dimension by -1\n",
    "    #   e.g. [x0, x1, x2, x3] -> [-x0, x1, -x2, x3]\n",
    "    x[..., ::2] *= -1\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc982b6-0382-4193-8172-41ad875102a7",
   "metadata": {},
   "source": [
    "We can finally combine all the pieces of the RoPE rotational transform!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e17ffd08-28af-4085-b015-866b896f5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rope_rotate(x, r_cos, r_sin):\n",
    "    \"\"\"Rotate embeddings using RoPE transform.\"\"\"\n",
    "    \n",
    "    return (x * r_cos) + (rope_swap(x) * r_sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82fb84aa-df57-47e6-860c-a29c429e4641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 22, 128]), torch.Size([32, 22, 128]), torch.Size([32, 22, 128]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode positions by rotating queries and keys\n",
    "Q = rope_rotate(Q, r_cos, r_sin)\n",
    "K = rope_rotate(K, r_cos, r_sin)\n",
    "\n",
    "Q.shape, K.shape, V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb2b562-517e-4d6e-afc8-a9c237d516ae",
   "metadata": {},
   "source": [
    "### Calculate Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757f219-5075-418c-aa40-da8c53b2bcc4",
   "metadata": {},
   "source": [
    "We've finally reached the attention equation! First, we'll compute the attention mask and then put all the pieces together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "943d4840-92df-45e2-8bca-5f187f3a57c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='mps:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute attention mask M\n",
    "n = len(X)\n",
    "mask = torch.ones(n, n, dtype=torch.bool, device=device).tril(diagonal=0)\n",
    "M = torch.zeros(n, n, device=device).masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2a39d5f-f2d3-47d8-81a6-bb086672425e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 22, 128])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute attention for all heads in parallel\n",
    "A = softmax(Q @ K.transpose(-2, -1) / np.sqrt(config.d_head) + M, dim=-1) @ V\n",
    "\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3145f-5cd7-491d-a4c5-a3a8adc4d076",
   "metadata": {},
   "source": [
    "### Recombine Attention Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2683dfed-bc0e-4a2c-b129-a42191a692d7",
   "metadata": {},
   "source": [
    "Next, we'll reassemble the attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d80b30d-50ee-4513-9ea3-9ff428dcad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_heads(x):\n",
    "    return x.transpose(-3, -2).contiguous().view(-1, int(config.n_heads * config.d_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90e4a2ec-12e4-4eaf-b657-ac91a22965fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine attention heads\n",
    "A = combine_heads(A)\n",
    "\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4d5595-1e05-4291-8bd8-9b33c2d929df",
   "metadata": {},
   "source": [
    "### Project Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e44b25f-76ed-43a6-929f-6817f735c09f",
   "metadata": {},
   "source": [
    "Now that we've calculated the attention representation, we need to project the attention representation back to embedding space before we combine with $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4991289-e866-4c2c-a1d8-3750e1e115c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention output projection\n",
    "attention_outputs = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(attention_outputs, \"attention_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ceadcd83-ed8e-4f9b-866b-82b415298ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project attention embeddings back to model space\n",
    "A = attention_outputs(A)\n",
    "\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130f43c-7246-4ba1-813d-566af8545f09",
   "metadata": {},
   "source": [
    "### Combine Outputs with Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0c943c2-3e22-4742-a0ce-84567abcff50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine attention embeddings with residuals\n",
    "X = residual + A\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a27df-3e8e-4e9e-a71b-82e28338170f",
   "metadata": {},
   "source": [
    "## Feedforward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f72504e-4642-454a-bf1e-9d6b299d4efe",
   "metadata": {},
   "source": [
    "### FFN Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6545d-12b2-4984-9ac1-3d3925d381cd",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"ffn-flow.svg\" width=\"940\">\n",
    "<figcaption>Figure 10: FFN Flow</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96852a87-ffc1-400a-9cc4-5f79a1853428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87b6a353-547a-47f7-8dec-667ce4c3608a",
   "metadata": {},
   "source": [
    "### Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83cab172-1b6d-4384-9811-e1c442de5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FFN normalization\n",
    "normalize_ffn = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained state\n",
    "llama.load_state(normalize_ffn, \"normalize_ffn\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c9dd0c0-afa0-4cc9-9a56-4bd0b6700842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preserve residuals\n",
    "residual = X\n",
    "\n",
    "# Normalize FFN inputs\n",
    "X = normalize_ffn(X)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258318e4-65b9-47ae-b934-244b9e2775ef",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab08bb80-e138-4ac8-bdc2-1885833ab302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SwiGLU FFN\n",
    "ffn_gates = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_ffn,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "ffn_inputs = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_ffn,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(ffn_gates, \"ffn_gates\", ffn_inputs, \"ffn_inputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7b82390-e39a-46d0-8475-f60c84af619b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 14336])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply transform\n",
    "F = silu(ffn_gates(X)) * ffn_inputs(X)\n",
    "\n",
    "F.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c881268-c8d4-4f6e-a5c9-e5d65765f13d",
   "metadata": {},
   "source": [
    "### Project Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b97a8505-687d-4123-8290-b3e270f5780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FFN output projection\n",
    "ffn_outputs = nn.Linear(\n",
    "    in_features=config.d_ffn,\n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(ffn_outputs, \"ffn_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad071436-a373-47ef-a434-bdf41bcbb91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project FFN embeddings back to model space\n",
    "F = ffn_outputs(F)\n",
    "\n",
    "F.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f97bd7-9de5-4903-9b7c-aab5412587a8",
   "metadata": {},
   "source": [
    "### Combine Outputs with Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ac86a4a-540d-437b-b8cc-8e24977f4417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine FFN embeddings with residuals\n",
    "X = residual + F\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c143495-e6ba-4998-99e9-a4ada9552636",
   "metadata": {},
   "source": [
    "## Stacking the Layers Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc90f22b-1171-49f1-b2d8-73569b16cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_layers(X):\n",
    "    # Compute cos and sin rotation matrices\n",
    "    r_cos, r_sin = rope_frequencies(len(X))\n",
    "\n",
    "    # Apply layer logic in a loop\n",
    "    for layer in range(config.n_layers):\n",
    "    \n",
    "        # Load pre-trained state for layer\n",
    "        load_pretrained_state(layer)\n",
    "    \n",
    "        #\n",
    "        # Attention\n",
    "        #\n",
    "    \n",
    "        # Normalize attention inputs\n",
    "        residual = X\n",
    "        X = normalize_attention(X)\n",
    "        \n",
    "        # Project embeddings to query, key, value spaces\n",
    "        Q = W_q(X)\n",
    "        K = W_k(X)\n",
    "        V = W_v(X)\n",
    "        \n",
    "        # Split attention heads\n",
    "        Q = split_heads(Q, config.n_heads)\n",
    "        K = split_heads(K, config.n_kv_heads)\n",
    "        V = split_heads(V, config.n_kv_heads)\n",
    "        \n",
    "        # Expand key/value groups\n",
    "        reps = config.n_heads // config.n_kv_heads\n",
    "        K = K.repeat_interleave(reps, dim=0)\n",
    "        V = V.repeat_interleave(reps, dim=0)\n",
    "        \n",
    "        # Encode positions by rotating queries and keys\n",
    "        Q = rope_rotate(Q, r_cos, r_sin)\n",
    "        K = rope_rotate(K, r_cos, r_sin)\n",
    "    \n",
    "        # Compute masked attention bias M\n",
    "        n = len(X)\n",
    "        mask = torch.ones(n, n, dtype=torch.bool, device=device).tril(diagonal=0)\n",
    "        M = torch.zeros(n, n, device=device).masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "        \n",
    "        # Compute attention for all heads in parallel\n",
    "        A = softmax(Q @ K.transpose(-2, -1) / np.sqrt(config.d_head) + M, dim=-1) @ V\n",
    "    \n",
    "        # Combine attention heads\n",
    "        A = combine_heads(A)\n",
    "        \n",
    "        # Project attention embeddings back to model space\n",
    "        A = attention_outputs(A)\n",
    "        \n",
    "        # Combine attention embeddings with residuals\n",
    "        X = residual + A\n",
    "        \n",
    "        #\n",
    "        # FFN\n",
    "        #\n",
    "    \n",
    "        # Normalize FFN inputs\n",
    "        residual = X\n",
    "        X = normalize_ffn(X)\n",
    "    \n",
    "        # Apply transform\n",
    "        F = silu(ffn_gates(X)) * ffn_inputs(X)\n",
    "    \n",
    "        # Project FFN embeddings back to model space\n",
    "        F = ffn_outputs(F)\n",
    "        \n",
    "        # Combine FFN embeddings with residuals\n",
    "        X = residual + F\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f585aa5-03e2-42c4-8582-0200d092e281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start over from initial tokens\n",
    "X = torch.tensor(token_ids, device=device)\n",
    "\n",
    "# Initial embeddings\n",
    "X = embeddings(X)\n",
    "\n",
    "# Contextualized embeddings\n",
    "X = context_layers(X)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83d86826-0a33-4a43-90c0-a3eae526b5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8842,  1.9047,  1.0641,  ..., -1.3221,  2.1526,  1.3637],\n",
       "        [ 0.5709, -0.4375, -0.1361,  ..., -0.0925, -0.2379, -0.1356],\n",
       "        [ 0.6849, -0.0598, -0.1049,  ...,  0.9282,  1.0530, -0.8878],\n",
       "        ...,\n",
       "        [-0.3983,  0.5822, -0.3068,  ...,  0.2404,  0.1521, -0.5723],\n",
       "        [-0.7122, -0.2405, -0.3738,  ...,  0.3690,  0.6642, -1.1241],\n",
       "        [-0.3904,  0.7667,  0.6280,  ...,  0.6883,  1.9776, -0.1630]], device='mps:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1603550e-f8c5-44fc-b8d1-19428aeff5b5",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d29083-7ea8-41cf-9299-1466c5f5014a",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"head-flow.svg\" width=\"700\">\n",
    "<figcaption>Figure 11: Head Flow</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ea5d20-7fe6-4e71-bca1-4aef72dc803d",
   "metadata": {},
   "source": [
    "### Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f030e10b-a7e1-4df9-8460-ece7d8d4bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure head normalization\n",
    "normalize_head = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(normalize_head, \"normalize_head\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38cae1a9-25b7-43a3-9134-1fc7bce64f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize head inputs\n",
    "X = normalize_head(X)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9066d-73ab-48d0-9185-469f86f7839b",
   "metadata": {},
   "source": [
    "### Project Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67c44655-5796-40f6-bbb1-300a1b6a86a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure output projection\n",
    "head_outputs = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.vocab_size,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(head_outputs, \"head_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09534e41-6d49-416a-b2a9-92ff4c402d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128256])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use last embedding to represent the entire sequence\n",
    "X = X[-1]\n",
    "\n",
    "# Project outputs to token space\n",
    "X = head_outputs(X)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb78c4-2e9e-430e-9c6e-bfd1e882e6fb",
   "metadata": {},
   "source": [
    "## Top Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "528773e0-0d4b-4641-8998-c0693105d4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boston'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select top scoring token\n",
    "token_id = X.argmax()\n",
    "\n",
    "# Decode token\n",
    "token = tokenizer.decode([token_id]).strip()\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf5b84b5-3fce-4f46-8d39-867eec194cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify answer\n",
    "assert token == \"Boston\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37cf2c-ceb5-4420-b2dd-0cc6e1c5df93",
   "metadata": {},
   "source": [
    "## Sample Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db6efc-ee1f-457d-834c-1577338c18c8",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8eeb0fff-2f5e-4b0e-a2bf-b1861129bf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "temperature = config.temperature\n",
    "temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b3879e58-b9d7-451d-b3da-a4387ad48ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply temperature\n",
    "X = X / temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366bad75-488b-4951-999a-457d2f75e69e",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6f49c1ed-a77d-4ec3-ba14-908e69452edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to probabilities\n",
    "probs = softmax(X)\n",
    "\n",
    "# Sort probabilities in descending order\n",
    "probs, indices = probs.sort(descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1108c86-6859-45d8-88ae-dae902d7b22a",
   "metadata": {},
   "source": [
    "### Top K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d50e33b2-17c8-4f86-a540-b4d0154c34ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "top_k = config.top_k\n",
    "top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc1f1989-6a8e-45a6-a791-975fd0cd4bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 50 of 128256\n"
     ]
    }
   ],
   "source": [
    "# Retain top k tokens\n",
    "probs = probs[:top_k]\n",
    "print(f\"Retained {len(probs)} of {len(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec20b139-494f-47cb-b772-3c63c3de9211",
   "metadata": {},
   "source": [
    "### Top P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "de639c47-170e-41a2-bf19-21ecff0992e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "top_p = config.top_p\n",
    "top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c052e39d-0dde-43fd-8a19-dbbb6d23af8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 1 of 128256\n"
     ]
    }
   ],
   "source": [
    "# Find cutoff where cumulative probability exceeds top_p\n",
    "cumulative_mask = probs.cumsum(dim=-1) > top_p\n",
    "threshold_index = torch.argmax(cumulative_mask).item()\n",
    "\n",
    "# Only apply threshold if top_p was exceeded\n",
    "if cumulative_mask.any():\n",
    "    probs = probs[:threshold_index+1]\n",
    "\n",
    "print(f\"Retained {len(probs)} of {len(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dea82f-bf61-434e-849f-ef5bce79f9cb",
   "metadata": {},
   "source": [
    "### Random Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3aed89c4-95e3-4431-aa13-ac507c982cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token id 65432, token 'Boston', score 1.000\n"
     ]
    }
   ],
   "source": [
    "# Print remaining token pool\n",
    "for i, prob in enumerate(probs):\n",
    "    print(f\"token id {indices[i]}, token '{tokenizer.decode([indices[i]])}', score {prob:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e39fde30-3a77-4752-941f-ced72393c8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boston'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample from remaining tokens weighted by probability\n",
    "sampled_index = torch.multinomial(probs, 1)\n",
    "\n",
    "# Convert sampled_index to original logits\n",
    "token_id = indices[sampled_index]\n",
    "\n",
    "# Decode token\n",
    "token = tokenizer.decode([token_id]).strip()\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c8acd-fe53-4c1f-8080-1d6bfaf0b027",
   "metadata": {},
   "source": [
    "## Complete Head Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f696ef95-7597-4f2b-bfd1-835956bb7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def head(X):\n",
    "    # Normalize head inputs\n",
    "    X = normalize_head(X)\n",
    "    \n",
    "    # Use last embedding to represent the entire sequence\n",
    "    X = X[-1]\n",
    "    \n",
    "    # Project outputs to token space\n",
    "    X = head_outputs(X)\n",
    "\n",
    "    #\n",
    "    # Temperature\n",
    "    #\n",
    "    \n",
    "    # Apply temperature\n",
    "    X = X / config.temperature\n",
    "\n",
    "    #\n",
    "    # Ranking\n",
    "    #\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probs = softmax(X)\n",
    "    \n",
    "    # Sort probabilities in descending order\n",
    "    probs, indices = probs.sort(descending=True)\n",
    "\n",
    "    #\n",
    "    # Top K\n",
    "    #\n",
    "    \n",
    "    # Retain top k tokens\n",
    "    probs = probs[:config.top_k]\n",
    "\n",
    "    #\n",
    "    # Top P\n",
    "    #\n",
    "    \n",
    "    # Find cutoff where cumulative probability exceeds top_p\n",
    "    cumulative_mask = probs.cumsum(dim=-1) > config.top_p\n",
    "    threshold_index = torch.argmax(cumulative_mask).item()\n",
    "    \n",
    "    # Only apply threshold if top_p was exceeded\n",
    "    if cumulative_mask.any():\n",
    "        probs = probs[:threshold_index+1]\n",
    "\n",
    "    #\n",
    "    # Random Selection\n",
    "    #\n",
    "    \n",
    "    # Sample from remaining tokens weighted by probability\n",
    "    sampled_index = torch.multinomial(probs, 1)\n",
    "    \n",
    "    # Convert sampled_index to original logits\n",
    "    token_id = indices[sampled_index]\n",
    "\n",
    "    return token_id.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568352c3-2573-440f-9c1e-ef0acccd245b",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2328afe7-d40a-4d47-91cd-be59b02b4485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3c9533b1-7011-4f84-85b5-51ca93b15b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@validate_call\n",
    "def prepare_messages(messages: list[Message]):\n",
    "    # Initialize prompt\n",
    "    prompt = \"\"\n",
    "    \n",
    "    # Format each message\n",
    "    for message in messages:\n",
    "        prompt += f\"<|start_header_id|>{message.role}<|end_header_id|>\\n\\n\"\n",
    "        prompt += message.content\n",
    "        prompt += \"<|eot_id|>\"\n",
    "\n",
    "    # Finish with the assistant role to prime the model's response\n",
    "    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9b875c2e-a1ac-45dd-8df3-02398fa5c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@validate_call\n",
    "def generate(messages: list[Message]):\n",
    "    # Format message prompt\n",
    "    prompt = prepare_messages(messages)\n",
    "    \n",
    "    # Split raw text into tokens\n",
    "    token_ids = tokenizer.encode(prompt, bos=True, eos=False, allowed_special=\"all\")\n",
    "    \n",
    "    # Generate output until we get a stop token or we exceed max_output_tokens.\n",
    "    for _ in range(config.max_output_tokens):\n",
    "        \n",
    "        # Start over from initial tokens\n",
    "        X = torch.tensor(token_ids, device=device)\n",
    "        \n",
    "        # Initial embeddings\n",
    "        X = embeddings(X)\n",
    "        \n",
    "        # Semantic embeddings\n",
    "        X = context_layers(X)\n",
    "        \n",
    "        # Head\n",
    "        token_id = head(X)\n",
    "        \n",
    "        # Check stopping criteria\n",
    "        if token_id in tokenizer.stop_tokens:\n",
    "            break\n",
    "    \n",
    "        # Print token\n",
    "        token = tokenizer.decode([token_id])\n",
    "        stdout.write(token)\n",
    "        \n",
    "        # Append to end of sequence\n",
    "        token_ids.append(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "34e8bf24-cb98-4618-a481-5712d9aa5477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Massachusetts is Boston."
     ]
    }
   ],
   "source": [
    "generate([\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the capital of Massachusetts?\",\n",
    "    },\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d4e0f567-921c-4a45-b394-10d92ee31dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden leaves descend\n",
      "Crimson maples stand tall still\n",
      "Autumn's fleeting dance"
     ]
    }
   ],
   "source": [
    "generate([\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a haiku about fall in New England.\",\n",
    "    },\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7c054-8863-4b24-83ba-24415c78a553",
   "metadata": {},
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf596e24-2e31-47f9-9bfc-be0731542b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725cb1fc-7c58-4ca9-a55e-929a125eefea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faee9494-3ca7-499d-b072-d71b87fbbe33",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b07abe-b5ad-4452-9067-ea9ee1e56982",
   "metadata": {},
   "source": [
    "Ainslie, Joshua, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. “GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.” arXiv. <https://doi.org/10.48550/arXiv.2305.13245>.\n",
    "\n",
    "Bengio, Yoshua, Réjean Ducharme, and Pascal Vincent. 2000. “A Neural Probabilistic Language Model.” In Advances in Neural Information Processing Systems. Vol. 13. MIT Press. <https://proceedings.neurips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html>.\n",
    "\n",
    "Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” arXiv. <https://doi.org/10.48550/arXiv.2005.14165>.\n",
    "\n",
    "Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv.Org. May 24, 2019. <https://arxiv.org/abs/1810.04805v2>.\n",
    "\n",
    "Dubey, Abhimanyu, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, et al. 2024. “The Llama 3 Herd of Models.” arXiv.Org. July 31, 2024. <https://arxiv.org/abs/2407.21783v2>.\n",
    "\n",
    "He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” arXiv. <https://doi.org/10.48550/arXiv.1512.03385>.\n",
    "\n",
    "Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. “Improving Language Understanding by Generative Pre-Training.”\n",
    "\n",
    "Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. “Neural Machine Translation of Rare Words with Subword Units.” arXiv. <https://doi.org/10.48550/arXiv.1508.07909>.\n",
    "\n",
    "Shazeer, Noam. 2020. “GLU Variants Improve Transformer.” arXiv. <https://doi.org/10.48550/arXiv.2002.05202>.\n",
    "\n",
    "Stanford Online, dir. 2024. Stanford CS25: V4 I Hyung Won Chung of OpenAI. <https://www.youtube.com/watch?v=orDKvo8h71o>.\n",
    "\n",
    "Su, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2021. “RoFormer: Enhanced Transformer with Rotary Position Embedding.” arXiv.Org. April 20, 2021. <https://arxiv.org/abs/2104.09864v5>.\n",
    "\n",
    "Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, et al. 2023. “LLaMA: Open and Efficient Foundation Language Models.” arXiv. <https://doi.org/10.48550/arXiv.2302.13971>.\n",
    "\n",
    "Touvron, Hugo, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et al. 2023. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” arXiv.Org. July 18, 2023. <https://arxiv.org/abs/2307.09288v2>.\n",
    "\n",
    "Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” arXiv. <https://doi.org/10.48550/arXiv.1706.03762>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bffd81b-306c-43f5-b229-c63095eee3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "stickshift": {
   "description": "Trace an Inference Through Each Layer of the SOTA Llama 3.1 Foundation Models",
   "draft": true,
   "title": "Transformer Teardown: Llama 3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
