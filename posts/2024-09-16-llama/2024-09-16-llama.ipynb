{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f5fa8b-9842-4be3-8cdc-3421d5019662",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "source": [
    "# Transformer Teardown: Llama 3.1\n",
    "\n",
    "> Trace an Inference Through Each Layer of the SOTA Llama 3.1 Foundation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924b74d-61d7-4cfc-a3c0-5c333bec5659",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In [the last Transformer Teardown](https://stickshift.github.io/2024/09/04/transformer-teardown.html), we dissected a DistilBERT text classification pipeline, tracing a single inference through the entire stack from raw data to final prediction. We learned about the main stages of a Transformer pipeline as well as fundamental Transformer concepts such as token embeddings and Multi-Head Self Attention. Studying BERT-based text classification models is a fantastic way to see the basic Transformer machinery in action. But BERT was published in 2018! It would be another 4 years before ChatGPT launched and Generative AI exploded onto the scene.\n",
    "\n",
    "In this Transformer Teardown, we're going to fast forward to present day. We'll use the same teardown process to unpack the state-of-the-art [Llama 3.1](https://llama.meta.com/) open source foundation models released by Meta in July. We'll walk through each step of a text generation pipeline one cell at a time, tracing an inference from raw text to the first output token. We'll illustrate the main ideas from the latest Transformer literature with minimal, straightforward, working Python code, giving you a close-up view of the core mechanisms driving the Generative AI revolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c3be3e-78e2-47f4-8714-d6613bea7a4c",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "040ef74f-fccb-4a91-9748-b3eba0398a0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sys import stdout\n",
    "from textwrap import dedent\n",
    "import warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas import Series\n",
    "from pydantic import BaseModel, validate_call\n",
    "from pytest import approx\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import relu, silu, softmax\n",
    "\n",
    "from llama_models.llama3.reference_impl.model import RMSNorm\n",
    "\n",
    "import stickshift as ss\n",
    "from stickshift import default_arg, take\n",
    "from stickshift.torch import device as torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8523438f-2c81-474b-bab5-e9a707fcc572",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure gpu\n",
    "device = torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca4d3dd0-eff4-4090-8673-9810d87b1b74",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "figure > img {\n",
       "    display:block;\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "figcaption {\n",
       "    text-align: center;\n",
       "}\n",
       "blockquote {\n",
       "    margin-top: 2.0rem !important;\n",
       "    margin-bottom: 2.0rem !important;\n",
       "    margin-left: 0 !important;\n",
       "    margin-right: 0 !important;\n",
       "    padding: 1.0rem !important;\n",
       "    background-color: rgba(0,0,0,0.05) !important;\n",
       "    border: 1px solid rgba(0,0,0,0.1) !important;\n",
       "    font-style: italic !important;\n",
       "}\n",
       "blockquote p {\n",
       "    margin: 0 !important;\n",
       "    padding: 0 !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "figure > img {\n",
    "    display:block;\n",
    "    margin-left: auto !important;\n",
    "    margin-right: auto !important;\n",
    "}\n",
    "figcaption {\n",
    "    text-align: center;\n",
    "}\n",
    "blockquote {\n",
    "    margin-top: 2.0rem !important;\n",
    "    margin-bottom: 2.0rem !important;\n",
    "    margin-left: 0 !important;\n",
    "    margin-right: 0 !important;\n",
    "    padding: 1.0rem !important;\n",
    "    background-color: rgba(0,0,0,0.05) !important;\n",
    "    border: 1px solid rgba(0,0,0,0.1) !important;\n",
    "    font-style: italic !important;\n",
    "}\n",
    "blockquote p {\n",
    "    margin: 0 !important;\n",
    "    padding: 0 !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58287922-dc47-44d9-a2c6-77bd0c8abb7f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61da974-fb00-438a-bb44-f3a432182478",
   "metadata": {},
   "source": [
    "# Llama Foundation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d95b5-8ee7-45bd-9d2d-7908012ac587",
   "metadata": {},
   "source": [
    "[Llama](https://llama.meta.com/) is a family of general purpose, state-of-the-art open source foundation models from Meta. According to the 3.1 technical report, the latest models can \"answer questions in at least 8 languages, write high quality code, solve complex reasoning problems, and use tools in a zero-shot way.\" (Dubey et al. 2024) The Llama 3.1 release includes 8B, 70B, and 405B sizes. While you need a multi-GPU cluster to run the 70B and 405B sizes, the 8B model is small enough to experiment with on a laptop. Not only did Meta release the pre-trained model checkpoints for all 3 sizes, they also published a fantastically detailed, [70 page technical report](https://arxiv.org/abs/2407.21783v2) as well as a complete [reference implementation](https://github.com/meta-llama/llama-models). Together, Llama 3.1 represents both a tremendous contribution to the AI community as well as an incredible learning opportunity to study the inner workings of a modern frontier model.\n",
    "\n",
    "Over the course of this post, we'll implement a complete text generation pipeline using only the research literature, pre-trained weights from the `Meta-Llama3.1-8B-Instruct` checkpoint, and Meta's reference implementation as a guide. After we load the 8B checkpoint, we'll review the stages of an end-to-end, text generation pipeline. In the sections that follow, we'll walk through a detailed teardown of each stage—tracing an inference from raw data to the first output token. In the last section, we'll put all the pieces together into a complete generative Transformer capable of producing long form content.\n",
    "\n",
    "Let the teardown begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a3d63-e556-4845-9c2f-cea2d0712d27",
   "metadata": {},
   "source": [
    "# Model Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb04ecc-6f9a-4e2a-9c23-7ae89a702f31",
   "metadata": {},
   "source": [
    "We'll start by loading the configuration and pre-trained weights for the `Meta-Llama3.1-8B-Instruct` checkpoint. The \"instruct\" versions of the Llama models include the raw pre-training and substantial post-training to support user and assistant interactions and complex tool-calling scenarios. The weights for all Llama checkpoints can be downloaded directly from [Meta](https://llama.meta.com/), [Hugging Face](https://huggingface.co/meta-llama), and [Kaggle](https://www.kaggle.com/organizations/metaresearch/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07362fc6-41cf-45b4-8d88-e8c55aa9ce9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checkpoint_path': PosixPath('/Users/andrewyoung/.llama/checkpoints/Meta-Llama3.1-8B-Instruct'), 'vocab_size': 128256, 'd_model': 4096, 'd_head': 128, 'd_ffn': 14336, 'n_layers': 32, 'n_heads': 32, 'n_kv_heads': 8, 'n_kv_groups': 4, 'rms_norm_eps': 1e-05, 'rope_theta': 500000.0, 'max_seq_len': 8192, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9, 'max_output_tokens': 500}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import custom utilities\n",
    "from stickshift.models import llama\n",
    "\n",
    "# Load model config\n",
    "config = llama.config(\"Meta-Llama3.1-8B-Instruct\")\n",
    "\n",
    "# Load pre-trained model parameters\n",
    "checkpoint = torch.load(\n",
    "    config.checkpoint_path / \"consolidated.00.pth\", \n",
    "    weights_only=True, \n",
    "    map_location=device,\n",
    ")\n",
    "\n",
    "config.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bc3a1e-91c4-4fec-896f-2f5ee6977915",
   "metadata": {},
   "source": [
    "We'll reference a number of the settings in `config` throughout the teardown. For now, a few interesting ones to note are `d_model`, `d_ffn`, `n_layers`, and `n_heads`. These represent the primary differences between the 8B, 70B, and 405B sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6acaf5d-b853-46fa-86e1-d63030074dbd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_pretrained_state(layer):    \n",
    "    # Load pre-trained state\n",
    "    llama.load_state(\n",
    "        normalize_attention, \"normalize_attention\", \n",
    "        normalize_ffn, \"normalize_ffn\", \n",
    "        w_q, \"w_q\", \n",
    "        w_k, \"w_k\", \n",
    "        w_v, \"w_v\", \n",
    "        attention_outputs, \"attention_outputs\",\n",
    "        ffn_gates, \"ffn_gates\",\n",
    "        ffn_inputs, \"ffn_inputs\",\n",
    "        ffn_outputs, \"ffn_outputs\",\n",
    "        checkpoint=checkpoint,\n",
    "        layer=layer,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb0698a-439d-44ad-959c-33b2a19a9593",
   "metadata": {},
   "source": [
    "# Text Generation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c8069-1148-48a7-bb07-a91f60b43a3f",
   "metadata": {},
   "source": [
    "In [the last teardown](https://stickshift.github.io/2024/09/04/transformer-teardown.html), we looked at a text classification Transformer. This time we're going to dissect a *text generation* Transformer. Instead of simply applying a label to the input text, the Head stage will be responsible for *generating* new content. But don't worry! It's not as complicated as it sounds.\n",
    "\n",
    "Figure X illustrates the stages in a text generation pipeline. It's very similar to the text classification pipeline we looked at last time. The Tokenize stage splits raw text into a sequence of tokens. The Embeddings stage maps the sequence of tokens to a sequence of embedding vectors. The Context Layers augment the embeddings with contextual signals drawn from the surrounding tokens, transforming individual token embeddings into contextualized \"semantic embeddings\". Finally, the Head stage converts the semantic embeddings into predictions. The key difference is, instead of predicting a label for the raw text, text generation Transformers *predict the next token*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc06ac-b312-491b-98b6-ed82d237b6cd",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"transformer-pipeline.svg\" width=\"940\">\n",
    "<figcaption>Figure X: Text Generation Pipeline</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe17de4c-55b7-48d3-8f78-43874b3c4bb4",
   "metadata": {},
   "source": [
    "But one token is just the beginning! The magical powers of Generative AI are manifested by simply running the token predictions in a loop. The predicted token in each iteration is appended to the end of the input sequence, and the process repeats. Over and over again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95117d8e-7ea3-4c2a-ad4a-1bd535771a5f",
   "metadata": {},
   "source": [
    "# Raw Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacc9c84-f587-49e4-85d5-e89ed78fa9db",
   "metadata": {},
   "source": [
    "Before we can tear anything down, we need a prompt. Since our goal is to trace an inference from raw text to the first output token, we want to start with a prompt that's specific enough to generate a consistent, one-word answer. If we do everything right, the first output token we predict should be \"Boston\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9832bc2-033e-4c48-903f-f05544c135eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "prompt += \"What is the capital of Massachusetts? Answer in one word.\"\n",
    "prompt += \"<|eot_id|>\"\n",
    "prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5ae4e-3b6d-4b71-9eed-a2dd512cf8f9",
   "metadata": {},
   "source": [
    "You can see `prompt` includes a number of special tokens. These would usually be injected by a framework like Hugging Face's `transformers`. We need to manually inject them because we're working with the model directly. You can find more information on the Llama 3.1 prompt syntax in the [Llama Prompting Guide](https://www.llama.com/docs/how-to-guides/prompting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0e94d-a969-43b8-bfad-2ceaa10ca7aa",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f879aa-2352-48a8-bc02-60adcd1d058a",
   "metadata": {},
   "source": [
    "The Tokenize stage splits raw text into a sequence of tokens using a fixed vocabulary. Llama uses a vocabulary of 128k tokens built on top of OpenAI's tiktoken tokenizer. We'll dig into the gory details in the later stages, but here we'll simply use the off-the-shelf Tokenizer from Meta's llama-models reference implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9605a1b6-beb8-4f7f-8ad2-67351929a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_models.llama3.api.tokenizer import Tokenizer\n",
    "\n",
    "# Load tokenizer model from checkpoint\n",
    "tokenizer = Tokenizer(str(config.checkpoint_path / \"tokenizer.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07c6b94b-36dc-43b0-8dc8-e99c28d26d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 128006, 882, 128007, 271, 3923, 374, 279, 6864, 315, 22108, 30, 22559, 304, 832, 3492, 13, 128009, 128006, 78191, 128007, 271]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split raw text into tokens\n",
    "token_ids = tokenizer.encode(prompt, bos=True, eos=False, allowed_special=\"all\")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bfa2427-7eba-4c68-b161-fc8238d4c2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e067e-95ba-4c7c-aef6-0e6c031057fa",
   "metadata": {},
   "source": [
    "We see `tokenizer.encode` split our prompt into 22 token ids. These ids represent the index of each token in Llama's 128k token vocabulary. We can always reverse the process with `tokenizer.decode`. If you look closely at the cell output below, you'll notice the tokenizer injected another special token `(128000, '<|begin_of_text|>')` to mark the beginning of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73c0e958-3d09-4f4e-8269-2cf7a1a39bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the capital of Massachusetts? Answer in one word.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode token ids back into raw text\n",
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "040cffac-921e-43ad-ac2b-069895233360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load token_ids into a tensor\n",
    "x = torch.tensor(token_ids, device=device)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f60316-a506-4912-b8ac-8bbaa83a4d7f",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccd09b-1a58-40e7-98f4-4794ea681c98",
   "metadata": {},
   "source": [
    "Embeddings are a key component of the Transformer architecture. They're also abstract mathematical structures that can be difficult to wrap your head around. To illustrate the crucial role embeddings play, let's use a quick metaphor.\n",
    "\n",
    "> If a Transformer was a brain, then embeddings would be the electrical signals carrying information through the brain.\n",
    "\n",
    "Continuing with the metaphor, the Embeddings stage of the pipeline would be your sensory organs where light rays and air vibrations are translated into electrical impulses. Token embeddings would be the fresh sensory percepts. Semantic embeddings would be the abstract thoughts at the top of the cortical stack. The idea of percepts traveling up the cortical stack is a perfect analogy for token embeddings traveling through the Transformer layers.\n",
    "\n",
    "Implementing Llama's Embeddings stage is relatively straightforward. We'll use a lookup table with a unique embedding for each of the 128k tokens in the vocabulary. Each embedding is a vector with $d_{model}$ elements that were randomly generated and then learned during training. Given a sequence of token ids, the lookup table returns their embeddings as row vectors stacked in an $n \\times d_{model}$ tensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811d333-dc1c-4177-950a-56ced36ab246",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"embeddings.svg\" width=\"940\">\n",
    "<figcaption>Figure X: Learned Token Embeddings</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43f02c0c-6073-401e-9f8e-b6f5159e7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings lookup table\n",
    "embeddings = nn.Embedding(\n",
    "    num_embeddings=config.vocab_size, \n",
    "    embedding_dim=config.d_model,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "llama.load_state(embeddings, \"embeddings\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd113f37-d587-45e0-aecd-5ddc7b82144a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map token ids to embeddings\n",
    "x = embeddings(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46771ed3-4483-4a7b-ade5-25179d13feba",
   "metadata": {},
   "source": [
    "We can see from `x.shape` that we successfully mapped the 22 token ids to 22 token embeddings stacked in an $n \\times d_{model}$ tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5da73e42-c82a-4469-bdf5-fe04e51bae6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
       "          6.3419e-05,  1.1902e-03],\n",
       "        [-1.6499e-04, -2.4319e-04,  1.6403e-04,  ..., -1.5163e-04,\n",
       "          3.5095e-04,  7.3242e-04],\n",
       "        [ 3.5095e-03,  7.2021e-03,  5.3406e-05,  ..., -7.2479e-04,\n",
       "         -1.0620e-02,  8.2779e-04],\n",
       "        ...,\n",
       "        [-9.7656e-03, -3.4637e-03,  1.8616e-03,  ..., -7.1411e-03,\n",
       "         -4.3030e-03,  8.6060e-03],\n",
       "        [-4.6158e-04, -3.9291e-04, -6.5863e-06,  ..., -6.2561e-04,\n",
       "         -5.0354e-04,  6.6757e-04],\n",
       "        [-2.8687e-03,  3.8910e-03, -1.7357e-04,  ...,  8.0872e-04,\n",
       "          5.0354e-04,  2.3041e-03]], device='mps:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show sample\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc36c605-3d13-4d36-98a0-021005ad0f75",
   "metadata": {},
   "source": [
    "Before we move on, a quick note on terminology. If you've used cloud-based LLM APIs like OpenAI or LangChain, you're likely familiar with the term \"embedding model\". An *embedding model* is really a combination of a tokenizer and embeddings table. These are often bundled together to give you everything you need to convert raw text into embedding vectors and can be used for a number of things independent of the LLM.\n",
    "\n",
    "Now that we've converted our raw text into token embeddings, it's time to start transforming!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab57f0-610c-46e5-b372-532c41699d43",
   "metadata": {},
   "source": [
    "# Context Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f094441-b5d6-48be-a3ff-63af16a8d9a6",
   "metadata": {},
   "source": [
    "Context layers are where the Transformer magic happens. Collectively, the Context Layers are responsible for transforming a sequence of token embeddings into a sequence of semantic embeddings. The mechanism works by passing the embeddings through multiple layers of attention and feedforward blocks. The attention blocks focus on relationships between embeddings, augmenting each one with a weighted combination of the surrounding embeddings. The feedforward blocks capitalize on the extra context, transforming each augmented embedding with the non-linear magic of a fully-connected multilayer perceptron. By stacking multiple layers together, Transformers repeat the pattern of attention and transformation, gradually converting representations of individual words into representations of abstract semantic concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714c9ef-7079-48f8-ad54-22dc6b45ff50",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"transformer-layers.svg\" width=\"940\">\n",
    "<figcaption>Figure X: Context Layers</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a17731-7c4a-48a4-b675-727aba824ec2",
   "metadata": {},
   "source": [
    "Figure X illustrates the flow of information through a single layer. Embeddings are first passed to the Attention block. The attention outputs are added to the attention inputs before being passed to the FFN block. Similarly, the FFN outputs are added to the FFN inputs before being passed to the next layer. Adding the inputs and outputs of each block is known as \"residual learning\" and is critical for providing a stable path for gradient flow during training (He et al. 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aadd18f-5a1a-4c70-a461-5f07eecba08f",
   "metadata": {},
   "source": [
    "## Decoder-Only Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67512e8-b4bb-4c90-ade5-7f000d323a66",
   "metadata": {},
   "source": [
    "Like most of today's generative models, Llama uses a \"decoder-only\" model architecture. Instead of using the fully connected self attention we saw in [the DistilBERT teardown](https://stickshift.github.io/2024/09/04/transformer-teardown.html), the context layers in Llama use *masked self attention*. The \"decoder-only\" term comes from the \"Attention is All You Need\" paper, where Vaswani et al. described layers of self attention as \"encoder layers\" and layers of masked self attention as \"decoder layers\". While Vaswani et al.'s *Vanilla* Transformer architecture processed inputs and outputs with encoder layers and decoder layers respectively, later researchers showed that by adding more compute you could achieve the same goals using a single stack of decoder layers. For a fascinating discussion of how decoders became the dominant architecture, I highly recommend watching [Hyung Won Chung's guest lecture at Stanford on the Future of AI](https://youtu.be/orDKvo8h71o?si=J2sxhYtL9LCd6IRk) from April of this year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5839ec-b94a-4209-b6a4-a1143b29ac87",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc610a3b-7ecd-41d1-a4af-f24394455f07",
   "metadata": {},
   "source": [
    "Attention is the signature component in the Transformer architecture. In the 7 years since Vaswani et al. published \"Attention is All You Need\", researchers have experimented with numerous variations. Up next, we'll briefly describe the general idea of attention before describing the specific version used in Llama in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddaf2b1-7bc4-493e-a27d-f947f942d975",
   "metadata": {},
   "source": [
    "### General Idea Behind Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99ae8b-678a-4715-bd40-f554c9e07b35",
   "metadata": {},
   "source": [
    "Starting with input embeddings stacked in an $n \\times d_{model}$ tensor $\\mathbf{X}$, the goal of attention is to map each embedding $\\Set{\\mathbf{x}_i | \\mathbf{x}_i \\in \\mathbf{X}}$ to an attention representation $\\mathbf{a}_i$ that includes relevant contextual signals drawn from the rest of the embeddings $\\Set{\\mathbf{x}_j | \\mathbf{x}_j \\in \\mathbf{X}, i \\neq j}$. We'll dig into what \"relevant contextual signals\" means in the next section. For now, you could imagine the contextual signals contributed to $\\mathbf{a}_i$ by each $\\mathbf{x}_j$ as a function $f_A$ of $\\mathbf{x}_i$, $\\mathbf{x}_j$ and their positions in the sequence $i$, $j$.\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_i = \\sum_{\\mathbf{x}_j \\in \\mathbf{X}} f_A(\\mathbf{x}_i, \\mathbf{x}_j, i, j)\n",
    "$$\n",
    "\n",
    "Of course, the hard part is defining the attention function $f_A(\\mathbf{x}_i, \\mathbf{x}_j, i, j)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a530aea3-cd26-4ed9-8c08-379809033866",
   "metadata": {},
   "source": [
    "### Multi-Head Masked Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee9eb4-9752-4d28-b39e-0f6d798b43ae",
   "metadata": {},
   "source": [
    "The multi-head masked self attention function used by Llama can be expressed using the following equation.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{A} &= softmax\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}} + \\mathbf{M}\\right)\\mathbf{V} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $d$ is the size of each attention head.\n",
    "* $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ terms are all linear projections of $\\mathbf{X}$.\n",
    "* $\\mathbf{Q}$ is an $n \\times d$ tensor of *queries* that represent selection criteria for surrounding embeddings that would add valuable context to the current representation.\n",
    "* $\\mathbf{K}$ is an $n \\times d$ tensor of *keys* that represent characteristics that satisfy the selection criteria.\n",
    "* $\\mathbf{V}$ is an $n \\times d$ tensor of *values* that represent the contextual signals one embedding transfers to another.\n",
    "* $\\mathbf{M}$ is an $n \\times n$ attention mask that prevents earlier embeddings from attending to later ones by adding a $-\\infty$ bias to the later embeddings' scores.\n",
    "* $\\mathbf{A}$ is an $n \\times d$ tensor of the attention representations $\\mathbf{a}_i$ from the last section.\n",
    "\n",
    "> To understand what's happening, it's really helpful to know $\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}}$ is an $n \\times n$ tensor of scores that define how much of each embedding's values to include from $\\mathbf{V}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d785788-bf74-4ddc-9192-7e9f1b50ba7c",
   "metadata": {},
   "source": [
    "The idea is the $\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}}$ term calculates the distance between each query $\\mathbf{q}_i$ and key $\\mathbf{k}_j$ where *distance* is measured by the angle between the vectors. The smaller the angle, the closer the vectors, and the better the match between $\\mathbf{q}_i$ and $\\mathbf{k}_j$. The result of the $\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}}$ term is an $n \\times n$ tensor of scores between each query and key. Next, the attention mask $\\mathbf{M}$ is added to prevent later embeddings from influencing earlier embeddings by adding a $-\\infty$ bias to their scores. Finally, these are all passed to $softmax$ to normalize the scores across the keys. The result is an $n \\times n$ tensor where each row $i$ represents query $\\mathbf{q}_i$ and each column $j$ represents how well key $\\mathbf{k}_j$ matches $\\mathbf{q}_i$. These scores are then applied to the values in $\\mathbf{V}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f41bead-8674-4311-a654-486a073c67fe",
   "metadata": {},
   "source": [
    "### Position Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ec9c4-ed84-40ec-9b6d-62e4eb8af7d3",
   "metadata": {},
   "source": [
    "The relevance of one embedding to another is heavily influenced by their proximity in the sequence, making the distance between embeddings a critical data point in all Transformer models. We saw last time that BERT-based models encode the absolute token positions directly in the initial token embeddings. More recent models including Llama have adopted *relative* position encoding schemes that have been shown to perform better on much longer sequences.\n",
    "\n",
    "Llama uses an approach known as Rotary Position Embedding (RoPE) from Su et al. (2021). Instead of baking the positions into the initial token embeddings, RoPE integrates the embedding positions into the attention calculation. We saw in the last section that the attention calculation relies on the angular distance between queries and keys. The main idea behind RoPE is to convert the distance between two embeddings in the sequence into the angular distance between their vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22d429-c695-4315-9d67-4278cb60564a",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"rope.svg\" width=\"940\">\n",
    "<figcaption>Figure X: RoPE Concept in 2D</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96520746-afec-46bb-9e27-ce016eac27aa",
   "metadata": {},
   "source": [
    "Figure X illustrates the intuition behind RoPE in 2-dimensions. On the left, we have a sequence of 2-dimensional vectors $\\begin{bmatrix}\\mathbf{x}_0 & \\mathbf{x}_1 & \\mathbf{x}_2\\end{bmatrix}^T$ with their 2-dimensional geometric interpretation plotted on the right. The matrix in the center represents a rotational transformation. Given an embedding's position $m$, the idea behind RoPE is to rotate the embedding vector a distance of $m \\theta$. Following this idea, $x_0$ would stay the same, $x_1$ would be rotated a distance of $\\theta$, and $x_2$ would be rotated $2 \\theta$.\n",
    "\n",
    "While Figure X illustrates the idea behind RoPE in 2D, implementing it for n-dimensional vectors is a little more complicated. In practice, RoPE splits the token embeddings into pairs, e.g. {::nomarkdown}$\\Set{(\\mathbf{x}_0,\\mathbf{x}_1), (\\mathbf{x}_2,\\mathbf{x}_3), \\dots}${:/}, and then applies 2D rotations to each pair using the rotation matrix $\\mathbf{R}_{\\Theta,m}^d$.\n",
    "\n",
    "Given a hyperparameter $\\Theta$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{R}_{\\Theta,m}^d &= \n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) & -sin(m \\theta_0) & 0 & 0 & \\dots & 0 & 0 \\\\\n",
    "sin(m \\theta_0) & cos(m \\theta_0) & 0 & 0 & \\dots & 0 & 0 \\\\\n",
    "0 & 0 & cos(m \\theta_1) & -sin(m \\theta_1) & \\dots & 0 & 0 \\\\\n",
    "0 & 0 & sin(m \\theta_1) & cos(m \\theta_1) & \\dots & 0 & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & \\dots & 0 & 0 & cos(m \\theta_{d/2-1}) & -sin(m \\theta_{d/2-1}) \\\\\n",
    "0 & 0 & \\dots & 0 & 0 & sin(m \\theta_{d/2-1}) & cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\text{where } \\theta_i &= \\frac{1}{\\Theta^{2i/d_{head}}}, \\text{and } i \\text{ is the embedding index}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Luckily, Su et al. (2021) give us a more compact approach to apply $\\mathbf{R}_{\\Theta,m}^d$ in practice. To implement RoPE, we'll calculate the $cos$ and $sin$ terms in the following equation up front and then share them across all the layers.\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{\\Theta,m}^d \\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 \\\\\n",
    "\\vdots \\\\\n",
    "x_{d/2-2} \\\\\n",
    "x_{d/2-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "-x_2 \\\\\n",
    "x_1 \\\\\n",
    "-x_4 \\\\\n",
    "x_3 \\\\\n",
    "\\vdots \\\\\n",
    "-x_{d/2-1} \\\\\n",
    "x_{d/2-2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c4727e-11ed-48a3-ae49-e39a5c9fc30e",
   "metadata": {},
   "source": [
    "One last equation before we get to the code. Let's expand the attention equation to account for position encoding.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{A} &= softmax\\left(\\frac{(\\mathbf{R}_{\\Theta}^d\\mathbf{W}_Q\\mathbf{X})(\\mathbf{R}_{\\Theta}^d\\mathbf{W}_K\\mathbf{X})^T}{\\sqrt{d}} + \\mathbf{M}\\right)\\mathbf{W}_V\\mathbf{X} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\mathbf{R}_{\\Theta}^d$ is the RoPE rotation tensor.\n",
    "* $\\mathbf{W}_Q$, $\\mathbf{W}_K$, and $\\mathbf{W}_B$ are the linear projections to calculate queries, keys, and values from $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "381948de-dc81-4966-8195-cb1604ce9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rope(n):\n",
    "    # Hyperparams\n",
    "    base = config.rope_theta\n",
    "    d = config.d_head\n",
    "    \n",
    "    # Compute theta_i = 1 / base^(2i/d) from i = 0 to d/2-1\n",
    "    thetas = 1.0 / base**(2 * torch.arange(d // 2, device=device) / d)\n",
    "    \n",
    "    # Compute m * theta_i for position m in 0 to n\n",
    "    frequencies = torch.stack([m*thetas for m in range(n)])\n",
    "    \n",
    "    # Duplicate each row\n",
    "    frequencies = torch.cat((frequencies, frequencies), dim=-1)\n",
    "    \n",
    "    # Apply cos, sin\n",
    "    cos = torch.cos(frequencies)\n",
    "    sin = torch.sin(frequencies)\n",
    "    \n",
    "    # Sanity check\n",
    "    assert cos.shape[0] == n and cos.shape[1] == config.d_head\n",
    "    assert sin.shape[0] == n and sin.shape[1] == config.d_head\n",
    "\n",
    "    return cos, sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2d615bd-0d4d-41de-ae67-8c4d890e6600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([22, 128]), torch.Size([22, 128]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute RoPE rotation matrices\n",
    "rope_cos, rope_sin = rope(len(x))\n",
    "\n",
    "rope_cos.shape, rope_sin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f6e1d-418b-433f-964b-169033c7b1f5",
   "metadata": {},
   "source": [
    "### Attention Workflow\n",
    "\n",
    "The following diagram shows the steps we'll walk through to calculate attention. The goal is to break the attention calculation down into small enough pieces that each one is only a few lines of code.\n",
    "\n",
    "<figure>\n",
    "<img src=\"attention-flow.svg\" width=\"700\">\n",
    "<figcaption>Figure X: Attention Flow</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ec00f-903d-451a-9eaf-20d703c44878",
   "metadata": {},
   "source": [
    "### Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c78ae6a-358f-4c39-bacd-8ced26179ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention normalization\n",
    "normalize_attention = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(normalize_attention, \"normalize_attention\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ba28ad6-ee8d-456d-9906-b899f4fba272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize attention inputs\n",
    "residual = x\n",
    "x = normalize_attention(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbfb7ec-4bf4-43c2-8fa5-c5a83a71abf7",
   "metadata": {},
   "source": [
    "### Project Queries, Keys, Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5b9b235-9849-417d-97ab-ee9c467a4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure query, key, value projections\n",
    "w_q = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "w_k = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "w_v = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(w_q, \"w_q\", w_k, \"w_k\", w_v, \"w_v\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03a5a60e-f1d9-465a-ac74-5463fbb4b5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([22, 4096]), torch.Size([22, 1024]), torch.Size([22, 1024]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project embeddings to query, key, value spaces\n",
    "q = w_q(x)\n",
    "k = w_k(x)\n",
    "v = w_v(x)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f0a35-20df-41f1-88d2-be4d315a47f8",
   "metadata": {},
   "source": [
    "### Split Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba71ca0f-fce8-4f4f-8736-738d96b38af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, n_heads):\n",
    "    return x.view(-1, n_heads, config.d_head).transpose(-3, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5106a5d7-071a-4b79-802d-485ca673fdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 22, 128]), torch.Size([8, 22, 128]), torch.Size([8, 22, 128]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split attention heads\n",
    "q = split_heads(q, config.n_heads)\n",
    "k = split_heads(k, config.n_kv_heads)\n",
    "v = split_heads(v, config.n_kv_heads)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1a332-cea9-4a8b-9359-682bd74d80bc",
   "metadata": {},
   "source": [
    "### Encode Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a42b71de-de76-4c5d-a99f-0809df12d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Convert x to -x1, x0, ... for the sin column of RoPE rotation matrix.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82fb84aa-df57-47e6-860c-a29c429e4641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 22, 128]), torch.Size([8, 22, 128]), torch.Size([8, 22, 128]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode positions by rotating queries and keys\n",
    "q = (q * rope_cos) + (rotate_half(q) * rope_sin)\n",
    "k = (k * rope_cos) + (rotate_half(k) * rope_sin)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444ad9c-9a0b-43bd-8afd-18a5c581777e",
   "metadata": {},
   "source": [
    "### Expand Key / Value Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5edbc7c2-40a5-4139-ab98-8127e5649212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 22, 128]), torch.Size([32, 22, 128]), torch.Size([32, 22, 128]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand key/value groups\n",
    "k = k.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "v = v.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "119c1377-e336-4f16-9b72-c237177dc5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert q.shape == k.shape == v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb2b562-517e-4d6e-afc8-a9c237d516ae",
   "metadata": {},
   "source": [
    "### Calculate Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "943d4840-92df-45e2-8bca-5f187f3a57c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute attention mask M\n",
    "n = len(x)\n",
    "mask = torch.ones(n, n, dtype=torch.bool, device=device).tril(diagonal=0)\n",
    "m = torch.zeros(n, n, device=device).masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2a39d5f-f2d3-47d8-81a6-bb086672425e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 22, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute attention for all heads in parallel\n",
    "a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head) + m, dim=-1) @ v\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3145f-5cd7-491d-a4c5-a3a8adc4d076",
   "metadata": {},
   "source": [
    "### Recombine Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d80b30d-50ee-4513-9ea3-9ff428dcad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_heads(x):\n",
    "    return x.transpose(-3, -2).contiguous().view(-1, int(config.n_heads * config.d_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90e4a2ec-12e4-4eaf-b657-ac91a22965fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine attention heads\n",
    "a = combine_heads(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4d5595-1e05-4291-8bd8-9b33c2d929df",
   "metadata": {},
   "source": [
    "### Project Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4991289-e866-4c2c-a1d8-3750e1e115c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention output projection\n",
    "attention_outputs = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(attention_outputs, \"attention_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ceadcd83-ed8e-4f9b-866b-82b415298ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project attention embeddings back to model space\n",
    "a = attention_outputs(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130f43c-7246-4ba1-813d-566af8545f09",
   "metadata": {},
   "source": [
    "### Combine Outputs with Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0c943c2-3e22-4742-a0ce-84567abcff50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine attention embeddings with residuals\n",
    "x = residual + a\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a27df-3e8e-4e9e-a71b-82e28338170f",
   "metadata": {},
   "source": [
    "## Feedforward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6545d-12b2-4984-9ac1-3d3925d381cd",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"ffn-flow.svg\" width=\"940\">\n",
    "<figcaption>Figure X: FFN Flow</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96852a87-ffc1-400a-9cc4-5f79a1853428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87b6a353-547a-47f7-8dec-667ce4c3608a",
   "metadata": {},
   "source": [
    "### Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83cab172-1b6d-4384-9811-e1c442de5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FFN normalization\n",
    "normalize_ffn = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained state\n",
    "llama.load_state(normalize_ffn, \"normalize_ffn\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c9dd0c0-afa0-4cc9-9a56-4bd0b6700842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize FFN inputs\n",
    "residual = x\n",
    "x = normalize_ffn(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258318e4-65b9-47ae-b934-244b9e2775ef",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab08bb80-e138-4ac8-bdc2-1885833ab302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SwiGLU FFN\n",
    "ffn_gates = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_ffn,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "ffn_inputs = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_ffn,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(ffn_gates, \"ffn_gates\", ffn_inputs, \"ffn_inputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7b82390-e39a-46d0-8475-f60c84af619b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 14336])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply transform\n",
    "f = silu(ffn_gates(x)) * ffn_inputs(x)\n",
    "\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c881268-c8d4-4f6e-a5c9-e5d65765f13d",
   "metadata": {},
   "source": [
    "### Project Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b97a8505-687d-4123-8290-b3e270f5780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FFN output projection\n",
    "ffn_outputs = nn.Linear(\n",
    "    in_features=config.d_ffn,\n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(ffn_outputs, \"ffn_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad071436-a373-47ef-a434-bdf41bcbb91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project FFN embeddings back to model space\n",
    "f = ffn_outputs(f)\n",
    "\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f97bd7-9de5-4903-9b7c-aab5412587a8",
   "metadata": {},
   "source": [
    "### Combine Outputs with Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ac86a4a-540d-437b-b8cc-8e24977f4417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine FFN embeddings with residuals\n",
    "x = residual + f\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c143495-e6ba-4998-99e9-a4ada9552636",
   "metadata": {},
   "source": [
    "## Decoder Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc90f22b-1171-49f1-b2d8-73569b16cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_layers(x):\n",
    "    # Compute RoPE rotation matrices\n",
    "    rope_cos, rope_sin = rope(len(x))\n",
    "\n",
    "    # Apply layer logic in a loop\n",
    "    for layer in range(config.n_layers):\n",
    "    \n",
    "        # Load pre-trained state for layer\n",
    "        load_pretrained_state(layer)\n",
    "    \n",
    "        #\n",
    "        # Attention\n",
    "        #\n",
    "    \n",
    "        # Normalize attention inputs\n",
    "        residual = x\n",
    "        x = normalize_attention(x)\n",
    "        \n",
    "        # Project embeddings to query, key, value spaces\n",
    "        q = w_q(x)\n",
    "        k = w_k(x)\n",
    "        v = w_v(x)\n",
    "        \n",
    "        # Split attention heads\n",
    "        q = split_heads(q, config.n_heads)\n",
    "        k = split_heads(k, config.n_kv_heads)\n",
    "        v = split_heads(v, config.n_kv_heads)\n",
    "    \n",
    "        # Encode positions by rotating queries and keys\n",
    "        q = (q * rope_cos) + (llama.rotate_half(q) * rope_sin)\n",
    "        k = (k * rope_cos) + (llama.rotate_half(k) * rope_sin)\n",
    "        \n",
    "        # Expand key/value groups\n",
    "        k = k.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "        v = v.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "    \n",
    "        # Compute masked attention bias M\n",
    "        n = len(x)\n",
    "        mask = torch.ones(n, n, dtype=torch.bool, device=device).tril(diagonal=0)\n",
    "        m = torch.zeros(n, n, device=device).masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "        \n",
    "        # Compute attention for all heads in parallel\n",
    "        a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head) + m, dim=-1) @ v\n",
    "    \n",
    "        # Combine attention heads\n",
    "        a = combine_heads(a)\n",
    "        \n",
    "        # Project attention embeddings back to model space\n",
    "        a = attention_outputs(a)\n",
    "        \n",
    "        # Combine attention embeddings with residuals\n",
    "        x = residual + a\n",
    "        \n",
    "        #\n",
    "        # FFN\n",
    "        #\n",
    "    \n",
    "        # Normalize FFN inputs\n",
    "        residual = x\n",
    "        x = normalize_ffn(x)\n",
    "    \n",
    "        # Apply transform\n",
    "        f = silu(ffn_gates(x)) * ffn_inputs(x)\n",
    "    \n",
    "        # Project FFN embeddings back to model space\n",
    "        f = ffn_outputs(f)\n",
    "        \n",
    "        # Combine FFN embeddings with residuals\n",
    "        x = residual + f\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f585aa5-03e2-42c4-8582-0200d092e281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start over from initial tokens\n",
    "x = torch.tensor(token_ids, device=device)\n",
    "\n",
    "# Initial embeddings\n",
    "x = embeddings(x)\n",
    "\n",
    "# Contextualized embeddings\n",
    "x = context_layers(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83d86826-0a33-4a43-90c0-a3eae526b5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8842,  1.9047,  1.0641,  ..., -1.3221,  2.1526,  1.3637],\n",
       "        [ 0.5711, -0.4364, -0.1372,  ..., -0.0924, -0.2381, -0.1401],\n",
       "        [-0.2568, -0.5273, -0.4703,  ...,  0.2887,  1.0332, -1.0900],\n",
       "        ...,\n",
       "        [-0.0088, -0.0946, -0.1153,  ...,  0.2482,  0.1270, -0.0051],\n",
       "        [ 0.2867, -0.0427,  0.3964,  ..., -0.1481,  0.2981, -0.3256],\n",
       "        [-0.9179,  0.6267,  0.4772,  ...,  0.0447,  1.8657, -0.3242]],\n",
       "       device='mps:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1603550e-f8c5-44fc-b8d1-19428aeff5b5",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d29083-7ea8-41cf-9299-1466c5f5014a",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"head-flow.svg\" width=\"700\">\n",
    "<figcaption>Figure X: Head Flow</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ea5d20-7fe6-4e71-bca1-4aef72dc803d",
   "metadata": {},
   "source": [
    "### Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f030e10b-a7e1-4df9-8460-ece7d8d4bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure head normalization\n",
    "normalize_head = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(normalize_head, \"normalize_head\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38cae1a9-25b7-43a3-9134-1fc7bce64f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize head inputs\n",
    "x = normalize_head(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9066d-73ab-48d0-9185-469f86f7839b",
   "metadata": {},
   "source": [
    "### Project Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "67c44655-5796-40f6-bbb1-300a1b6a86a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure output projection\n",
    "head_outputs = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.vocab_size,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(head_outputs, \"head_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09534e41-6d49-416a-b2a9-92ff4c402d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128256])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use last embedding to represent the entire sequence\n",
    "x = x[-1]\n",
    "\n",
    "# Project outputs to token space\n",
    "x = head_outputs(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb78c4-2e9e-430e-9c6e-bfd1e882e6fb",
   "metadata": {},
   "source": [
    "## Top Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "528773e0-0d4b-4641-8998-c0693105d4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boston'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select top scoring token\n",
    "token_id = x.argmax()\n",
    "\n",
    "# Decode token\n",
    "token = tokenizer.decode([token_id]).strip()\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf5b84b5-3fce-4f46-8d39-867eec194cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify answer\n",
    "assert token == \"Boston\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37cf2c-ceb5-4420-b2dd-0cc6e1c5df93",
   "metadata": {},
   "source": [
    "## Sample Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db6efc-ee1f-457d-834c-1577338c18c8",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8eeb0fff-2f5e-4b0e-a2bf-b1861129bf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "temperature = config.temperature\n",
    "temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3879e58-b9d7-451d-b3da-a4387ad48ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply temperature\n",
    "x = x / temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366bad75-488b-4951-999a-457d2f75e69e",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f49c1ed-a77d-4ec3-ba14-908e69452edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to probabilities\n",
    "probs = softmax(x)\n",
    "\n",
    "# Sort probabilities in descending order\n",
    "probs, indices = probs.sort(descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1108c86-6859-45d8-88ae-dae902d7b22a",
   "metadata": {},
   "source": [
    "### Top K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d50e33b2-17c8-4f86-a540-b4d0154c34ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "top_k = config.top_k\n",
    "top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc1f1989-6a8e-45a6-a791-975fd0cd4bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 50 of 128256\n"
     ]
    }
   ],
   "source": [
    "# Retain top k tokens\n",
    "probs = probs[:top_k]\n",
    "print(f\"Retained {len(probs)} of {len(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec20b139-494f-47cb-b772-3c63c3de9211",
   "metadata": {},
   "source": [
    "### Top P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de639c47-170e-41a2-bf19-21ecff0992e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "top_p = config.top_p\n",
    "top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c052e39d-0dde-43fd-8a19-dbbb6d23af8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 2 of 128256\n"
     ]
    }
   ],
   "source": [
    "# Find cutoff where cumulative probability exceeds top_p\n",
    "cumulative_mask = probs.cumsum(dim=-1) > top_p\n",
    "threshold_index = torch.argmax(cumulative_mask).item()\n",
    "\n",
    "# Only apply threshold if top_p was exceeded\n",
    "if cumulative_mask.any():\n",
    "    probs = probs[:threshold_index+1]\n",
    "\n",
    "print(f\"Retained {len(probs)} of {len(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dea82f-bf61-434e-849f-ef5bce79f9cb",
   "metadata": {},
   "source": [
    "### Random Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3aed89c4-95e3-4431-aa13-ac507c982cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token id 65432, token 'Boston', score 0.661\n",
      "token id 791, token 'The', score 0.338\n"
     ]
    }
   ],
   "source": [
    "# Print remaining token pool\n",
    "for i, prob in enumerate(probs):\n",
    "    print(f\"token id {indices[i]}, token '{tokenizer.decode([indices[i]])}', score {prob:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e39fde30-3a77-4752-941f-ced72393c8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boston'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample from remaining tokens weighted by probability\n",
    "sampled_index = torch.multinomial(probs, 1)\n",
    "\n",
    "# Convert sampled_index to original logits\n",
    "token_id = indices[sampled_index]\n",
    "\n",
    "# Decode token\n",
    "token = tokenizer.decode([token_id]).strip()\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c8acd-fe53-4c1f-8080-1d6bfaf0b027",
   "metadata": {},
   "source": [
    "## Complete Head Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f696ef95-7597-4f2b-bfd1-835956bb7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def head(x):\n",
    "    # Normalize head inputs\n",
    "    x = normalize_head(x)\n",
    "    \n",
    "    # Use last embedding to represent the entire sequence\n",
    "    x = x[-1]\n",
    "    \n",
    "    # Project outputs to token space\n",
    "    x = head_outputs(x)\n",
    "\n",
    "    #\n",
    "    # Temperature\n",
    "    #\n",
    "    \n",
    "    # Apply temperature\n",
    "    x = x / config.temperature\n",
    "\n",
    "    #\n",
    "    # Ranking\n",
    "    #\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probs = softmax(x)\n",
    "    \n",
    "    # Sort probabilities in descending order\n",
    "    probs, indices = probs.sort(descending=True)\n",
    "\n",
    "    #\n",
    "    # Top K\n",
    "    #\n",
    "    \n",
    "    # Retain top k tokens\n",
    "    probs = probs[:config.top_k]\n",
    "\n",
    "    #\n",
    "    # Top P\n",
    "    #\n",
    "    \n",
    "    # Find cutoff where cumulative probability exceeds top_p\n",
    "    cumulative_mask = probs.cumsum(dim=-1) > config.top_p\n",
    "    threshold_index = torch.argmax(cumulative_mask).item()\n",
    "    \n",
    "    # Only apply threshold if top_p was exceeded\n",
    "    if cumulative_mask.any():\n",
    "        probs = probs[:threshold_index+1]\n",
    "\n",
    "    #\n",
    "    # Random Selection\n",
    "    #\n",
    "    \n",
    "    # Sample from remaining tokens weighted by probability\n",
    "    sampled_index = torch.multinomial(probs, 1)\n",
    "    \n",
    "    # Convert sampled_index to original logits\n",
    "    token_id = indices[sampled_index]\n",
    "\n",
    "    return token_id.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568352c3-2573-440f-9c1e-ef0acccd245b",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2328afe7-d40a-4d47-91cd-be59b02b4485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3c9533b1-7011-4f84-85b5-51ca93b15b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@validate_call\n",
    "def prepare_messages(messages: list[Message]):\n",
    "    # Initialize prompt\n",
    "    prompt = \"\"\n",
    "    \n",
    "    # Format each message\n",
    "    for message in messages:\n",
    "        prompt += f\"<|start_header_id|>{message.role}<|end_header_id|>\\n\\n\"\n",
    "        prompt += message.content\n",
    "        prompt += \"<|eot_id|>\"\n",
    "\n",
    "    # Finish with the assistant role to prime the model's response\n",
    "    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b875c2e-a1ac-45dd-8df3-02398fa5c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@validate_call\n",
    "def generate(messages: list[Message]):\n",
    "    # Format message prompt\n",
    "    prompt = prepare_messages(messages)\n",
    "    \n",
    "    # Split raw text into tokens\n",
    "    token_ids = tokenizer.encode(prompt, bos=True, eos=False, allowed_special=\"all\")\n",
    "    \n",
    "    # Generate output until we get a stop token or we exceed max_output_tokens.\n",
    "    for _ in range(config.max_output_tokens):\n",
    "        \n",
    "        # Start over from initial tokens\n",
    "        x = torch.tensor(token_ids, device=device)\n",
    "        \n",
    "        # Initial embeddings\n",
    "        x = embeddings(x)\n",
    "        \n",
    "        # Contextualized embeddings\n",
    "        x = context_layers(x)\n",
    "        \n",
    "        # Head\n",
    "        token_id = head(x)\n",
    "        \n",
    "        # Check stopping criteria\n",
    "        if token_id in tokenizer.stop_tokens:\n",
    "            break\n",
    "    \n",
    "        # Print token\n",
    "        token = tokenizer.decode([token_id])\n",
    "        stdout.write(token)\n",
    "        \n",
    "        # Append to end of sequence\n",
    "        token_ids.append(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "34e8bf24-cb98-4618-a481-5712d9aa5477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Massachusetts is Boston."
     ]
    }
   ],
   "source": [
    "generate([\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is capital of Massachusetts?\",\n",
    "    },\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7c054-8863-4b24-83ba-24415c78a553",
   "metadata": {},
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf596e24-2e31-47f9-9bfc-be0731542b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725cb1fc-7c58-4ca9-a55e-929a125eefea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faee9494-3ca7-499d-b072-d71b87fbbe33",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b07abe-b5ad-4452-9067-ea9ee1e56982",
   "metadata": {},
   "source": [
    "Ainslie, Joshua, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. “GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.” arXiv. https://doi.org/10.48550/arXiv.2305.13245.\n",
    "\n",
    "Bengio, Yoshua, Réjean Ducharme, and Pascal Vincent. 2000. “A Neural Probabilistic Language Model.” In Advances in Neural Information Processing Systems. Vol. 13. MIT Press. https://proceedings.neurips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html.\n",
    "\n",
    "Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” arXiv. https://doi.org/10.48550/arXiv.2005.14165.\n",
    "\n",
    "Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv.Org. May 24, 2019. https://arxiv.org/abs/1810.04805v2.\n",
    "\n",
    "Dubey, Abhimanyu, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, et al. 2024. “The Llama 3 Herd of Models.” arXiv.Org. July 31, 2024. https://arxiv.org/abs/2407.21783v2.\n",
    "\n",
    "He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” arXiv. https://doi.org/10.48550/arXiv.1512.03385.\n",
    "\n",
    "Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. “Improving Language Understanding by Generative Pre-Training.”\n",
    "\n",
    "Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. “Neural Machine Translation of Rare Words with Subword Units.” arXiv. https://doi.org/10.48550/arXiv.1508.07909.\n",
    "\n",
    "Shazeer, Noam. 2020. “GLU Variants Improve Transformer.” arXiv. https://doi.org/10.48550/arXiv.2002.05202.\n",
    "Stanford Online, dir. 2024. Stanford CS25: V4 I Hyung Won Chung of OpenAI. https://www.youtube.com/watch?v=orDKvo8h71o.\n",
    "\n",
    "Su, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2021. “RoFormer: Enhanced Transformer with Rotary Position Embedding.” arXiv.Org. April 20, 2021. https://arxiv.org/abs/2104.09864v5.\n",
    "\n",
    "Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, et al. 2023. “LLaMA: Open and Efficient Foundation Language Models.” arXiv. https://doi.org/10.48550/arXiv.2302.13971.\n",
    "\n",
    "Touvron, Hugo, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et al. 2023. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” arXiv.Org. July 18, 2023. https://arxiv.org/abs/2307.09288v2.\n",
    "\n",
    "Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” arXiv. https://doi.org/10.48550/arXiv.1706.03762."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492c939-8e8a-4d21-962c-8fd134486fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "stickshift": {
   "description": "Trace an Inference Through Each Layer of the SOTA Llama 3.1 Foundation Models",
   "title": "Transformer Teardown: Llama 3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
