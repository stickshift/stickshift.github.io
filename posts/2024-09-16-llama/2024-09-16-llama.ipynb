{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f5fa8b-9842-4be3-8cdc-3421d5019662",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "source": [
    "# Transformer Teardown: Llama 3.1\n",
    "\n",
    "> Trace an Inference Through Each Layer of the SOTA Llama 3.1 Foundation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924b74d-61d7-4cfc-a3c0-5c333bec5659",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In [the last Transformer Teardown](https://stickshift.github.io/2024/09/04/transformer-teardown.html), we dissected a DistilBERT text classification pipeline, tracing a single inference through the entire stack from raw data to final prediction. We learned about the main stages of a Transformer pipeline as well as fundamental Transformer concepts such as token embeddings and Multi-Head Self Attention. Studying BERT-based text classification models is a fantastic way to see the basic Transformer machinery in action. But BERT was published in 2018! It would be another 4 years before ChatGPT launched and Generative AI exploded onto the scene.\n",
    "\n",
    "In this Transformer Teardown, we're going to fast forward to present day. We'll use the same teardown process to unpack the state-of-the-art [Llama 3.1](https://llama.meta.com/) open source foundation models released by Meta in July. We'll walk through each step of a text generation pipeline one cell at a time, tracing an inference from raw text to the first output token. We'll illustrate the main ideas from the latest Transformer literature with minimal, straightforward, working Python code, giving you a close-up view of the core mechanisms driving the Generative AI revolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c3be3e-78e2-47f4-8714-d6613bea7a4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "040ef74f-fccb-4a91-9748-b3eba0398a0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sys import stdout\n",
    "from textwrap import dedent\n",
    "import warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas import Series\n",
    "from pydantic import BaseModel, validate_call\n",
    "from pytest import approx\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import relu, silu, softmax\n",
    "\n",
    "from llama_models.llama3.reference_impl.model import RMSNorm\n",
    "\n",
    "import stickshift as ss\n",
    "from stickshift import default_arg, take\n",
    "from stickshift.torch import device as torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8523438f-2c81-474b-bab5-e9a707fcc572",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure gpu\n",
    "device = torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca4d3dd0-eff4-4090-8673-9810d87b1b74",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "figure > img {\n",
       "    display:block;\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "figcaption {\n",
       "    text-align: center;\n",
       "}\n",
       "blockquote {\n",
       "    margin-top: 2.0rem !important;\n",
       "    margin-bottom: 2.0rem !important;\n",
       "    margin-left: 0 !important;\n",
       "    margin-right: 0 !important;\n",
       "    padding: 1.0rem !important;\n",
       "    background-color: rgba(0,0,0,0.05) !important;\n",
       "    border: 1px solid rgba(0,0,0,0.1) !important;\n",
       "    font-style: italic !important;\n",
       "}\n",
       "blockquote p {\n",
       "    margin: 0 !important;\n",
       "    padding: 0 !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "figure > img {\n",
    "    display:block;\n",
    "    margin-left: auto !important;\n",
    "    margin-right: auto !important;\n",
    "}\n",
    "figcaption {\n",
    "    text-align: center;\n",
    "}\n",
    "blockquote {\n",
    "    margin-top: 2.0rem !important;\n",
    "    margin-bottom: 2.0rem !important;\n",
    "    margin-left: 0 !important;\n",
    "    margin-right: 0 !important;\n",
    "    padding: 1.0rem !important;\n",
    "    background-color: rgba(0,0,0,0.05) !important;\n",
    "    border: 1px solid rgba(0,0,0,0.1) !important;\n",
    "    font-style: italic !important;\n",
    "}\n",
    "blockquote p {\n",
    "    margin: 0 !important;\n",
    "    padding: 0 !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58287922-dc47-44d9-a2c6-77bd0c8abb7f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61da974-fb00-438a-bb44-f3a432182478",
   "metadata": {},
   "source": [
    "# Llama Foundation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d95b5-8ee7-45bd-9d2d-7908012ac587",
   "metadata": {},
   "source": [
    "[Llama](https://llama.meta.com/) is a family of general purpose, state-of-the-art open source foundation models from Meta. According to the 3.1 technical report, the latest models can \"answer questions in at least 8 languages, write high quality code, solve complex reasoning problems, and use tools in a zero-shot way.\" (Dubey et al. 2024) The Llama 3.1 release includes 8B, 70B, and 405B sizes. While you need a multi-GPU cluster to run the 70B and 405B sizes, the 8B model is small enough to experiment with on a laptop. Not only did Meta release the pre-trained model checkpoints for all 3 sizes, they also published a fantastically detailed, [70 page technical report](https://arxiv.org/abs/2407.21783v2) as well as a complete [reference implementation](https://github.com/meta-llama/llama-models). Together, Llama 3.1 represents both a tremendous contribution to the AI community as well as an incredible learning opportunity to study the inner workings of a modern frontier model.\n",
    "\n",
    "Over the course of this post, we'll implement a complete text generation pipeline using only the research literature, pre-trained weights from the `Meta-Llama3.1-8B-Instruct` checkpoint, and Meta's reference implementation as a guide. After we load the 8B checkpoint, we'll review the stages of an end-to-end, text generation pipeline. In the sections that follow, we'll walk through a detailed teardown of each stage—tracing an inference from raw data to the first output token. In the last section, we'll put all the pieces together into a complete generative Transformer capable of producing long form content.\n",
    "\n",
    "Let the teardown begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a3d63-e556-4845-9c2f-cea2d0712d27",
   "metadata": {},
   "source": [
    "# Model Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb04ecc-6f9a-4e2a-9c23-7ae89a702f31",
   "metadata": {},
   "source": [
    "We'll start by loading the configuration and pre-trained weights for the `Meta-Llama3.1-8B-Instruct` checkpoint. The \"instruct\" versions of the Llama models include the raw pre-training and substantial post-training to support user and assistant interactions and complex tool-calling scenarios. The weights for all Llama checkpoints can be downloaded directly from [Meta](https://llama.meta.com/), [Hugging Face](https://huggingface.co/meta-llama), and [Kaggle](https://www.kaggle.com/organizations/metaresearch/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07362fc6-41cf-45b4-8d88-e8c55aa9ce9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checkpoint_path': PosixPath('/Users/andrewyoung/.llama/checkpoints/Meta-Llama3.1-8B-Instruct'), 'vocab_size': 128256, 'd_model': 4096, 'd_head': 128, 'd_ffn': 14336, 'n_layers': 32, 'n_heads': 32, 'n_kv_heads': 8, 'n_kv_groups': 4, 'rms_norm_eps': 1e-05, 'rope_theta': 500000.0, 'max_seq_len': 8192, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9, 'max_output_tokens': 500}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import custom utilities\n",
    "from stickshift.models import llama\n",
    "\n",
    "# Load model config\n",
    "config = llama.config(\"Meta-Llama3.1-8B-Instruct\")\n",
    "\n",
    "# Load pre-trained model parameters\n",
    "checkpoint = torch.load(\n",
    "    config.checkpoint_path / \"consolidated.00.pth\", \n",
    "    weights_only=True, \n",
    "    map_location=device,\n",
    ")\n",
    "\n",
    "config.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bc3a1e-91c4-4fec-896f-2f5ee6977915",
   "metadata": {},
   "source": [
    "We'll reference a number of the settings in `config` throughout the teardown. For now, a few interesting ones to note are `d_model`, `d_ffn`, `n_layers`, and `n_heads`. These represent the primary differences between the 8B, 70B, and 405B sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6acaf5d-b853-46fa-86e1-d63030074dbd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "def load_pretrained_state(layer):    \n",
    "    # Load pre-trained state\n",
    "    llama.load_state(\n",
    "        normalize_attention, \"normalize_attention\", \n",
    "        normalize_ffn, \"normalize_ffn\", \n",
    "        w_q, \"w_q\", \n",
    "        w_k, \"w_k\", \n",
    "        w_v, \"w_v\", \n",
    "        attention_outputs, \"attention_outputs\",\n",
    "        ffn_gates, \"ffn_gates\",\n",
    "        ffn_inputs, \"ffn_inputs\",\n",
    "        ffn_outputs, \"ffn_outputs\",\n",
    "        checkpoint=checkpoint,\n",
    "        layer=layer,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb0698a-439d-44ad-959c-33b2a19a9593",
   "metadata": {},
   "source": [
    "# Text Generation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c8069-1148-48a7-bb07-a91f60b43a3f",
   "metadata": {},
   "source": [
    "In [the last teardown](https://stickshift.github.io/2024/09/04/transformer-teardown.html), we looked at a text classification Transformer. This time we're going to dissect a *text generation* Transformer. Instead of simply applying a label to the input text, the Head stage will be responsible for *generating* new content. But don't worry! It's not as complicated as it sounds.\n",
    "\n",
    "Figure X illustrates the stages in a text generation pipeline. It's very similar to the text classification pipeline we looked at last time. The Tokenize stage splits raw text into a sequence of tokens. The Embeddings stage maps the sequence of tokens to a sequence of embedding vectors. The Context Layers augment the embeddings with contextual signals drawn from the surrounding tokens, transforming individual token embeddings into contextualized \"semantic embeddings\". Finally, the Head stage converts the semantic embeddings into predictions. The key difference is, instead of predicting a label for the raw text, text generation Transformers *predict the next token*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc06ac-b312-491b-98b6-ed82d237b6cd",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"transformer-pipeline.svg\" width=\"940\">\n",
    "<figcaption>Figure X: Text Generation Pipeline</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe17de4c-55b7-48d3-8f78-43874b3c4bb4",
   "metadata": {},
   "source": [
    "But one token is just the beginning! The magical powers of Generative AI are manifested by simply running the token predictions in a loop. The predicted token in each iteration is appended to the end of the input sequence, and the process repeats. Over and over again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95117d8e-7ea3-4c2a-ad4a-1bd535771a5f",
   "metadata": {},
   "source": [
    "# Raw Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacc9c84-f587-49e4-85d5-e89ed78fa9db",
   "metadata": {},
   "source": [
    "Before we can tear anything down, we need a prompt. Since our goal is to trace an inference from raw text to the first output token, we want to start with a prompt that's specific enough to generate a consistent, one-word answer. If we do everything right, the first output token we predict should be \"Boston\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9832bc2-033e-4c48-903f-f05544c135eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "prompt += \"What is the capital of Massachusetts? Answer in one word.\"\n",
    "prompt += \"<|eot_id|>\"\n",
    "prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5ae4e-3b6d-4b71-9eed-a2dd512cf8f9",
   "metadata": {},
   "source": [
    "You can see `prompt` includes a number of special tokens. These would usually be injected by a framework like Hugging Face's `transformers`. We need to manually inject them because we're working with the model directly. You can find more information on the Llama 3.1 prompt syntax in the [Llama Prompting Guide](https://www.llama.com/docs/how-to-guides/prompting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0e94d-a969-43b8-bfad-2ceaa10ca7aa",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f879aa-2352-48a8-bc02-60adcd1d058a",
   "metadata": {},
   "source": [
    "The Tokenize stage splits raw text into a sequence of tokens using a fixed vocabulary. Llama uses a vocabulary of 128k tokens built on top of OpenAI's tiktoken tokenizer. We'll dig into the gory details in the later stages, but here we'll simply use the off-the-shelf Tokenizer from Meta's llama-models reference implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9605a1b6-beb8-4f7f-8ad2-67351929a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_models.llama3.api.tokenizer import Tokenizer\n",
    "\n",
    "# Load tokenizer model from checkpoint\n",
    "tokenizer = Tokenizer(str(config.checkpoint_path / \"tokenizer.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07c6b94b-36dc-43b0-8dc8-e99c28d26d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 128006, 882, 128007, 271, 3923, 374, 279, 6864, 315, 22108, 30, 22559, 304, 832, 3492, 13, 128009, 128006, 78191, 128007, 271]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split raw text into tokens\n",
    "token_ids = tokenizer.encode(prompt, bos=True, eos=False, allowed_special=\"all\")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bfa2427-7eba-4c68-b161-fc8238d4c2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e067e-95ba-4c7c-aef6-0e6c031057fa",
   "metadata": {},
   "source": [
    "We see `tokenizer.encode` split our prompt into 22 token ids. These ids represent the index of each token in Llama's 128k token vocabulary. We can always reverse the process with `tokenizer.decode`. If you look closely at the cell output below, you'll notice the tokenizer injected another special token `(128000, '<|begin_of_text|>')` to mark the beginning of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73c0e958-3d09-4f4e-8269-2cf7a1a39bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the capital of Massachusetts? Answer in one word.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode token ids back into raw text\n",
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "040cffac-921e-43ad-ac2b-069895233360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load token_ids into a tensor\n",
    "x = torch.tensor(token_ids, device=device)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f60316-a506-4912-b8ac-8bbaa83a4d7f",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccd09b-1a58-40e7-98f4-4794ea681c98",
   "metadata": {},
   "source": [
    "Embeddings are a key component of the Transformer architecture. They're also abstract mathematical structures that can be difficult to wrap your head around. To illustrate the crucial role embeddings play, let's use a quick metaphor.\n",
    "\n",
    "> If a Transformer was a brain, then embeddings would be the electrical signals carrying information through the brain.\n",
    "\n",
    "Continuing with the metaphor, the Embeddings stage of the pipeline would be your sensory organs where light rays and air vibrations are translated into electrical impulses. Token embeddings would be the fresh sensory percepts. Semantic embeddings would be the abstract thoughts at the top of the cortical stack. The idea of percepts traveling up the cortical stack is a perfect analogy for token embeddings traveling through the Transformer layers.\n",
    "\n",
    "Implementing Llama's Embeddings stage is relatively straightforward. We'll use a lookup table with a unique embedding for each of the 128k tokens in the vocabulary. Each embedding is a vector with $d_{model}$ elements that were randomly generated and then learned during training. Given a sequence of token ids, the lookup table returns their embeddings as row vectors stacked in an $n \\times d_{model}$ tensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811d333-dc1c-4177-950a-56ced36ab246",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"embeddings.svg\" width=\"940\">\n",
    "<figcaption>Figure X: Learned Token Embeddings</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43f02c0c-6073-401e-9f8e-b6f5159e7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings lookup table\n",
    "embeddings = nn.Embedding(\n",
    "    num_embeddings=config.vocab_size, \n",
    "    embedding_dim=config.d_model,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "llama.load_state(embeddings, \"embeddings\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd113f37-d587-45e0-aecd-5ddc7b82144a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map token ids to embeddings\n",
    "x = embeddings(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46771ed3-4483-4a7b-ade5-25179d13feba",
   "metadata": {},
   "source": [
    "We can see from `x.shape` that we successfully mapped the 22 token ids to 22 token embeddings stacked in an $n \\times d_{model}$ tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5da73e42-c82a-4469-bdf5-fe04e51bae6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
       "          6.3419e-05,  1.1902e-03],\n",
       "        [-1.6499e-04, -2.4319e-04,  1.6403e-04,  ..., -1.5163e-04,\n",
       "          3.5095e-04,  7.3242e-04],\n",
       "        [ 3.5095e-03,  7.2021e-03,  5.3406e-05,  ..., -7.2479e-04,\n",
       "         -1.0620e-02,  8.2779e-04],\n",
       "        ...,\n",
       "        [-9.7656e-03, -3.4637e-03,  1.8616e-03,  ..., -7.1411e-03,\n",
       "         -4.3030e-03,  8.6060e-03],\n",
       "        [-4.6158e-04, -3.9291e-04, -6.5863e-06,  ..., -6.2561e-04,\n",
       "         -5.0354e-04,  6.6757e-04],\n",
       "        [-2.8687e-03,  3.8910e-03, -1.7357e-04,  ...,  8.0872e-04,\n",
       "          5.0354e-04,  2.3041e-03]], device='mps:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show sample\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc36c605-3d13-4d36-98a0-021005ad0f75",
   "metadata": {},
   "source": [
    "Before we move on, a quick note on terminology. If you've used cloud-based LLM APIs like OpenAI or LangChain, you're likely familiar with the term \"embedding model\". An *embedding model* is really a combination of a tokenizer and embeddings table. These are often bundled together to give you everything you need to convert raw text into embedding vectors and can be used for a number of things independent of the LLM.\n",
    "\n",
    "Now that we've converted our raw text into token embeddings, it's time to start transforming!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab57f0-610c-46e5-b372-532c41699d43",
   "metadata": {},
   "source": [
    "# Context Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f094441-b5d6-48be-a3ff-63af16a8d9a6",
   "metadata": {},
   "source": [
    "Context layers are where the Transformer magic happens. Collectively, the Context Layers are responsible for transforming a sequence of token embeddings into a sequence of semantic embeddings. The mechanism works by passing the embeddings through multiple layers of attention and feedforward blocks. The attention blocks focus on relationships between embeddings, augmenting each one with a weighted combination of the surrounding embeddings. The feedforward blocks capitalize on the extra context, transforming each augmented embedding with the non-linear magic of a fully-connected multilayer perceptron. By stacking multiple layers together, Transformers repeat the pattern of attention and transformation, gradually converting representations of individual words into representations of abstract semantic concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714c9ef-7079-48f8-ad54-22dc6b45ff50",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"transformer-layers.svg\" width=\"940\">\n",
    "<figcaption>Figure X: Context Layers</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a17731-7c4a-48a4-b675-727aba824ec2",
   "metadata": {},
   "source": [
    "Figure X illustrates the flow of information through a single layer. Embeddings are first passed to the Attention block. The attention outputs are added to the attention inputs before being passed to the FFN block. Similarly, the FFN outputs are added to the FFN inputs before being passed to the next layer. Adding the inputs and outputs of each block is known as \"residual learning\" and is critical for providing a stable path for gradient flow during training (He et al. 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aadd18f-5a1a-4c70-a461-5f07eecba08f",
   "metadata": {},
   "source": [
    "## Decoder-Only Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67512e8-b4bb-4c90-ade5-7f000d323a66",
   "metadata": {},
   "source": [
    "Like most of today's generative models, Llama uses a \"decoder-only\" model architecture. Instead of using the fully connected self attention we saw in [the DistilBERT teardown](https://stickshift.github.io/2024/09/04/transformer-teardown.html), the context layers in Llama use *masked self attention*. The \"decoder-only\" term comes from the \"Attention is All You Need\" paper, where Vaswani et al. described layers of self attention as \"encoder layers\" and layers of masked self attention as \"decoder layers\". While Vaswani et al.'s *Vanilla* Transformer architecture processed inputs and outputs with encoder layers and decoder layers respectively, later researchers showed that by adding more compute you could achieve the same goals using a single stack of decoder layers. For a fascinating discussion of how decoders became the dominant architecture, I highly recommend watching [Hyung Won Chung's guest lecture at Stanford on the Future of AI](https://youtu.be/orDKvo8h71o?si=J2sxhYtL9LCd6IRk) from April of this year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5839ec-b94a-4209-b6a4-a1143b29ac87",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc610a3b-7ecd-41d1-a4af-f24394455f07",
   "metadata": {},
   "source": [
    "Attention is the signature component in the Transformer architecture. In the 7 years since Vaswani et al. published \"Attention is All You Need\", researchers have experimented with numerous attention variations of all shapes and sizes. Before we jump into the code, we'll quickly review the fundamental concepts behind attention followed by details on the specific approach chosen by the Llama authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddaf2b1-7bc4-493e-a27d-f947f942d975",
   "metadata": {},
   "source": [
    "### What is Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e68a4-c57a-44c9-88d1-b9cc3577e405",
   "metadata": {},
   "source": [
    "Given our input embeddings are stacked in an $n \\times d_{model}$ tensor $\\mathbf{X}$, the goal of attention is to map each embedding $\\set{\\mathbf{x}_i \\mid \\mathbf{x}_i \\in \\mathbf{X}}$ to an attention representation $\\mathbf{a}_i$ that includes relevant contextual signals drawn from the rest of the embeddings $\\set{\\mathbf{x}_j \\mid \\mathbf{x}_j \\in \\mathbf{X}, i \\neq j}$. \n",
    "\n",
    "For example, let's imagine we've mapped the sentence `I love New York` to the sequence of token embeddings $\\mathbf{x} = [E_{I}, E_{love}, E_{New}, E_{York}]$. The embedding $\\mathbf{x}_2$ represents the word `New` *in isolation*. The word `New` can mean a lot of things; many of which have nothing to do with this sentence. Our goal would be to generate an attention representation $\\mathbf{a}_2$ containing signals from the other embeddings $\\set{E_{I}, E_{love}, E_{York}}$ that would help us create a *better* version of $\\mathbf{x}_2$:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{2*} = \\mathbf{x}_{2} + \\mathbf{a}_{2}\n",
    "$$\n",
    "\n",
    "Let's assume each embedding contributes \"something\" to a_i. Even though we can't quantify \"something\" yet, we can write $\\mathbf{a}_i$ as an unknown function $f_A$ of the two embeddings $\\mathbf{x}_i$, $\\mathbf{x}_j$:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_i = \\sum_{\\mathbf{x}_j \\in \\mathbf{X}} f_A(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "All of the attention variations in the Transformer literature—e.g. Self Attention, Multi-Head Self Attention, Linear Attention, Grouped Query Attention—are different approaches to implement $f_A$. In practice, the authors of a new Transformer model start with Vaswani et al.'s attention definition and then select from the large, a la carte menu of improvements that have been published since, resulting in their own unique variation of attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a4fbf6-26c3-48b0-b154-2cf72108cca3",
   "metadata": {},
   "source": [
    "### Attention in Llama 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809be56b-99a1-490f-8f10-17b590a4cc66",
   "metadata": {},
   "source": [
    "Defining attention in Llama 3.1 requires a bit of backtracking. Most of the details can be found in Llama 1 (Touvron et al. 2023) with a few changes in Llama 2 (Touvron et al. 2023) and only minor adjustments in Llama 3 (Dubey et al. 2024).\n",
    "\n",
    "Starting with the standard Masked Self Attention definition from Vaswani et al. (2017), Llama adopts the following improvements that factor into the attention calculation:\n",
    "\n",
    "* Pre-normalization with RMSNorm: improves training stability and inference speed.\n",
    "* Grouped Query Attention (GQA) with 8 key/value heads: improves inference speed and reduce memory overhead of key-value caches.\n",
    "* Rotary Position Embedding (RoPE) with $\\Theta = 500,000$: relative position encoding improves performance on longer context windows.\n",
    "\n",
    "We'll start by describing the standard Masked Self Attention and then describe how these improvements modify the final attention calculation in Llama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f40ac23-8fa7-4c3b-a48b-bb5936c42e01",
   "metadata": {},
   "source": [
    "### Masked Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a440c1-a833-44da-aa69-7e327c1b30f0",
   "metadata": {},
   "source": [
    "Given $n$ input embeddings of length $d_{model}$ stacked in an $n \\times d_{model}$ tensor $\\mathbf{X}$, the standard masked self attention algorithm from Vaswani et al. can be expressed using the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{A} = softmax\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_K}} + \\mathbf{M}\\right)\\mathbf{V} \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ are all linear projections of $\\mathbf{X}$.\n",
    "* $\\mathbf{Q}$ is an $n \\times d_K$ tensor of *queries* that represent selection criteria for surrounding embeddings that would add valuable context to the current representation.\n",
    "* $\\mathbf{K}$ is an $n \\times d_K$ tensor of *keys* that represent characteristics that satisfy the selection criteria in the queries.\n",
    "* $\\mathbf{V}$ is an $n \\times d_{model}$ tensor of *values* that represent the contextual signals to transfer from one embedding to another.\n",
    "* The $\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_K}}$ term is an $n \\times n$ tensor of \"attention weights\" that define how much of each embedding's values to include from $\\mathbf{V}$.\n",
    "* $\\mathbf{M}$ is an $n \\times n$ attention mask that prevents earlier embeddings from attending to later ones by adding a $-\\infty$ bias to the later embeddings' scores.\n",
    "* $\\mathbf{A}$ is an $n \\times d_{model}$ tensor of attention representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c80f9-2fcf-493e-b365-ae2b70db6ed3",
   "metadata": {},
   "source": [
    "The idea is the $\\mathbf{Q}\\mathbf{K}^T$ term calculates the *angular distance* between each query vector $\\mathbf{q}_i$ and key vector $\\mathbf{k}_j$ by taking their dot product. The smaller the angle, the closer the vectors, and the better the match between $\\mathbf{q}_i$ and $\\mathbf{k}_j$. The result of the $\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_K}}$ term is an $n \\times n$ tensor of raw scores where row $i$ represents query $\\mathbf{q}_i$ and column $j$ represents how well key $\\mathbf{k}_j$ matches $\\mathbf{q}_i$.\n",
    "\n",
    "The rationale for the mask term $\\mathbf{M}$ requires a backstory. In the original Transformer architecture, decoder layers played a more limited role. All of the input tokens for our prompt \"What is the capital of Massachusetts?\" would be processed by the encoder while the output tokens would be handled by the decoder. Since all of the tokens in our prompt are available when we start, the encoder's Attention blocks can connect them all together using what's called \"cross attention\". But this isn't true for the decoder. Since the model only generates one token at a time, the output tokens generated at the beginning aren't allowed to attend to tokens generated later. Put another way, the words I say now can't be influenced by the words I say later because they haven't happened yet. This idea is referred to as \"causal attention\" and is implemented using the mask term $\\mathbf{M}$. \n",
    "\n",
    "As described earlier, decoder-only architectures that process both input and output tokens with the same layers have taken over as the dominant standard for generative Transformers. *This can be really confusing at first!* Why shouldn't we allow $E_{capital}$ to attend to $E_{Massachusetts}$? Because decoder-only models have repeatedly demonstrated simpler single-stack architectures scale better. Masking the input tokens is an intentional trade off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeec1f2-63cd-4bf9-8c11-75fa57379d7e",
   "metadata": {},
   "source": [
    "### Grouped Query Attention (GQA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed5ec8-c601-460e-8582-d868c0937d66",
   "metadata": {},
   "source": [
    "The standard Masked Self Attention definition from Vaswani et al. (2017) is quadratic in computational complexity and memory overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb7fd7-e85f-4a0b-b3c3-acdd92fdfe73",
   "metadata": {},
   "source": [
    "### Rotary Position Embedding (RoPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4f716e-be29-4845-bc4e-d5d3ab953763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f665b-721e-4ca3-9d76-c9b8b06060cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Masked Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee9eb4-9752-4d28-b39e-0f6d798b43ae",
   "metadata": {},
   "source": [
    "Given $n$ input embeddings of length $d$ are stacked in an $n \\times d$ tensor $\\mathbf{X}$, the masked self attention function used by Llama can be expressed using the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{A} = softmax\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}} + \\mathbf{M}\\right)\\mathbf{V} \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ are all linear projections of $\\mathbf{X}$.\n",
    "* $\\mathbf{Q}$ is an $n \\times d$ tensor of *queries* that represent selection criteria for surrounding embeddings that would add valuable context to the current representation.\n",
    "* $\\mathbf{K}$ is an $n \\times d$ tensor of *keys* that represent characteristics that satisfy the selection criteria in the queries.\n",
    "* $\\mathbf{V}$ is an $n \\times d$ tensor of *values* that represent the contextual signals to transfer from one embedding to another.\n",
    "* The $\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}}$ term is an $n \\times n$ tensor of \"attention weights\" that define how much of each embedding's values to include from $\\mathbf{V}$.\n",
    "* $\\mathbf{M}$ is an $n \\times n$ attention mask that prevents earlier embeddings from attending to later ones by adding a $-\\infty$ bias to the later embeddings' scores.\n",
    "* $\\mathbf{A}$ is an $n \\times d$ tensor of attention representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9daaa9a-e6d2-4b49-b7a9-6b03d481d436",
   "metadata": {},
   "source": [
    "The idea is the $\\mathbf{Q}\\mathbf{K}^T$ term calculates the *angular distance* between each query vector $\\mathbf{q}_i$ and key vector $\\mathbf{k}_j$ by taking their dot product. The smaller the angle, the closer the vectors, and the better the match between $\\mathbf{q}_i$ and $\\mathbf{k}_j$. The result of the $\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_K}}$ term is an $n \\times n$ tensor of raw scores where row $i$ represents query $\\mathbf{q}_i$ and column $j$ represents how well key $\\mathbf{k}_j$ matches $\\mathbf{q}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d785788-bf74-4ddc-9192-7e9f1b50ba7c",
   "metadata": {},
   "source": [
    "The idea is the $\\mathbf{Q}\\mathbf{K}^T$ term calculates the distance between each query $\\mathbf{q}_i$ and key $\\mathbf{k}_j$ where *distance* is measured by the angle between the vectors. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The smaller the angle, the closer the vectors, and the better the match between $\\mathbf{q}_i$ and $\\mathbf{k}_j$. The result of the $\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}}$ term is an $n \\times n$ tensor of scores between each query and key. Next, the attention mask $\\mathbf{M}$ is added to prevent later embeddings from influencing earlier embeddings by adding a $-\\infty$ bias to their scores. Finally, these are all passed to $softmax$ to normalize the scores across the keys. The result is an $n \\times n$ tensor where each row $i$ represents query $\\mathbf{q}_i$ and each column $j$ represents how well key $\\mathbf{k}_j$ matches $\\mathbf{q}_i$. These scores are then applied to the values in $\\mathbf{V}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f56163-5fc8-4334-a204-b4db356d6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Position Encoding with RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ec9c4-ed84-40ec-9b6d-62e4eb8af7d3",
   "metadata": {},
   "source": [
    "The relevance of one embedding to another is heavily influenced by the distance between them. We saw last time that BERT-based models encode the absolute token positions directly in the token embeddings. More recent models including Llama have adopted *relative* position encoding schemes that have been shown to perform better on much longer sequences.\n",
    "\n",
    "Llama uses an approach known as Rotary Position Embedding (RoPE) from Su et al. (2021). Instead of baking the positions into the token embeddings at the beginning, RoPE modifies the attention equation to take the positions into account.\n",
    "\n",
    "integrates the embedding positions into the attention calculation. We saw in the last section that the attention calculation relies on the angular distance between queries and keys. The main idea behind RoPE is to convert the distance between two embeddings in the sequence into the angular distance between their vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22d429-c695-4315-9d67-4278cb60564a",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"rope.svg\" width=\"940\">\n",
    "<figcaption>Figure X: RoPE Concept in 2D</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b9014-ffb5-4d91-bb3e-525371e904df",
   "metadata": {},
   "source": [
    "Figure X illustrates the intuition behind RoPE in 2-dimensions. On the left, we have a sequence of 2-dimensional vectors $\\begin{bmatrix}\\mathbf{x}_0 & \\mathbf{x}_1 & \\mathbf{x}_2\\end{bmatrix}^T$ with their 2-dimensional geometric interpretation plotted on the right. The matrix in the center represents a rotational transformation. Given an embedding's position $m$, the idea behind RoPE is to rotate the embedding vector a distance of $m \\theta$. Following this idea, $\\mathbf{x}_0$ would stay the same, $\\mathbf{x}_1$ would be rotated a distance of $\\theta$, and $\\mathbf{x}_2$ would be rotated $2 \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197b12c2-0768-43fe-b87a-0d2e08b1ee72",
   "metadata": {},
   "source": [
    "While Figure X illustrates the idea behind RoPE in 2D, implementing it for n-dimensional vectors is a little more complicated. In practice, RoPE splits each embedding $\\mathbf{x}$ into pairs $\\set{(x_0, x_1), (x_2, x_3), \\dots, (x_{d-2}, x_{d-1})}$, and then applies 2D rotations to each pair just like Figure X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96520746-afec-46bb-9e27-ce016eac27aa",
   "metadata": {},
   "source": [
    "\n",
    "While Figure X illustrates the idea behind RoPE in 2D, implementing it for n-dimensional vectors is a little more complicated. In practice, RoPE splits the token embeddings into pairs, e.g. {::nomarkdown}$\\Set{(\\mathbf{x}_0,\\mathbf{x}_1), (\\mathbf{x}_2,\\mathbf{x}_3), \\dots}${:/}, and then applies 2D rotations to each pair using the rotation matrix $\\mathbf{R}_{\\Theta,m}^d$.\n",
    "\n",
    "Given a hyperparameter $\\Theta$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{R}_{\\Theta,m}^d &= \n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) & -sin(m \\theta_0) & 0 & 0 & \\dots & 0 & 0 \\\\\n",
    "sin(m \\theta_0) & cos(m \\theta_0) & 0 & 0 & \\dots & 0 & 0 \\\\\n",
    "0 & 0 & cos(m \\theta_1) & -sin(m \\theta_1) & \\dots & 0 & 0 \\\\\n",
    "0 & 0 & sin(m \\theta_1) & cos(m \\theta_1) & \\dots & 0 & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & \\dots & 0 & 0 & cos(m \\theta_{d/2-1}) & -sin(m \\theta_{d/2-1}) \\\\\n",
    "0 & 0 & \\dots & 0 & 0 & sin(m \\theta_{d/2-1}) & cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\text{where } \\theta_i &= \\frac{1}{\\Theta^{2i/d_{head}}}, \\text{and } i \\text{ is the embedding index}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Luckily, Su et al. (2021) give us a more compact approach to apply $\\mathbf{R}_{\\Theta,m}^d$ in practice. To implement RoPE, we'll calculate the $cos$ and $sin$ terms in the following equation up front and then share them across all the layers.\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{\\Theta,m}^d \\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\vdots \\\\\n",
    "x_{d-2} \\\\\n",
    "x_{d-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "-x_1 \\\\\n",
    "x_0 \\\\\n",
    "-x_3 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "-x_{d-1} \\\\\n",
    "x_{d-2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdbddc2-e126-4b68-9e46-7ced9c3d2087",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{R}_{\\Theta,m}^d \\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\vdots \\\\\n",
    "x_{d-2} \\\\\n",
    "x_{d-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "-x_1 \\\\\n",
    "x_0 \\\\\n",
    "-x_3 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "-x_{d-1} \\\\\n",
    "x_{d-2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "x_0 cos(m \\theta_0) - x_1 sin(m \\theta_0)\\\\\n",
    "x_0 sin(m \\theta_0) + x_1 cos(m \\theta_0)\\\\\n",
    "x_2 cos(m \\theta_1) - x_3 sin(m \\theta_1)\\\\\n",
    "x_2 sin(m \\theta_1) + x_3 cos(m \\theta_1)\\\\\n",
    "\\vdots \\\\\n",
    "x_{d-2} cos(m \\theta_{d/2-1}) - x_{d-1} sin(m \\theta_{d/2-1})\\\\\n",
    "x_{d-2} sin(m \\theta_{d/2-1}) + x_{d-1} cos(m \\theta_{d/2-1})\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a02dd4b-40b5-4d3d-865f-7aa806b24d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "base = config.rope_theta\n",
    "d = config.d_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1dbb162-c84c-4b1c-b8f6-8df573f7dd9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute theta_i = 1 / base^(2i/d) from i = 0 to d/2-1\n",
    "thetas = 1.0 / base**(2 * torch.arange(d // 2, device=device) / d)\n",
    "\n",
    "thetas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "381948de-dc81-4966-8195-cb1604ce9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rope(n):\n",
    "    \"\"\"Calculate RoPE rotation matrices to be shared across all layers.\"\"\"\n",
    "    \n",
    "    # Hyperparams\n",
    "    base = config.rope_theta\n",
    "    d = config.d_head\n",
    "    \n",
    "    # Compute theta_i = 1 / base^(2i/d) from i = 0 to d/2-1\n",
    "    thetas = 1.0 / base**(2 * torch.arange(d // 2, device=device) / d)\n",
    "    \n",
    "    # Compute m * theta_i for position m in 0 to n\n",
    "    frequencies = torch.stack([m*thetas for m in range(n)])\n",
    "    \n",
    "    # Duplicate each row\n",
    "    frequencies = torch.cat((frequencies, frequencies), dim=-1)\n",
    "    \n",
    "    # Apply cos, sin\n",
    "    cos = torch.cos(frequencies)\n",
    "    sin = torch.sin(frequencies)\n",
    "    \n",
    "    # Sanity check\n",
    "    assert cos.shape[0] == n and cos.shape[1] == config.d_head\n",
    "    assert sin.shape[0] == n and sin.shape[1] == config.d_head\n",
    "\n",
    "    return cos, sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2d615bd-0d4d-41de-ae67-8c4d890e6600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([22, 128]), torch.Size([22, 128]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute RoPE rotation matrices\n",
    "rope_cos, rope_sin = rope(len(x))\n",
    "\n",
    "rope_cos.shape, rope_sin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f6e1d-418b-433f-964b-169033c7b1f5",
   "metadata": {},
   "source": [
    "### Attention Workflow\n",
    "\n",
    "Before we get to the attention code, let's rewrite the attention equation one more time with all of the elements we need to calculate.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{A} &= softmax\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}} + \\mathbf{M}\\right)\\mathbf{V} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "expands to\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{A} &= softmax\\left(\\frac{(\\mathbf{R}_{\\Theta}^d\\mathbf{W}_Q\\mathbf{X})(\\mathbf{R}_{\\Theta}^d\\mathbf{W}_K\\mathbf{X})^T}{\\sqrt{d}} + \\mathbf{M}\\right)\\mathbf{W}_V\\mathbf{X} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\mathbf{R}_{\\Theta}^d$ is the RoPE rotation tensor.\n",
    "* $\\mathbf{W}_Q$, $\\mathbf{W}_K$, $\\mathbf{W}_V$ are the linear projections for queries, keys, and values respectively.\n",
    "\n",
    "The flowchart below enumerates the steps we'll walk through to calculate $\\mathbf{A}$. While it may look like there are a lot of steps, the goal is to break the calculation down into small enough pieces that each one is only a few lines of code.\n",
    "\n",
    "<figure>\n",
    "<img src=\"attention-flow.svg\" width=\"700\">\n",
    "<figcaption>Figure X: Attention Workflow</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ec00f-903d-451a-9eaf-20d703c44878",
   "metadata": {},
   "source": [
    "### Normalize Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fbfb22-ec40-4abe-bbb6-0bd535317b67",
   "metadata": {},
   "source": [
    "In the original Transformer architecture, the Attention and FFN blocks normalized the outputs of each sub-layer. In contrast, Llama normalizes the inputs to each sub-layer to improve training stability. Furthermore, they also replace the standard LayerNorm algorithm with the RMSNorm algorithm designed by Zhang and Sennrich (2019) to be less computationally expensive and scale better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c78ae6a-358f-4c39-bacd-8ced26179ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention normalization\n",
    "normalize_attention = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(normalize_attention, \"normalize_attention\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ba28ad6-ee8d-456d-9906-b899f4fba272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preserve residuals\n",
    "residual = x\n",
    "\n",
    "# Normalize attention inputs\n",
    "x = normalize_attention(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbfb7ec-4bf4-43c2-8fa5-c5a83a71abf7",
   "metadata": {},
   "source": [
    "### Project Queries, Keys, Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74990d-7f9e-4ad5-8fc3-b9f8b4737b4c",
   "metadata": {},
   "source": [
    "Next, we'll configure and then apply the linear projections $\\mathbf{W}_Q$, $\\mathbf{W}_K$, $\\mathbf{W}_V$ that map input embeddings $\\mathbf{X}$ to query, key, and value subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5b9b235-9849-417d-97ab-ee9c467a4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure query, key, value projections\n",
    "w_q = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "w_k = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "w_v = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(w_q, \"w_q\", w_k, \"w_k\", w_v, \"w_v\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03a5a60e-f1d9-465a-ac74-5463fbb4b5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([22, 4096]), torch.Size([22, 1024]), torch.Size([22, 1024]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project embeddings to query, key, value spaces\n",
    "q = w_q(x)\n",
    "k = w_k(x)\n",
    "v = w_v(x)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a7924c-93d7-4786-ab39-7603d9e8847a",
   "metadata": {},
   "source": [
    "One of the challenges with scaling Transformers is the quadratic complexity of comparing every query to every key in $\\mathbf{Q}\\mathbf{K}^T$. If you look closely at the output above, you'll notice the dimension of $\\mathbf{Q}$ is 4 times that of $\\mathbf{K}$ and $\\mathbf{V}$. This is because Llama uses a technique called Grouped Query Attention (GQA) from Ainslie et al. (2023). Instead of each attention head learning a separate set of queries, keys, and values, the attention heads are arranged in groups that share a set of keys and values across the group, dramatically reducing the compute and memory resources that are required. (Touvron et al. 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f0a35-20df-41f1-88d2-be4d315a47f8",
   "metadata": {},
   "source": [
    "### Split Attention Heads\n",
    "\n",
    "Next, we'll take the full scale queries, keys, and values, and split them into attention heads. The resulting tensors will have shape $(h, n, d_{head})$ where $h$ is the number of heads. Note there are 32 query heads but only 8 key/value heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba71ca0f-fce8-4f4f-8736-738d96b38af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, n_heads):\n",
    "    return x.view(-1, n_heads, config.d_head).transpose(-3, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5106a5d7-071a-4b79-802d-485ca673fdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 22, 128]), torch.Size([8, 22, 128]), torch.Size([8, 22, 128]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split attention heads\n",
    "q = split_heads(q, config.n_heads)\n",
    "k = split_heads(k, config.n_kv_heads)\n",
    "v = split_heads(v, config.n_kv_heads)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1a332-cea9-4a8b-9359-682bd74d80bc",
   "metadata": {},
   "source": [
    "### Encode Positions\n",
    "\n",
    "Now that we've split queries, keys, and values into attention heads, it's time to rotate the queries and keys according to the embedding positions using the RoPE rotation matrix $\\mathbf{R}_{\\Theta}^d$.\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{\\Theta,m}^d \\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\vdots \\\\\n",
    "x_{d/2-2} \\\\\n",
    "x_{d/2-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "-x_1 \\\\\n",
    "x_0 \\\\\n",
    "-x_3 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "-x_{d/2-1} \\\\\n",
    "x_{d/2-2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b71de-de76-4c5d-a99f-0809df12d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Convert x to -x1, x0, ... for the sin column of RoPE rotation matrix.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb84aa-df57-47e6-860c-a29c429e4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode positions by rotating queries and keys\n",
    "q = (q * rope_cos) + (rotate_half(q) * rope_sin)\n",
    "k = (k * rope_cos) + (rotate_half(k) * rope_sin)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444ad9c-9a0b-43bd-8afd-18a5c581777e",
   "metadata": {},
   "source": [
    "### Expand Key / Value Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edbc7c2-40a5-4139-ab98-8127e5649212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand key/value groups\n",
    "k = k.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "v = v.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c1377-e336-4f16-9b72-c237177dc5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert q.shape == k.shape == v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb2b562-517e-4d6e-afc8-a9c237d516ae",
   "metadata": {},
   "source": [
    "### Calculate Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d4840-92df-45e2-8bca-5f187f3a57c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attention mask M\n",
    "n = len(x)\n",
    "mask = torch.ones(n, n, dtype=torch.bool, device=device).tril(diagonal=0)\n",
    "m = torch.zeros(n, n, device=device).masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a39d5f-f2d3-47d8-81a6-bb086672425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attention for all heads in parallel\n",
    "a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head) + m, dim=-1) @ v\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3145f-5cd7-491d-a4c5-a3a8adc4d076",
   "metadata": {},
   "source": [
    "### Recombine Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80b30d-50ee-4513-9ea3-9ff428dcad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_heads(x):\n",
    "    return x.transpose(-3, -2).contiguous().view(-1, int(config.n_heads * config.d_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4a2ec-12e4-4eaf-b657-ac91a22965fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine attention heads\n",
    "a = combine_heads(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4d5595-1e05-4291-8bd8-9b33c2d929df",
   "metadata": {},
   "source": [
    "### Project Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4991289-e866-4c2c-a1d8-3750e1e115c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention output projection\n",
    "attention_outputs = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(attention_outputs, \"attention_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceadcd83-ed8e-4f9b-866b-82b415298ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project attention embeddings back to model space\n",
    "a = attention_outputs(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130f43c-7246-4ba1-813d-566af8545f09",
   "metadata": {},
   "source": [
    "### Combine Outputs with Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c943c2-3e22-4742-a0ce-84567abcff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine attention embeddings with residuals\n",
    "x = residual + a\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a27df-3e8e-4e9e-a71b-82e28338170f",
   "metadata": {},
   "source": [
    "## Feedforward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6545d-12b2-4984-9ac1-3d3925d381cd",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"ffn-flow.svg\" width=\"940\">\n",
    "<figcaption>Figure X: FFN Flow</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96852a87-ffc1-400a-9cc4-5f79a1853428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87b6a353-547a-47f7-8dec-667ce4c3608a",
   "metadata": {},
   "source": [
    "### Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cab172-1b6d-4384-9811-e1c442de5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FFN normalization\n",
    "normalize_ffn = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained state\n",
    "llama.load_state(normalize_ffn, \"normalize_ffn\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9dd0c0-afa0-4cc9-9a56-4bd0b6700842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve residuals\n",
    "residual = x\n",
    "\n",
    "# Normalize FFN inputs\n",
    "x = normalize_ffn(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258318e4-65b9-47ae-b934-244b9e2775ef",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08bb80-e138-4ac8-bdc2-1885833ab302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SwiGLU FFN\n",
    "ffn_gates = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_ffn,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "ffn_inputs = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_ffn,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(ffn_gates, \"ffn_gates\", ffn_inputs, \"ffn_inputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b82390-e39a-46d0-8475-f60c84af619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transform\n",
    "f = silu(ffn_gates(x)) * ffn_inputs(x)\n",
    "\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c881268-c8d4-4f6e-a5c9-e5d65765f13d",
   "metadata": {},
   "source": [
    "### Project Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a8505-687d-4123-8290-b3e270f5780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FFN output projection\n",
    "ffn_outputs = nn.Linear(\n",
    "    in_features=config.d_ffn,\n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(ffn_outputs, \"ffn_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad071436-a373-47ef-a434-bdf41bcbb91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project FFN embeddings back to model space\n",
    "f = ffn_outputs(f)\n",
    "\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f97bd7-9de5-4903-9b7c-aab5412587a8",
   "metadata": {},
   "source": [
    "### Combine Outputs with Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac86a4a-540d-437b-b8cc-8e24977f4417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine FFN embeddings with residuals\n",
    "x = residual + f\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c143495-e6ba-4998-99e9-a4ada9552636",
   "metadata": {},
   "source": [
    "## Stacking the Layers Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc90f22b-1171-49f1-b2d8-73569b16cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_layers(x):\n",
    "    # Compute RoPE rotation matrices\n",
    "    rope_cos, rope_sin = rope(len(x))\n",
    "\n",
    "    # Apply layer logic in a loop\n",
    "    for layer in range(config.n_layers):\n",
    "    \n",
    "        # Load pre-trained state for layer\n",
    "        load_pretrained_state(layer)\n",
    "    \n",
    "        #\n",
    "        # Attention\n",
    "        #\n",
    "    \n",
    "        # Normalize attention inputs\n",
    "        residual = x\n",
    "        x = normalize_attention(x)\n",
    "        \n",
    "        # Project embeddings to query, key, value spaces\n",
    "        q = w_q(x)\n",
    "        k = w_k(x)\n",
    "        v = w_v(x)\n",
    "        \n",
    "        # Split attention heads\n",
    "        q = split_heads(q, config.n_heads)\n",
    "        k = split_heads(k, config.n_kv_heads)\n",
    "        v = split_heads(v, config.n_kv_heads)\n",
    "    \n",
    "        # Encode positions by rotating queries and keys\n",
    "        q = (q * rope_cos) + (llama.rotate_half(q) * rope_sin)\n",
    "        k = (k * rope_cos) + (llama.rotate_half(k) * rope_sin)\n",
    "        \n",
    "        # Expand key/value groups\n",
    "        k = k.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "        v = v.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "    \n",
    "        # Compute masked attention bias M\n",
    "        n = len(x)\n",
    "        mask = torch.ones(n, n, dtype=torch.bool, device=device).tril(diagonal=0)\n",
    "        m = torch.zeros(n, n, device=device).masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "        \n",
    "        # Compute attention for all heads in parallel\n",
    "        a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head) + m, dim=-1) @ v\n",
    "    \n",
    "        # Combine attention heads\n",
    "        a = combine_heads(a)\n",
    "        \n",
    "        # Project attention embeddings back to model space\n",
    "        a = attention_outputs(a)\n",
    "        \n",
    "        # Combine attention embeddings with residuals\n",
    "        x = residual + a\n",
    "        \n",
    "        #\n",
    "        # FFN\n",
    "        #\n",
    "    \n",
    "        # Normalize FFN inputs\n",
    "        residual = x\n",
    "        x = normalize_ffn(x)\n",
    "    \n",
    "        # Apply transform\n",
    "        f = silu(ffn_gates(x)) * ffn_inputs(x)\n",
    "    \n",
    "        # Project FFN embeddings back to model space\n",
    "        f = ffn_outputs(f)\n",
    "        \n",
    "        # Combine FFN embeddings with residuals\n",
    "        x = residual + f\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f585aa5-03e2-42c4-8582-0200d092e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start over from initial tokens\n",
    "x = torch.tensor(token_ids, device=device)\n",
    "\n",
    "# Initial embeddings\n",
    "x = embeddings(x)\n",
    "\n",
    "# Contextualized embeddings\n",
    "x = context_layers(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d86826-0a33-4a43-90c0-a3eae526b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1603550e-f8c5-44fc-b8d1-19428aeff5b5",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d29083-7ea8-41cf-9299-1466c5f5014a",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"head-flow.svg\" width=\"700\">\n",
    "<figcaption>Figure X: Head Flow</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ea5d20-7fe6-4e71-bca1-4aef72dc803d",
   "metadata": {},
   "source": [
    "### Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f030e10b-a7e1-4df9-8460-ece7d8d4bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure head normalization\n",
    "normalize_head = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(normalize_head, \"normalize_head\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cae1a9-25b7-43a3-9134-1fc7bce64f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize head inputs\n",
    "x = normalize_head(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9066d-73ab-48d0-9185-469f86f7839b",
   "metadata": {},
   "source": [
    "### Project Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c44655-5796-40f6-bbb1-300a1b6a86a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure output projection\n",
    "head_outputs = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.vocab_size,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(head_outputs, \"head_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09534e41-6d49-416a-b2a9-92ff4c402d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use last embedding to represent the entire sequence\n",
    "x = x[-1]\n",
    "\n",
    "# Project outputs to token space\n",
    "x = head_outputs(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb78c4-2e9e-430e-9c6e-bfd1e882e6fb",
   "metadata": {},
   "source": [
    "## Top Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528773e0-0d4b-4641-8998-c0693105d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top scoring token\n",
    "token_id = x.argmax()\n",
    "\n",
    "# Decode token\n",
    "token = tokenizer.decode([token_id]).strip()\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b84b5-3fce-4f46-8d39-867eec194cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify answer\n",
    "assert token == \"Boston\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37cf2c-ceb5-4420-b2dd-0cc6e1c5df93",
   "metadata": {},
   "source": [
    "## Sample Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db6efc-ee1f-457d-834c-1577338c18c8",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb0fff-2f5e-4b0e-a2bf-b1861129bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "temperature = config.temperature\n",
    "temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3879e58-b9d7-451d-b3da-a4387ad48ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply temperature\n",
    "x = x / temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366bad75-488b-4951-999a-457d2f75e69e",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49c1ed-a77d-4ec3-ba14-908e69452edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to probabilities\n",
    "probs = softmax(x)\n",
    "\n",
    "# Sort probabilities in descending order\n",
    "probs, indices = probs.sort(descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1108c86-6859-45d8-88ae-dae902d7b22a",
   "metadata": {},
   "source": [
    "### Top K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e33b2-17c8-4f86-a540-b4d0154c34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "top_k = config.top_k\n",
    "top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f1989-6a8e-45a6-a791-975fd0cd4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain top k tokens\n",
    "probs = probs[:top_k]\n",
    "print(f\"Retained {len(probs)} of {len(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec20b139-494f-47cb-b772-3c63c3de9211",
   "metadata": {},
   "source": [
    "### Top P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de639c47-170e-41a2-bf19-21ecff0992e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "top_p = config.top_p\n",
    "top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c052e39d-0dde-43fd-8a19-dbbb6d23af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cutoff where cumulative probability exceeds top_p\n",
    "cumulative_mask = probs.cumsum(dim=-1) > top_p\n",
    "threshold_index = torch.argmax(cumulative_mask).item()\n",
    "\n",
    "# Only apply threshold if top_p was exceeded\n",
    "if cumulative_mask.any():\n",
    "    probs = probs[:threshold_index+1]\n",
    "\n",
    "print(f\"Retained {len(probs)} of {len(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dea82f-bf61-434e-849f-ef5bce79f9cb",
   "metadata": {},
   "source": [
    "### Random Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed89c4-95e3-4431-aa13-ac507c982cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print remaining token pool\n",
    "for i, prob in enumerate(probs):\n",
    "    print(f\"token id {indices[i]}, token '{tokenizer.decode([indices[i]])}', score {prob:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39fde30-3a77-4752-941f-ced72393c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from remaining tokens weighted by probability\n",
    "sampled_index = torch.multinomial(probs, 1)\n",
    "\n",
    "# Convert sampled_index to original logits\n",
    "token_id = indices[sampled_index]\n",
    "\n",
    "# Decode token\n",
    "token = tokenizer.decode([token_id]).strip()\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c8acd-fe53-4c1f-8080-1d6bfaf0b027",
   "metadata": {},
   "source": [
    "## Complete Head Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f696ef95-7597-4f2b-bfd1-835956bb7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def head(x):\n",
    "    # Normalize head inputs\n",
    "    x = normalize_head(x)\n",
    "    \n",
    "    # Use last embedding to represent the entire sequence\n",
    "    x = x[-1]\n",
    "    \n",
    "    # Project outputs to token space\n",
    "    x = head_outputs(x)\n",
    "\n",
    "    #\n",
    "    # Temperature\n",
    "    #\n",
    "    \n",
    "    # Apply temperature\n",
    "    x = x / config.temperature\n",
    "\n",
    "    #\n",
    "    # Ranking\n",
    "    #\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probs = softmax(x)\n",
    "    \n",
    "    # Sort probabilities in descending order\n",
    "    probs, indices = probs.sort(descending=True)\n",
    "\n",
    "    #\n",
    "    # Top K\n",
    "    #\n",
    "    \n",
    "    # Retain top k tokens\n",
    "    probs = probs[:config.top_k]\n",
    "\n",
    "    #\n",
    "    # Top P\n",
    "    #\n",
    "    \n",
    "    # Find cutoff where cumulative probability exceeds top_p\n",
    "    cumulative_mask = probs.cumsum(dim=-1) > config.top_p\n",
    "    threshold_index = torch.argmax(cumulative_mask).item()\n",
    "    \n",
    "    # Only apply threshold if top_p was exceeded\n",
    "    if cumulative_mask.any():\n",
    "        probs = probs[:threshold_index+1]\n",
    "\n",
    "    #\n",
    "    # Random Selection\n",
    "    #\n",
    "    \n",
    "    # Sample from remaining tokens weighted by probability\n",
    "    sampled_index = torch.multinomial(probs, 1)\n",
    "    \n",
    "    # Convert sampled_index to original logits\n",
    "    token_id = indices[sampled_index]\n",
    "\n",
    "    return token_id.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568352c3-2573-440f-9c1e-ef0acccd245b",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2328afe7-d40a-4d47-91cd-be59b02b4485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9533b1-7011-4f84-85b5-51ca93b15b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@validate_call\n",
    "def prepare_messages(messages: list[Message]):\n",
    "    # Initialize prompt\n",
    "    prompt = \"\"\n",
    "    \n",
    "    # Format each message\n",
    "    for message in messages:\n",
    "        prompt += f\"<|start_header_id|>{message.role}<|end_header_id|>\\n\\n\"\n",
    "        prompt += message.content\n",
    "        prompt += \"<|eot_id|>\"\n",
    "\n",
    "    # Finish with the assistant role to prime the model's response\n",
    "    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b875c2e-a1ac-45dd-8df3-02398fa5c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@validate_call\n",
    "def generate(messages: list[Message]):\n",
    "    # Format message prompt\n",
    "    prompt = prepare_messages(messages)\n",
    "    \n",
    "    # Split raw text into tokens\n",
    "    token_ids = tokenizer.encode(prompt, bos=True, eos=False, allowed_special=\"all\")\n",
    "    \n",
    "    # Generate output until we get a stop token or we exceed max_output_tokens.\n",
    "    for _ in range(config.max_output_tokens):\n",
    "        \n",
    "        # Start over from initial tokens\n",
    "        x = torch.tensor(token_ids, device=device)\n",
    "        \n",
    "        # Initial embeddings\n",
    "        x = embeddings(x)\n",
    "        \n",
    "        # Semantic embeddings\n",
    "        x = context_layers(x)\n",
    "        \n",
    "        # Head\n",
    "        token_id = head(x)\n",
    "        \n",
    "        # Check stopping criteria\n",
    "        if token_id in tokenizer.stop_tokens:\n",
    "            break\n",
    "    \n",
    "        # Print token\n",
    "        token = tokenizer.decode([token_id])\n",
    "        stdout.write(token)\n",
    "        \n",
    "        # Append to end of sequence\n",
    "        token_ids.append(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e8bf24-cb98-4618-a481-5712d9aa5477",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate([\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is capital of Massachusetts?\",\n",
    "    },\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7c054-8863-4b24-83ba-24415c78a553",
   "metadata": {},
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf596e24-2e31-47f9-9bfc-be0731542b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725cb1fc-7c58-4ca9-a55e-929a125eefea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faee9494-3ca7-499d-b072-d71b87fbbe33",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b07abe-b5ad-4452-9067-ea9ee1e56982",
   "metadata": {},
   "source": [
    "Ainslie, Joshua, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. “GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.” arXiv. <https://doi.org/10.48550/arXiv.2305.13245>.\n",
    "\n",
    "Bengio, Yoshua, Réjean Ducharme, and Pascal Vincent. 2000. “A Neural Probabilistic Language Model.” In Advances in Neural Information Processing Systems. Vol. 13. MIT Press. <https://proceedings.neurips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html>.\n",
    "\n",
    "Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” arXiv. <https://doi.org/10.48550/arXiv.2005.14165>.\n",
    "\n",
    "Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv.Org. May 24, 2019. <https://arxiv.org/abs/1810.04805v2>.\n",
    "\n",
    "Dubey, Abhimanyu, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, et al. 2024. “The Llama 3 Herd of Models.” arXiv.Org. July 31, 2024. <https://arxiv.org/abs/2407.21783v2>.\n",
    "\n",
    "He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” arXiv. <https://doi.org/10.48550/arXiv.1512.03385>.\n",
    "\n",
    "Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. “Improving Language Understanding by Generative Pre-Training.”\n",
    "\n",
    "Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. “Neural Machine Translation of Rare Words with Subword Units.” arXiv. <https://doi.org/10.48550/arXiv.1508.07909>.\n",
    "\n",
    "Shazeer, Noam. 2020. “GLU Variants Improve Transformer.” arXiv. <https://doi.org/10.48550/arXiv.2002.05202>.\n",
    "\n",
    "Stanford Online, dir. 2024. Stanford CS25: V4 I Hyung Won Chung of OpenAI. <https://www.youtube.com/watch?v=orDKvo8h71o>.\n",
    "\n",
    "Su, Jianlin, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2021. “RoFormer: Enhanced Transformer with Rotary Position Embedding.” arXiv.Org. April 20, 2021. <https://arxiv.org/abs/2104.09864v5>.\n",
    "\n",
    "Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, et al. 2023. “LLaMA: Open and Efficient Foundation Language Models.” arXiv. <https://doi.org/10.48550/arXiv.2302.13971>.\n",
    "\n",
    "Touvron, Hugo, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et al. 2023. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” arXiv.Org. July 18, 2023. <https://arxiv.org/abs/2307.09288v2>.\n",
    "\n",
    "Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” arXiv. <https://doi.org/10.48550/arXiv.1706.03762>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "stickshift": {
   "description": "Trace an Inference Through Each Layer of the SOTA Llama 3.1 Foundation Models",
   "title": "Transformer Teardown: Llama 3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
