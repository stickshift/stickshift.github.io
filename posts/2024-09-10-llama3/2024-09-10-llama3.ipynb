{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06347ff4-85c9-49dc-a561-cfea5aaf4db3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "source": [
    "# Transformer Teardown: Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da22c3d-c844-4280-9b4f-c840bb2d865d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In [the last Transformer Teardown](https://stickshift.github.io/2024/09/04/transformer-teardown.html), we dissected a DistilBERT text classification pipeline, tracing a single inference through the entire Transformer stack from raw data to final prediction. We learned about the main stages of a Transformer pipeline as well as fundamental Transformer concepts such as token embeddings and Multi-Head Self Attention.\n",
    "\n",
    "Exploring BERT-based models is a fantastic way to see the core ideas from Vaswani et al.'s original Transformer paper in action. But BERT was released 6 years ago! It would be another 4 years before ChatGPT would even exist! It's safe to say a lot has changed since then.\n",
    "\n",
    "The goal of this post is to fast forward to present day. We'll use the same teardown process to unpack the latest [Llama 3.1](https://llama.meta.com/) foundation models released by Meta last month. By comparing Llama 3.1 (Dubey et al. 2024) with Vaswani et al.'s *Vanilla* Transformer architecture (Vaswani et al. 2017), we'll see how the Transformer architecture has grown up—what's changed and what hasn't—over the past 7 years of AI revolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bdfe4-94f0-4e7d-a90f-defe1625c130",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d850f817-3a96-44a3-b551-2814dab297a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas import Series\n",
    "from pytest import approx\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu, silu, softmax\n",
    "\n",
    "from llama_models.llama3.api.tokenizer import Tokenizer\n",
    "from llama_models.llama3.reference_impl.model import RMSNorm\n",
    "\n",
    "import stickshift as ss\n",
    "from stickshift import default_arg, take\n",
    "from stickshift.models import llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c174d1af-fd9e-4d0d-9f08-e59ca4af5f35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure gpu\n",
    "device = ss.torch.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563bf2a8-5154-49d2-a5bc-83076d4d484f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".stickshift-figure {\n",
       "    display: block;\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".stickshift-figure {\n",
    "    display: block;\n",
    "    margin-left: auto !important;\n",
    "    margin-right: auto !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7355b475-d266-423d-9fe4-214bfcc9337d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Text Generation with Llama 3\n",
    "\n",
    "In our last post, we looked at a `text-classification` Transformer where the Head stage of the pipeline used the contextualized token embeddings as features in a binary classifier. Text classification Transformers are an ideal place to start because they're the simplest and most familiar. This time we're going to dissect a `text-generation` Transformer. Instead of simply applying a label to the input text, the Head stage in a text generation pipeline is responsible for *generating* new content. Don't worry—it's not as complicated as it sounds. By the end of this teardown, you'll have a solid grasp of how all the elements in a state-of-the-art generative model fit together.\n",
    "\n",
    "[Llama 3](https://llama.meta.com/) is a set of foundation models released by Meta over the summer that can \"answer questions in at least 8 languages, write high quality code, solve complex reasoning problems, and use tools in a zero-shot way.\" (Dubey et al. 2024) The Llama 3 release includes 8B, 70B, and 405B sizes. While Meta recommends a cluster with at least 8 GPUs to run the 70B and 405B models, you can run the 8B model on a single GPU w/ 20GB of memory.\n",
    "\n",
    "The goal of this teardown is to walk through each step in a Llama 3 text generation pipeline using only the research literature, weights from the `Meta-Llama3.1-8B-Instruct` checkpoint, and Meta's reference implementation as a guide. Along the way, we'll learn about the core improvements used by modern generative Transformers including:\n",
    "\n",
    "* Pre-normalization with RMSNorm (Brown et al. 2020), (Zhang and Sennrich 2019)\n",
    "* Rotary Position Encoding (RoPE) (Su et al. 2021)\n",
    "* SwiGLU Activation (Shazeer 2020)\n",
    "* Grouped Query Attention (GQA) (Ainslie et al. 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f9a785-f3f1-40d0-9665-b32958eb1098",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d92a6f-19b2-4355-a68d-d2deb93aff9a",
   "metadata": {},
   "source": [
    "Before we jump in, we need a prompt! This is a text-generation tutorial after all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d167217-0eb0-4408-b5ed-4fdc9f315e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = \"Write a haiku\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4d296-d70f-4597-96c0-7e5dbb448e5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Checkpoint: 8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a509bf-9a2b-4d6a-b4fd-322ed4af31eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We'll start by loading the hyperparameters and pre-trained weights for the `Meta-Llama3.1-8B-Instruct` checkpoint. The weights for all Llama checkpoints can be downloaded directly from [Meta](https://llama.meta.com/) and [Hugging Face](https://huggingface.co/meta-llama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55aa2322-410c-4720-9e45-2c9eb68dd8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checkpoint_path': PosixPath('/Users/andrewyoung/.llama/checkpoints/Meta-Llama3.1-8B-Instruct'),\n",
       " 'vocab_size': 128256,\n",
       " 'd_model': 4096,\n",
       " 'd_head': 128,\n",
       " 'd_ffn': 14336,\n",
       " 'n_layers': 32,\n",
       " 'n_heads': 32,\n",
       " 'n_kv_heads': 8,\n",
       " 'n_kv_groups': 4,\n",
       " 'rms_norm_eps': 1e-05,\n",
       " 'rope_theta': 500000.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model config\n",
    "config = llama.config(\"Meta-Llama3.1-8B-Instruct\")\n",
    "\n",
    "# Load model parameters\n",
    "checkpoint = torch.load(config.checkpoint_path / \"consolidated.00.pth\", weights_only=True, map_location=device)\n",
    "\n",
    "config.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "691d01c1-da77-4410-ab37-424024f5556e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_pretrained_state(layer):    \n",
    "    # Load pre-trained state\n",
    "    llama.load_state(\n",
    "        normalize_attention, \"normalize_attention\", \n",
    "        normalize_ffn, \"normalize_ffn\", \n",
    "        w_q, \"w_q\", \n",
    "        w_k, \"w_k\", \n",
    "        w_v, \"w_v\", \n",
    "        attention_outputs, \"attention_outputs\",\n",
    "        ffn_gates, \"ffn_gates\",\n",
    "        ffn_inputs, \"ffn_inputs\",\n",
    "        ffn_outputs, \"ffn_outputs\",\n",
    "        checkpoint=checkpoint,\n",
    "        layer=layer,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aff05e-a07b-4b36-a50f-c58cb6fc1e84",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Transformer Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d845533-c472-464c-9c00-275616d30693",
   "metadata": {},
   "source": [
    "The stages of a `text-generation` pipeline are the same as the `text-classification` pipeline we saw last time. The Tokenize stage splits raw text into tokens. The Embeddings stage converts individual tokens into embedding vectors. The Context Layers augment the input embeddings with contextual signals drawn from the surrounding tokens. The Head stage converts the contextualized embeddings into task-specific predictions.\n",
    "\n",
    "The key differences between text generation and text classification are the *task-specific* predictions. While text classification Transformers predict a label for the raw text, text generation Transformers predict the next token. The predicted token is appended to the end of the input sequence, and the process repeats.\n",
    "\n",
    "<img src=\"transformer-pipeline.svg\" class=\"stickshift-figure\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab8cd61-85ae-4fa6-acae-de7c7053b786",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdf7ba8-caf5-480b-bc8e-88aef06f3a1b",
   "metadata": {},
   "source": [
    "The Tokenize stage splits raw text into tokens using a fixed vocabulary. Llama 3 uses a vocabulary of 128k tokens built on top of [OpenAI's tiktoken](https://github.com/openai/tiktoken) tokenizer. We'll dig into the details of the later stages but here we'll simply use the off-the-shelf Tokenizer from Meta's [llama-models](https://github.com/meta-llama/llama-models) reference implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e32934a-bf49-46ab-8326-44d10d63e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer model from checkpoint\n",
    "tokenizer = Tokenizer(str(config.checkpoint_path / \"tokenizer.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c17a641c-8673-47cc-b5fd-f1f6f4dec4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 8144, 264, 6520, 39342]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split raw text into tokens\n",
    "token_ids = tokenizer.encode(prompt, bos=True, eos=False)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a5a247d-4cdc-47b3-bb66-2344ad22b360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Write a haiku'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode token ids back into raw text\n",
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa0acf-5be2-44e2-97f4-bbc4ba3e4486",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c9eee8-e93f-48d9-9e9c-ed97bf0e84e3",
   "metadata": {},
   "source": [
    "The Embeddings stage converts individual tokens into embedding vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a85e9908-06bd-4fe8-b2f4-1c67cd465f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings lookup table\n",
    "embeddings = nn.Embedding(\n",
    "    num_embeddings=config.vocab_size, \n",
    "    embedding_dim=config.d_model,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(embeddings, \"embeddings\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ccebd8f-8225-4bc3-8b12-a367620847e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load token_ids into a tensor\n",
    "token_values = torch.tensor(token_ids, device=device)\n",
    "\n",
    "token_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9499370b-69ae-4762-81f7-1adb37bcbb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Record sequence length n\n",
    "n = len(token_values)\n",
    "\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9018c1ba-d851-48c0-8aec-3362da89cecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4096])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map token values to embeddings\n",
    "x = embeddings(token_values)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91b63aee-ed54-425c-98cf-1306666eebe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
       "          6.3419e-05,  1.1902e-03],\n",
       "        [ 1.0925e-02, -3.4943e-03,  1.8997e-03,  ..., -1.0437e-02,\n",
       "         -5.5542e-03, -1.0864e-02],\n",
       "        [-1.3199e-03, -6.3324e-04, -8.8882e-04,  ..., -1.2329e-02,\n",
       "         -4.8218e-03,  6.7353e-06],\n",
       "        [-1.1841e-02, -4.9133e-03,  7.2021e-03,  ..., -1.1683e-04,\n",
       "          6.4087e-03,  5.0964e-03],\n",
       "        [-9.2773e-03,  1.1353e-02,  2.1729e-02,  ...,  1.8066e-02,\n",
       "          9.7656e-04,  4.0588e-03]], device='mps:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show sample\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d1ae0d-4702-461f-bcde-24f6d5857414",
   "metadata": {},
   "source": [
    "# Context Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690aea2b-ccc5-4480-b835-dcf0b40eabb7",
   "metadata": {},
   "source": [
    "The Context Layers in a Transformer are responsible for infusing each token embedding with contextual signals drawn from the rest of the sequence. The mechanism works by passing the token embeddings through multiple layers of attention and feedforward blocks. The attention blocks focus on relationships between tokens, augmenting each embedding with contextual information drawn from the surrounding embeddings. The feedforward blocks capitalize on the extra context, transforming each augmented embedding using a fully connected neural network. This pattern of attention and transformation is repeated over and over again, gradually converting representations of individual words into representations of abstract semantic concepts over a series of small increments.\n",
    "\n",
    "<img src=\"transformer-layers.svg\" class=\"stickshift-figure\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a29d2e-08b0-4e7a-ab5c-77752b44c271",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02279831-454b-44d8-9936-8e6dfc47873c",
   "metadata": {},
   "source": [
    "### Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "744b2da1-20bb-4e52-a6ea-a883e78bf7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention normalization\n",
    "normalize_attention = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(normalize_attention, \"normalize_attention\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae0e97bf-cb15-41dc-a103-2db9fed51076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4096])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize attention inputs\n",
    "residual = x\n",
    "x = normalize_attention(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e000d-2f6c-4517-89af-fa162d4c2e57",
   "metadata": {},
   "source": [
    "### Project Queries, Keys, Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c80d4879-ba66-449e-82cb-35e366d1cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure query, key, value projections\n",
    "w_q = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "w_k = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "w_v = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(w_q, \"w_q\", w_k, \"w_k\", w_v, \"w_v\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09a586eb-dc5c-482f-83d1-c8f358b2adec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4096]), torch.Size([5, 1024]), torch.Size([5, 1024]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project embeddings to query, key, value spaces\n",
    "q = w_q(x)\n",
    "k = w_k(x)\n",
    "v = w_v(x)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4fbce-a453-4ebd-b13e-0c014e69c6d6",
   "metadata": {},
   "source": [
    "### Split Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd757d84-d973-456a-b4ed-0dd57b31e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, n_heads):\n",
    "    return x.view(-1, n_heads, config.d_head).transpose(-3, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17d3364c-4647-4d83-bf0f-59f3d0f12c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 5, 128]), torch.Size([8, 5, 128]), torch.Size([8, 5, 128]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split attention heads\n",
    "q = split_heads(q, config.n_heads)\n",
    "k = split_heads(k, config.n_kv_heads)\n",
    "v = split_heads(v, config.n_kv_heads)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1228b160-f021-4529-8936-9daed82a8e27",
   "metadata": {},
   "source": [
    "### Encode Positions (RoPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09a2bc69-9bb6-47ab-a3cb-5b0f2a82c036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 128]), torch.Size([5, 128]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute rope_cos and rope_sin\n",
    "base = config.rope_theta\n",
    "d = config.d_head\n",
    "\n",
    "# Compute theta_i = 1 / base^(2i/d) from i = 0 to d/2-1\n",
    "thetas = 1.0 / base**(2 * torch.arange(d // 2, device=device) / d)\n",
    "\n",
    "# Compute m * theta_i for position m in 0 to n\n",
    "frequencies = torch.stack([m*thetas for m in range(n)])\n",
    "\n",
    "# Duplicate each row\n",
    "frequencies = torch.cat((frequencies, frequencies), dim=-1)\n",
    "\n",
    "# Apply cos, sin\n",
    "rope_cos = torch.cos(frequencies)\n",
    "rope_sin = torch.sin(frequencies)\n",
    "\n",
    "rope_cos.shape, rope_sin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13369549-f661-49e8-8bc2-60bd9e106afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert rope_cos.shape[0] == n and rope_cos.shape[1] == config.d_head\n",
    "assert rope_sin.shape[0] == n and rope_sin.shape[1] == config.d_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df4e744d-a49f-4087-81db-b23ea17051c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 5, 128]), torch.Size([8, 5, 128]), torch.Size([8, 5, 128]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode positions by rotating queries and keys\n",
    "q = (q * rope_cos) + (llama.rotate_half(q) * rope_sin)\n",
    "k = (k * rope_cos) + (llama.rotate_half(k) * rope_sin)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e4c08-a857-4ea7-a81d-513f3835ce1d",
   "metadata": {},
   "source": [
    "### Expand Key / Value Groups (GQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9ca6a71-f43a-4b6a-a84a-f0f7c53f5f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 5, 128]), torch.Size([32, 5, 128]), torch.Size([32, 5, 128]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand key/value groups\n",
    "k = k.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "v = v.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dc7802b-8b5a-43a7-9752-59aab46d1b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert q.shape == k.shape == v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21619c-70bf-4bca-bc0c-13704f17ed07",
   "metadata": {},
   "source": [
    "### Calculate Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d3f15e1-5de8-4fb5-a430-dcce398657ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]], device='mps:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute masked attention bias M\n",
    "mask = torch.ones(n, n, dtype=torch.bool, device=device).tril(diagonal=0)\n",
    "m = torch.zeros(n, n, device=device).masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18342d97-3e5c-483d-b11b-92dbb3a717d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5, 128])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute attention for all heads in parallel\n",
    "a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head) + m, dim=-1) @ v\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ae03c0-09d2-435a-9f53-0fd300b965f9",
   "metadata": {},
   "source": [
    "### Recombine Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "176a6b11-d24f-420f-b25e-0362c7f9a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_heads(x):\n",
    "    return x.transpose(-3, -2).contiguous().view(-1, int(config.n_heads * config.d_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05d7e054-822d-43eb-8e0b-8c068b22d941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4096])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine attention heads\n",
    "a = combine_heads(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d2d702-8cd1-4f46-b66d-49100a54ec7d",
   "metadata": {},
   "source": [
    "### Project Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca228f80-7a27-4f3d-bd5b-1c7fca3b9c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention output projection\n",
    "attention_outputs = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(attention_outputs, \"attention_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6078b47b-a104-4f05-b84c-161ab12a1fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4096])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project attention embeddings back to model space\n",
    "a = attention_outputs(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fc1dc6-7f72-4938-9503-9f02fc7c41ac",
   "metadata": {},
   "source": [
    "### Combine w/ Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c08c7041-cfae-4286-a5c0-a6ac1cf095b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4096])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine attention embeddings with residuals\n",
    "x = residual + a\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40394845-477a-4d74-b95b-a5df2b05690c",
   "metadata": {},
   "source": [
    "## FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b07fd-ec9d-493f-ac63-55ade8980d7b",
   "metadata": {},
   "source": [
    "### Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "282ff39a-0b80-43d9-b48c-282a72d4b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FFN normalization\n",
    "normalize_ffn = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained state\n",
    "llama.load_state(normalize_ffn, \"normalize_ffn\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "373aa399-40ba-423b-9871-25c0cd841489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4096])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize FFN inputs\n",
    "residual = x\n",
    "x = normalize_ffn(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7501951b-ba5d-4f58-853b-fb9e017a3cef",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21e42429-bfcb-43c8-b92f-0dbad588a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SwiGLU FFN\n",
    "ffn_gates = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_ffn,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "ffn_inputs = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_ffn,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(ffn_gates, \"ffn_gates\", ffn_inputs, \"ffn_inputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4927418-ffc4-4aac-b3c5-c62cfeb93f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 14336])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply FFN\n",
    "f = silu(ffn_gates(x)) * ffn_inputs(x)\n",
    "\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a383c198-a280-4675-ae24-dbc43c838b96",
   "metadata": {},
   "source": [
    "### Project Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26d4ac14-c19b-4102-9b49-824cd0e28de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FFN output projection\n",
    "ffn_outputs = nn.Linear(\n",
    "    in_features=config.d_ffn,\n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(ffn_outputs, \"ffn_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5baad73d-76c8-4cae-80b4-800563f92336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4096])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project FFN embeddings back to model space\n",
    "f = ffn_outputs(f)\n",
    "\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c274e4b-8d29-4e14-96ff-0113403b7012",
   "metadata": {},
   "source": [
    "### Combine w/ Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50883355-6553-45de-9e9b-3ad27371ff88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4096])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine FFN embeddings with residuals\n",
    "x = residual + f\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4afd793-c7d6-410f-8ea7-a0c87aaa368f",
   "metadata": {},
   "source": [
    "## Stacking the Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7caa67da-3466-4427-971e-8b212f33e84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4096])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start over from initial token embeddings\n",
    "x = embeddings(token_values)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94982f78-8e24-4f60-844e-941b38cca48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply layer logic in a loop\n",
    "for layer in range(config.n_layers):\n",
    "\n",
    "    # Load pre-trained state for layer\n",
    "    load_pretrained_state(layer)\n",
    "\n",
    "    #\n",
    "    # Attention\n",
    "    #\n",
    "\n",
    "    # Normalize attention inputs\n",
    "    residual = x\n",
    "    x = normalize_attention(x)\n",
    "    \n",
    "    # Project embeddings to query, key, value spaces\n",
    "    q = w_q(x)\n",
    "    k = w_k(x)\n",
    "    v = w_v(x)\n",
    "    \n",
    "    # Split attention heads\n",
    "    q = split_heads(q, config.n_heads)\n",
    "    k = split_heads(k, config.n_kv_heads)\n",
    "    v = split_heads(v, config.n_kv_heads)\n",
    "\n",
    "    # Encode positions by rotating queries and keys\n",
    "    q = (q * rope_cos) + (llama.rotate_half(q) * rope_sin)\n",
    "    k = (k * rope_cos) + (llama.rotate_half(k) * rope_sin)\n",
    "    \n",
    "    # Expand key/value groups\n",
    "    k = k.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "    v = v.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "\n",
    "    # Compute masked attention bias M\n",
    "    mask = torch.ones(n, n, dtype=torch.bool, device=device).tril(diagonal=0)\n",
    "    m = torch.zeros(n, n, device=device).masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "    \n",
    "    # Compute attention for all heads in parallel\n",
    "    a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head) + m, dim=-1) @ v\n",
    "\n",
    "    # Combine attention heads\n",
    "    a = combine_heads(a)\n",
    "    \n",
    "    # Project attention embeddings back to model space\n",
    "    a = attention_outputs(a)\n",
    "    \n",
    "    # Combine attention embeddings with residuals\n",
    "    x = residual + a\n",
    "    \n",
    "    #\n",
    "    # FFN\n",
    "    #\n",
    "\n",
    "    # Normalize FFN inputs\n",
    "    residual = x\n",
    "    x = normalize_ffn(x)\n",
    "\n",
    "    # Apply FFN\n",
    "    f = silu(ffn_gates(x)) * ffn_inputs(x)\n",
    "\n",
    "    # Project FFN embeddings back to model space\n",
    "    f = ffn_outputs(f)\n",
    "    \n",
    "    # Combine FFN embeddings with residuals\n",
    "    x = residual + f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae7bf87-1b85-45cc-9a5e-94bffa34d9c8",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a556f-ae27-4797-9f91-0330fda4b1e0",
   "metadata": {},
   "source": [
    "## Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "411660dd-e187-4043-90d1-5a7f519f425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure head normalization\n",
    "normalize_head = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(normalize_head, \"normalize_head\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c05af81b-b8b2-4c4e-b589-432316e3e7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4096])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize head inputs\n",
    "x = normalize_head(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c56884-1f30-4aa4-b83c-b8c8dbcfc0f4",
   "metadata": {},
   "source": [
    "## Predict Next Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94679c13-fffa-46b4-aad5-98eee9d8e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure output projection\n",
    "head_outputs = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.vocab_size,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(head_outputs, \"head_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51f794f5-7e09-4292-a83e-3e83db72d7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use last embedding to represent the entire sequence\n",
    "features = x[-1]\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a776956-9e97-4aec-b1ac-6933e7d3338d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' about'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict next token\n",
    "logits = head_outputs(features)\n",
    "token_id = logits.argmax()\n",
    "\n",
    "tokenizer.decode([token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6716036-205c-492d-a5df-781fe1afdf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9737fc2f-e297-477c-a9ca-4fec37475681",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "stickshift": {
   "description": "Trace an Inference from Raw Data to Prediction Through Llama 3 text-generation Pipeline",
   "title": "Transformer Teardown: Llama 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
