{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06347ff4-85c9-49dc-a561-cfea5aaf4db3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "source": [
    "# Transformer Teardown: Llama 3\n",
    "\n",
    "> Trace an Inference from Raw Data to Prediction Through Llama 3 text-generation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b67daf-9441-458b-8a0c-a1659aa7eb57",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In [the last Transformer Teardown](https://stickshift.github.io/2024/09/04/transformer-teardown.html), we dissected a BERT-based text classification pipeline, tracing a single inference through the entire stack from raw data to final prediction. While no longer state-of-the-art, we started with BERT to build a strong foundation in Transformer fundamentals. In this post, we'll apply what we learned to unpack [the latest Llama 3 models](https://llama.meta.com/) released by Meta last month. \n",
    "\n",
    "Following the same teardown process as last time, we'll start by creating an off-the-shelf Llama 3 `text-generation` pipeline, apply the pipeline to generate an answer to an arbitrary question, and then manually regenerate the same answer step by step, using clear, minimal Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61c904-900e-4ad7-8fd2-1cdc813b896f",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78df83c1-4707-4ebc-9111-30a0b3189173",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas import Series\n",
    "from pytest import approx\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu, softmax\n",
    "import transformers\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, repeat_kv\n",
    "\n",
    "from stickshift import default_arg, take\n",
    "from stickshift.models import llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b2896c-52a5-45b2-a0e4-e80ff9c3130e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Disable stderr\n",
    "# sys.stderr = open(os.devnull, 'w')\n",
    "\n",
    "# Configure gpu\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcd4b9-8766-41b5-9a50-7fe746b03a43",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Text Generation with Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a7710-fb82-445a-844b-9a42509966b8",
   "metadata": {},
   "source": [
    "We start by using Hugging Face's `pipeline` API to create an end-to-end text generation pipeline using Llama 3 8B model, and then use the model to answer the question \"What is the capital of Massachusetts?\" Our goal throughout the rest of the post will be to recreate the steps from raw text to the answer \"Boston\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48bc03c-7c0b-4d88-bd87-9f142a49a8ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95c98880f1b48e799221d1b627757e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create off-the-shelf Llama 3 text generation pipeline\n",
    "transformer = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07f11c07-2b2f-41c1-93b1-92572074c6f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is the capital of Massachusetts? Boston\\nWhat is the population of Massachusetts? 6.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer question\n",
    "transformer(\"What is the capital of Massachusetts?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d1c83-bda7-451d-9cc1-8f01b0846692",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "source": [
    "# Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62fa34b9-229f-466f-a749-b64b179084c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "# Load model config and pre-trained parameters\n",
    "config = llama.config(transformer.model)\n",
    "parameters = transformer.model.state_dict()\n",
    "llama_model = transformer.model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcce5d94-c218-44c3-933a-6885e213fd8b",
   "metadata": {
    "collapsed": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.embed_tokens.weight',\n",
       " 'model.layers.0.self_attn.q_proj.weight',\n",
       " 'model.layers.0.self_attn.k_proj.weight',\n",
       " 'model.layers.0.self_attn.v_proj.weight',\n",
       " 'model.layers.0.self_attn.o_proj.weight',\n",
       " 'model.layers.0.mlp.gate_proj.weight',\n",
       " 'model.layers.0.mlp.up_proj.weight',\n",
       " 'model.layers.0.mlp.down_proj.weight',\n",
       " 'model.layers.0.input_layernorm.weight',\n",
       " 'model.layers.0.post_attention_layernorm.weight',\n",
       " 'model.layers.1.self_attn.q_proj.weight',\n",
       " 'model.layers.1.self_attn.k_proj.weight',\n",
       " 'model.layers.1.self_attn.v_proj.weight',\n",
       " 'model.layers.1.self_attn.o_proj.weight',\n",
       " 'model.layers.1.mlp.gate_proj.weight',\n",
       " 'model.layers.1.mlp.up_proj.weight',\n",
       " 'model.layers.1.mlp.down_proj.weight',\n",
       " 'model.layers.1.input_layernorm.weight',\n",
       " 'model.layers.1.post_attention_layernorm.weight',\n",
       " 'model.layers.2.self_attn.q_proj.weight',\n",
       " 'model.layers.2.self_attn.k_proj.weight',\n",
       " 'model.layers.2.self_attn.v_proj.weight',\n",
       " 'model.layers.2.self_attn.o_proj.weight',\n",
       " 'model.layers.2.mlp.gate_proj.weight',\n",
       " 'model.layers.2.mlp.up_proj.weight',\n",
       " 'model.layers.2.mlp.down_proj.weight',\n",
       " 'model.layers.2.input_layernorm.weight',\n",
       " 'model.layers.2.post_attention_layernorm.weight',\n",
       " 'model.layers.3.self_attn.q_proj.weight',\n",
       " 'model.layers.3.self_attn.k_proj.weight',\n",
       " 'model.layers.3.self_attn.v_proj.weight',\n",
       " 'model.layers.3.self_attn.o_proj.weight',\n",
       " 'model.layers.3.mlp.gate_proj.weight',\n",
       " 'model.layers.3.mlp.up_proj.weight',\n",
       " 'model.layers.3.mlp.down_proj.weight',\n",
       " 'model.layers.3.input_layernorm.weight',\n",
       " 'model.layers.3.post_attention_layernorm.weight',\n",
       " 'model.layers.4.self_attn.q_proj.weight',\n",
       " 'model.layers.4.self_attn.k_proj.weight',\n",
       " 'model.layers.4.self_attn.v_proj.weight',\n",
       " 'model.layers.4.self_attn.o_proj.weight',\n",
       " 'model.layers.4.mlp.gate_proj.weight',\n",
       " 'model.layers.4.mlp.up_proj.weight',\n",
       " 'model.layers.4.mlp.down_proj.weight',\n",
       " 'model.layers.4.input_layernorm.weight',\n",
       " 'model.layers.4.post_attention_layernorm.weight',\n",
       " 'model.layers.5.self_attn.q_proj.weight',\n",
       " 'model.layers.5.self_attn.k_proj.weight',\n",
       " 'model.layers.5.self_attn.v_proj.weight',\n",
       " 'model.layers.5.self_attn.o_proj.weight',\n",
       " 'model.layers.5.mlp.gate_proj.weight',\n",
       " 'model.layers.5.mlp.up_proj.weight',\n",
       " 'model.layers.5.mlp.down_proj.weight',\n",
       " 'model.layers.5.input_layernorm.weight',\n",
       " 'model.layers.5.post_attention_layernorm.weight',\n",
       " 'model.layers.6.self_attn.q_proj.weight',\n",
       " 'model.layers.6.self_attn.k_proj.weight',\n",
       " 'model.layers.6.self_attn.v_proj.weight',\n",
       " 'model.layers.6.self_attn.o_proj.weight',\n",
       " 'model.layers.6.mlp.gate_proj.weight',\n",
       " 'model.layers.6.mlp.up_proj.weight',\n",
       " 'model.layers.6.mlp.down_proj.weight',\n",
       " 'model.layers.6.input_layernorm.weight',\n",
       " 'model.layers.6.post_attention_layernorm.weight',\n",
       " 'model.layers.7.self_attn.q_proj.weight',\n",
       " 'model.layers.7.self_attn.k_proj.weight',\n",
       " 'model.layers.7.self_attn.v_proj.weight',\n",
       " 'model.layers.7.self_attn.o_proj.weight',\n",
       " 'model.layers.7.mlp.gate_proj.weight',\n",
       " 'model.layers.7.mlp.up_proj.weight',\n",
       " 'model.layers.7.mlp.down_proj.weight',\n",
       " 'model.layers.7.input_layernorm.weight',\n",
       " 'model.layers.7.post_attention_layernorm.weight',\n",
       " 'model.layers.8.self_attn.q_proj.weight',\n",
       " 'model.layers.8.self_attn.k_proj.weight',\n",
       " 'model.layers.8.self_attn.v_proj.weight',\n",
       " 'model.layers.8.self_attn.o_proj.weight',\n",
       " 'model.layers.8.mlp.gate_proj.weight',\n",
       " 'model.layers.8.mlp.up_proj.weight',\n",
       " 'model.layers.8.mlp.down_proj.weight',\n",
       " 'model.layers.8.input_layernorm.weight',\n",
       " 'model.layers.8.post_attention_layernorm.weight',\n",
       " 'model.layers.9.self_attn.q_proj.weight',\n",
       " 'model.layers.9.self_attn.k_proj.weight',\n",
       " 'model.layers.9.self_attn.v_proj.weight',\n",
       " 'model.layers.9.self_attn.o_proj.weight',\n",
       " 'model.layers.9.mlp.gate_proj.weight',\n",
       " 'model.layers.9.mlp.up_proj.weight',\n",
       " 'model.layers.9.mlp.down_proj.weight',\n",
       " 'model.layers.9.input_layernorm.weight',\n",
       " 'model.layers.9.post_attention_layernorm.weight',\n",
       " 'model.layers.10.self_attn.q_proj.weight',\n",
       " 'model.layers.10.self_attn.k_proj.weight',\n",
       " 'model.layers.10.self_attn.v_proj.weight',\n",
       " 'model.layers.10.self_attn.o_proj.weight',\n",
       " 'model.layers.10.mlp.gate_proj.weight',\n",
       " 'model.layers.10.mlp.up_proj.weight',\n",
       " 'model.layers.10.mlp.down_proj.weight',\n",
       " 'model.layers.10.input_layernorm.weight',\n",
       " 'model.layers.10.post_attention_layernorm.weight',\n",
       " 'model.layers.11.self_attn.q_proj.weight',\n",
       " 'model.layers.11.self_attn.k_proj.weight',\n",
       " 'model.layers.11.self_attn.v_proj.weight',\n",
       " 'model.layers.11.self_attn.o_proj.weight',\n",
       " 'model.layers.11.mlp.gate_proj.weight',\n",
       " 'model.layers.11.mlp.up_proj.weight',\n",
       " 'model.layers.11.mlp.down_proj.weight',\n",
       " 'model.layers.11.input_layernorm.weight',\n",
       " 'model.layers.11.post_attention_layernorm.weight',\n",
       " 'model.layers.12.self_attn.q_proj.weight',\n",
       " 'model.layers.12.self_attn.k_proj.weight',\n",
       " 'model.layers.12.self_attn.v_proj.weight',\n",
       " 'model.layers.12.self_attn.o_proj.weight',\n",
       " 'model.layers.12.mlp.gate_proj.weight',\n",
       " 'model.layers.12.mlp.up_proj.weight',\n",
       " 'model.layers.12.mlp.down_proj.weight',\n",
       " 'model.layers.12.input_layernorm.weight',\n",
       " 'model.layers.12.post_attention_layernorm.weight',\n",
       " 'model.layers.13.self_attn.q_proj.weight',\n",
       " 'model.layers.13.self_attn.k_proj.weight',\n",
       " 'model.layers.13.self_attn.v_proj.weight',\n",
       " 'model.layers.13.self_attn.o_proj.weight',\n",
       " 'model.layers.13.mlp.gate_proj.weight',\n",
       " 'model.layers.13.mlp.up_proj.weight',\n",
       " 'model.layers.13.mlp.down_proj.weight',\n",
       " 'model.layers.13.input_layernorm.weight',\n",
       " 'model.layers.13.post_attention_layernorm.weight',\n",
       " 'model.layers.14.self_attn.q_proj.weight',\n",
       " 'model.layers.14.self_attn.k_proj.weight',\n",
       " 'model.layers.14.self_attn.v_proj.weight',\n",
       " 'model.layers.14.self_attn.o_proj.weight',\n",
       " 'model.layers.14.mlp.gate_proj.weight',\n",
       " 'model.layers.14.mlp.up_proj.weight',\n",
       " 'model.layers.14.mlp.down_proj.weight',\n",
       " 'model.layers.14.input_layernorm.weight',\n",
       " 'model.layers.14.post_attention_layernorm.weight',\n",
       " 'model.layers.15.self_attn.q_proj.weight',\n",
       " 'model.layers.15.self_attn.k_proj.weight',\n",
       " 'model.layers.15.self_attn.v_proj.weight',\n",
       " 'model.layers.15.self_attn.o_proj.weight',\n",
       " 'model.layers.15.mlp.gate_proj.weight',\n",
       " 'model.layers.15.mlp.up_proj.weight',\n",
       " 'model.layers.15.mlp.down_proj.weight',\n",
       " 'model.layers.15.input_layernorm.weight',\n",
       " 'model.layers.15.post_attention_layernorm.weight',\n",
       " 'model.layers.16.self_attn.q_proj.weight',\n",
       " 'model.layers.16.self_attn.k_proj.weight',\n",
       " 'model.layers.16.self_attn.v_proj.weight',\n",
       " 'model.layers.16.self_attn.o_proj.weight',\n",
       " 'model.layers.16.mlp.gate_proj.weight',\n",
       " 'model.layers.16.mlp.up_proj.weight',\n",
       " 'model.layers.16.mlp.down_proj.weight',\n",
       " 'model.layers.16.input_layernorm.weight',\n",
       " 'model.layers.16.post_attention_layernorm.weight',\n",
       " 'model.layers.17.self_attn.q_proj.weight',\n",
       " 'model.layers.17.self_attn.k_proj.weight',\n",
       " 'model.layers.17.self_attn.v_proj.weight',\n",
       " 'model.layers.17.self_attn.o_proj.weight',\n",
       " 'model.layers.17.mlp.gate_proj.weight',\n",
       " 'model.layers.17.mlp.up_proj.weight',\n",
       " 'model.layers.17.mlp.down_proj.weight',\n",
       " 'model.layers.17.input_layernorm.weight',\n",
       " 'model.layers.17.post_attention_layernorm.weight',\n",
       " 'model.layers.18.self_attn.q_proj.weight',\n",
       " 'model.layers.18.self_attn.k_proj.weight',\n",
       " 'model.layers.18.self_attn.v_proj.weight',\n",
       " 'model.layers.18.self_attn.o_proj.weight',\n",
       " 'model.layers.18.mlp.gate_proj.weight',\n",
       " 'model.layers.18.mlp.up_proj.weight',\n",
       " 'model.layers.18.mlp.down_proj.weight',\n",
       " 'model.layers.18.input_layernorm.weight',\n",
       " 'model.layers.18.post_attention_layernorm.weight',\n",
       " 'model.layers.19.self_attn.q_proj.weight',\n",
       " 'model.layers.19.self_attn.k_proj.weight',\n",
       " 'model.layers.19.self_attn.v_proj.weight',\n",
       " 'model.layers.19.self_attn.o_proj.weight',\n",
       " 'model.layers.19.mlp.gate_proj.weight',\n",
       " 'model.layers.19.mlp.up_proj.weight',\n",
       " 'model.layers.19.mlp.down_proj.weight',\n",
       " 'model.layers.19.input_layernorm.weight',\n",
       " 'model.layers.19.post_attention_layernorm.weight',\n",
       " 'model.layers.20.self_attn.q_proj.weight',\n",
       " 'model.layers.20.self_attn.k_proj.weight',\n",
       " 'model.layers.20.self_attn.v_proj.weight',\n",
       " 'model.layers.20.self_attn.o_proj.weight',\n",
       " 'model.layers.20.mlp.gate_proj.weight',\n",
       " 'model.layers.20.mlp.up_proj.weight',\n",
       " 'model.layers.20.mlp.down_proj.weight',\n",
       " 'model.layers.20.input_layernorm.weight',\n",
       " 'model.layers.20.post_attention_layernorm.weight',\n",
       " 'model.layers.21.self_attn.q_proj.weight',\n",
       " 'model.layers.21.self_attn.k_proj.weight',\n",
       " 'model.layers.21.self_attn.v_proj.weight',\n",
       " 'model.layers.21.self_attn.o_proj.weight',\n",
       " 'model.layers.21.mlp.gate_proj.weight',\n",
       " 'model.layers.21.mlp.up_proj.weight',\n",
       " 'model.layers.21.mlp.down_proj.weight',\n",
       " 'model.layers.21.input_layernorm.weight',\n",
       " 'model.layers.21.post_attention_layernorm.weight',\n",
       " 'model.layers.22.self_attn.q_proj.weight',\n",
       " 'model.layers.22.self_attn.k_proj.weight',\n",
       " 'model.layers.22.self_attn.v_proj.weight',\n",
       " 'model.layers.22.self_attn.o_proj.weight',\n",
       " 'model.layers.22.mlp.gate_proj.weight',\n",
       " 'model.layers.22.mlp.up_proj.weight',\n",
       " 'model.layers.22.mlp.down_proj.weight',\n",
       " 'model.layers.22.input_layernorm.weight',\n",
       " 'model.layers.22.post_attention_layernorm.weight',\n",
       " 'model.layers.23.self_attn.q_proj.weight',\n",
       " 'model.layers.23.self_attn.k_proj.weight',\n",
       " 'model.layers.23.self_attn.v_proj.weight',\n",
       " 'model.layers.23.self_attn.o_proj.weight',\n",
       " 'model.layers.23.mlp.gate_proj.weight',\n",
       " 'model.layers.23.mlp.up_proj.weight',\n",
       " 'model.layers.23.mlp.down_proj.weight',\n",
       " 'model.layers.23.input_layernorm.weight',\n",
       " 'model.layers.23.post_attention_layernorm.weight',\n",
       " 'model.layers.24.self_attn.q_proj.weight',\n",
       " 'model.layers.24.self_attn.k_proj.weight',\n",
       " 'model.layers.24.self_attn.v_proj.weight',\n",
       " 'model.layers.24.self_attn.o_proj.weight',\n",
       " 'model.layers.24.mlp.gate_proj.weight',\n",
       " 'model.layers.24.mlp.up_proj.weight',\n",
       " 'model.layers.24.mlp.down_proj.weight',\n",
       " 'model.layers.24.input_layernorm.weight',\n",
       " 'model.layers.24.post_attention_layernorm.weight',\n",
       " 'model.layers.25.self_attn.q_proj.weight',\n",
       " 'model.layers.25.self_attn.k_proj.weight',\n",
       " 'model.layers.25.self_attn.v_proj.weight',\n",
       " 'model.layers.25.self_attn.o_proj.weight',\n",
       " 'model.layers.25.mlp.gate_proj.weight',\n",
       " 'model.layers.25.mlp.up_proj.weight',\n",
       " 'model.layers.25.mlp.down_proj.weight',\n",
       " 'model.layers.25.input_layernorm.weight',\n",
       " 'model.layers.25.post_attention_layernorm.weight',\n",
       " 'model.layers.26.self_attn.q_proj.weight',\n",
       " 'model.layers.26.self_attn.k_proj.weight',\n",
       " 'model.layers.26.self_attn.v_proj.weight',\n",
       " 'model.layers.26.self_attn.o_proj.weight',\n",
       " 'model.layers.26.mlp.gate_proj.weight',\n",
       " 'model.layers.26.mlp.up_proj.weight',\n",
       " 'model.layers.26.mlp.down_proj.weight',\n",
       " 'model.layers.26.input_layernorm.weight',\n",
       " 'model.layers.26.post_attention_layernorm.weight',\n",
       " 'model.layers.27.self_attn.q_proj.weight',\n",
       " 'model.layers.27.self_attn.k_proj.weight',\n",
       " 'model.layers.27.self_attn.v_proj.weight',\n",
       " 'model.layers.27.self_attn.o_proj.weight',\n",
       " 'model.layers.27.mlp.gate_proj.weight',\n",
       " 'model.layers.27.mlp.up_proj.weight',\n",
       " 'model.layers.27.mlp.down_proj.weight',\n",
       " 'model.layers.27.input_layernorm.weight',\n",
       " 'model.layers.27.post_attention_layernorm.weight',\n",
       " 'model.layers.28.self_attn.q_proj.weight',\n",
       " 'model.layers.28.self_attn.k_proj.weight',\n",
       " 'model.layers.28.self_attn.v_proj.weight',\n",
       " 'model.layers.28.self_attn.o_proj.weight',\n",
       " 'model.layers.28.mlp.gate_proj.weight',\n",
       " 'model.layers.28.mlp.up_proj.weight',\n",
       " 'model.layers.28.mlp.down_proj.weight',\n",
       " 'model.layers.28.input_layernorm.weight',\n",
       " 'model.layers.28.post_attention_layernorm.weight',\n",
       " 'model.layers.29.self_attn.q_proj.weight',\n",
       " 'model.layers.29.self_attn.k_proj.weight',\n",
       " 'model.layers.29.self_attn.v_proj.weight',\n",
       " 'model.layers.29.self_attn.o_proj.weight',\n",
       " 'model.layers.29.mlp.gate_proj.weight',\n",
       " 'model.layers.29.mlp.up_proj.weight',\n",
       " 'model.layers.29.mlp.down_proj.weight',\n",
       " 'model.layers.29.input_layernorm.weight',\n",
       " 'model.layers.29.post_attention_layernorm.weight',\n",
       " 'model.layers.30.self_attn.q_proj.weight',\n",
       " 'model.layers.30.self_attn.k_proj.weight',\n",
       " 'model.layers.30.self_attn.v_proj.weight',\n",
       " 'model.layers.30.self_attn.o_proj.weight',\n",
       " 'model.layers.30.mlp.gate_proj.weight',\n",
       " 'model.layers.30.mlp.up_proj.weight',\n",
       " 'model.layers.30.mlp.down_proj.weight',\n",
       " 'model.layers.30.input_layernorm.weight',\n",
       " 'model.layers.30.post_attention_layernorm.weight',\n",
       " 'model.layers.31.self_attn.q_proj.weight',\n",
       " 'model.layers.31.self_attn.k_proj.weight',\n",
       " 'model.layers.31.self_attn.v_proj.weight',\n",
       " 'model.layers.31.self_attn.o_proj.weight',\n",
       " 'model.layers.31.mlp.gate_proj.weight',\n",
       " 'model.layers.31.mlp.up_proj.weight',\n",
       " 'model.layers.31.mlp.down_proj.weight',\n",
       " 'model.layers.31.input_layernorm.weight',\n",
       " 'model.layers.31.post_attention_layernorm.weight',\n",
       " 'model.norm.weight',\n",
       " 'lm_head.weight']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4c60aa0-997a-49e0-9e1b-12efdec42493",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "def load_state(*args, layer=None):\n",
    "    # Defaults\n",
    "    layer = default_arg(layer, lambda: 0)\n",
    "\n",
    "    for module, key in take(2, args):\n",
    "        match key:\n",
    "            case \"value_embeddings\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[\"model.embed_tokens.weight\"],\n",
    "                })\n",
    "            case \"normalize_inputs\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.input_layernorm.weight\"],\n",
    "                })\n",
    "            case \"queries\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.self_attn.q_proj.weight\"],\n",
    "                })\n",
    "            case \"keys\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.self_attn.k_proj.weight\"],\n",
    "                })\n",
    "            case \"values\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.self_attn.v_proj.weight\"],\n",
    "                })                \n",
    "            case \"attention_outputs\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.self_attn.o_proj.weight\"],\n",
    "                })                \n",
    "            case \"normalize_attention\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.post_attention_layernorm.weight\"],\n",
    "                })\n",
    "            case \"gate\":\n",
    "                module.load_state_dict({\n",
    "                    \"0.weight\": parameters[f\"model.layers.{layer}.mlp.gate_proj.weight\"],\n",
    "                })\n",
    "            case \"up\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.mlp.up_proj.weight\"],\n",
    "                })\n",
    "            case \"down\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.mlp.down_proj.weight\"],\n",
    "                })\n",
    "            case \"normalize_context\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[\"model.norm.weight\"],\n",
    "                })\n",
    "            case \"classifier\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[\"lm_head.weight\"],\n",
    "                })\n",
    "            case _:\n",
    "                raise ValueError(f\"Unexpected key {key}\")\n",
    "\n",
    "\n",
    "def load_pretrained_state(layer):    \n",
    "    # Load pre-trained state\n",
    "    load_state(\n",
    "        normalize_inputs, \"normalize_inputs\", \n",
    "        queries, \"queries\", \n",
    "        keys, \"keys\", \n",
    "        values, \"values\", \n",
    "        attention_outputs, \"attention_outputs\", \n",
    "        normalize_attention, \"normalize_attention\",\n",
    "        gate, \"gate\",\n",
    "        up, \"up\",\n",
    "        down, \"down\",\n",
    "        layer=layer,\n",
    "    )                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9daed854-f7e8-4206-9be6-c590717f76e9",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "def compare_embeddings(t, llama_t):\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Move both tensors to cpu\n",
    "        t = t.to(\"cpu\")\n",
    "        llama_t = llama_t.to(\"cpu\")\n",
    "    \n",
    "        # Squeeze llama\n",
    "        llama_t = llama_t.squeeze()\n",
    "        assert t.shape == llama_t.shape\n",
    "\n",
    "        # Reshape both to be 1 long list of embeddings\n",
    "        t = t.reshape(-1, t.shape[-1])\n",
    "        llama_t = llama_t.reshape(-1, llama_t.shape[-1])\n",
    "        assert t.shape == llama_t.shape\n",
    "\n",
    "        # Compare each embedding\n",
    "        for i in range(t.shape[0]):\n",
    "            e1 = t[i]\n",
    "            e2 = llama_t[i]\n",
    "            score = torch.dot(e1, e2) / torch.norm(e2)**2\n",
    "            error = 1.0 - score\n",
    "            errors.append(error.abs().item())\n",
    "\n",
    "    return Series(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094ecaa0-789e-4a15-a06d-86f345ddca66",
   "metadata": {},
   "source": [
    "# Transformer Pipeline\n",
    "\n",
    "Llama 3 follows the same multi-stage Transformer pipeline that we saw with BERT. We'll walk through each of the stages, illustrating each of the steps in Llama 3. Using BERT as a baseline, we will focus the discussion on the key changes in Llama 3. For more detail on the fundamentals, please see [the previous Transformer Teardown](https://stickshift.github.io/2024/09/04/transformer-teardown.html).\n",
    "\n",
    "<img src=\"transformer-pipeline.svg\" class=\"stickshift-figure\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b190253c-a1e9-4072-a9c5-48c0a56b8a7e",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d116c6-5d2d-497d-9ba4-fcb36e0d598b",
   "metadata": {},
   "source": [
    "The Tokenize stage is responsible for breaking raw data into a sequence of \"tokens\". The following cells use an algorithm known as Byte Pair Encoding (BPE) (REFERENCE) to split the sentence \"What is the capital of Massachusetts?\" into token ids `[128000, 3923, 374, 279, 6864, 315, 22108, 30]`.\n",
    "\n",
    "Since our primary interest is in the Transformer layers that come later, we'll use Hugging Face's off-the-shelf tokenizer implementation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29ac3463-eab4-4118-9c8b-60735046eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokenizer from transformer\n",
    "tokenizer = transformer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cf2141e-30c8-481e-9167-4c8108d5f8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,   3923,    374,    279,   6864,    315,  22108,     30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize sentence\n",
    "batch = tokenizer(\"What is the capital of Massachusetts?\", return_tensors=\"pt\")\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f0786-5aac-490c-84f3-b9d515119f55",
   "metadata": {},
   "source": [
    "We can also use the tokenizer to decode the token ids back into raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ec064fd-2a8e-4e40-812d-93542f798dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>',\n",
       " 'What',\n",
       " ' is',\n",
       " ' the',\n",
       " ' capital',\n",
       " ' of',\n",
       " ' Massachusetts',\n",
       " '?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(input_id) for input_id in batch.input_ids[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61b0af3-dd2d-4143-bd76-1bb8ea2e6c02",
   "metadata": {},
   "source": [
    "The following table highlights the differences in tokenization hyperparameters between BERT and Llama 3.\n",
    "\n",
    "|                | BERT       | Llama 3 |\n",
    "| -------------: | ---------- | ------- |\n",
    "| **Algorithm**  | Word Piece | BPE     |\n",
    "| **Vocab Size** | 30k        | 128k    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb23e07-f645-470a-a3ea-cf0a518cd40f",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e4fe97-8295-4976-a89d-1e82ef989261",
   "metadata": {},
   "source": [
    "The Embeddings stage of the Transformer pipeline converts each of the token ids into an \"embedding\". This is the first big difference between BERT and Llama 3. While BERT used learned embeddings to encode the tokens' absolute positions, Llama 3 uses Rotary Position Encoding (RoPE) (REFERENCE) to encode the tokens' *relative* positions. RoPE moves responsibility for the position encoding from the Embeddings stage to the attention calculation in the Context stage. For now, the Embeddings stage only needs to worry about the token values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64fcd9c7-a6ec-46e5-b831-eae812eaff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize value embeddings lookup table\n",
    "value_embeddings = nn.Embedding(\n",
    "    num_embeddings=config.vocab_size, \n",
    "    embedding_dim=config.d_model,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(value_embeddings, \"value_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79635ac3-5757-49ab-a456-30cb6ea587d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>',\n",
       " 'What',\n",
       " ' is',\n",
       " ' the',\n",
       " ' capital',\n",
       " ' of',\n",
       " ' Massachusetts',\n",
       " '?']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate token values\n",
    "token_values = torch.squeeze(batch.input_ids)\n",
    "\n",
    "[tokenizer.decode(token_id) for token_id in token_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5c3bdec-6b1f-4280-bd08-b07dfe1694a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Record sequence length\n",
    "n = len(token_values)\n",
    "\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "446323d6-a4bd-45bc-ab6a-13ad413d8b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map token values to embeddings\n",
    "v = value_embeddings(token_values)\n",
    "\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e5ef02e-3b3f-45e9-b2ef-1689f4d4a138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
       "          6.3419e-05,  1.1902e-03],\n",
       "        [ 2.0752e-02, -1.2894e-03,  2.8229e-03,  ...,  2.1973e-02,\n",
       "          3.1128e-03,  1.0681e-02],\n",
       "        [-2.6093e-03,  7.7057e-04,  2.6131e-04,  ...,  1.1902e-02,\n",
       "          4.6387e-03,  9.1553e-03],\n",
       "        ...,\n",
       "        [ 1.2817e-03,  9.1171e-04,  2.0905e-03,  ...,  1.6251e-03,\n",
       "          4.0894e-03, -4.0283e-03],\n",
       "        [ 1.2146e-02,  1.1597e-02,  1.7822e-02,  ...,  1.9684e-03,\n",
       "         -1.4771e-02, -2.5940e-03],\n",
       "        [-4.8523e-03, -1.8005e-03,  7.2937e-03,  ...,  2.3956e-03,\n",
       "         -1.3657e-03, -5.4932e-03]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show sample of value embeddings\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36462c14-5a9f-4884-b918-ad4d1c45ffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save value embeddings as our \"input embeddings\"\n",
    "hidden_states = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfcdbf5-9f5e-4f06-bb4d-ab18c489a93c",
   "metadata": {},
   "source": [
    "# Context Layers\n",
    "\n",
    "The Context Layers in a Transformer are responsible for infusing each token embedding with contextual signals drawn from the rest of the sequence. The mechanism works by passing the token embeddings through multiple layers of attention and feedforward blocks. The attention blocks focus on relationships between tokens, augmenting each embedding with contextual information drawn from the surrounding embeddings. The feedforward blocks capitalize on the extra context, transforming each augmented embedding using a fully connected neural network. This pattern of attention and transformation is repeated over and over again, gradually converting representations of individual words into representations of abstract semantic concepts over a series of small increments.\n",
    "\n",
    "<img src=\"transformer-layers.svg\" class=\"stickshift-figure\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c7ee9-e46e-48b8-90a8-560051623739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c08ae315-c1ad-46b1-a8b5-b8d7c70e102b",
   "metadata": {},
   "source": [
    "## Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9df1cf-0f3e-4ca6-be31-70c92eb6219b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68dac36-c138-4c92-b45a-1c50bd6dfad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f75f8bea-fd82-4d79-8882-cbd92c62ab00",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8de23-b55f-45b5-8e03-c07e56357fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e9f2c-3251-4558-9eed-c3e63c2774da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37f49475-5c54-46dc-8d63-47b9cfd21135",
   "metadata": {},
   "source": [
    "## Normalize Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65604745-f525-44ed-aa4b-70805c01586a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73343f30-be08-4445-96d7-ce4515830c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3875c79f-c6d0-4bb7-93e5-441afd3861c2",
   "metadata": {},
   "source": [
    "## FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed6cd5-7361-4027-a20d-8a51bd5a53a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0344c0-eb46-4621-a4ee-e25b4817b73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37377cda-1edf-45c9-9f54-bd4794788ce7",
   "metadata": {},
   "source": [
    "## Normalize FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2158321-38f1-435e-ad09-b0f226b68525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2d599-5aa9-47f8-a7c8-485158a4d9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f6c380c-c0dc-4853-bea2-1f06c3ee89b5",
   "metadata": {},
   "source": [
    "## Stacking the Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c17071-a834-42dd-a04d-fe55eef6d5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b32de4-ece9-493d-aba5-6c4f9a2ad772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97fe4d68-92fa-4b22-b784-3db7934a9e2e",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2721b4-4952-416e-9776-ce25a114e175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dc4bc9-c57a-4bbf-a2db-29a9d3a2602b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2080ffe6-cfa4-4c60-b80d-dd7ccc4443da",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c401b-3075-436c-a172-abd45c11c414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c327d-f8f1-40cf-8499-b2fb84e6c23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63545b-aee3-488a-a7c2-1b4500cdb1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b37ff29-b647-4280-b16f-45cc5eccea30",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ac821-cc3d-4816-84f3-cf947e2b88df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f62c31-07b4-48ae-a472-5abac49436f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "stickshift": {
   "description": "Trace an Inference from Raw Data to Prediction Through Llama 3 text-generation Pipeline",
   "title": "Transformer Teardown: Llama 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
