{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06347ff4-85c9-49dc-a561-cfea5aaf4db3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "source": [
    "# Transformer Teardown: Llama 3.1\n",
    "\n",
    "> Trace an Inference Through Each Layer in a SOTA Transformer Using Llama 3.1 Open Source Foundation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da22c3d-c844-4280-9b4f-c840bb2d865d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In [the last Transformer Teardown](https://stickshift.github.io/2024/09/04/transformer-teardown.html), we dissected a DistilBERT text classification pipeline, tracing a single inference through the entire Transformer stack from raw data to final prediction. We learned about the main stages of a Transformer pipeline as well as fundamental Transformer concepts such as token embeddings and Multi-Head Self Attention. Exploring BERT-based models is a fantastic way to see the core Transformer concepts in action. But BERT was released 6 years ago! ChatGPT wouldn't even exist for another 4 years. It's safe to say a lot has changed since then.\n",
    "\n",
    "In this post, we'll fast forward to present day. We'll use the same teardown process to unpack the latest [Llama 3.1](https://llama.meta.com/) open source foundation models released by Meta over the summer. We'll break the model down and walk through each step one cell at a time, giving you a close-up view of a modern LLM's inner workings. By the time we're done, you'll leave with a much stronger understanding of the core mechanisms driving the Generative AI revolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a70ef9-db34-41bf-8b75-1868ed3179f3",
   "metadata": {},
   "source": [
    "# Llama Foundation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0a5de8-94f6-40a4-bdbb-bfa3a50a9876",
   "metadata": {},
   "source": [
    "[Llama](https://llama.meta.com/) is a family of general purpose, state-of-the-art open source foundation models from Meta. According to the 3.1 technical report, the latest models can \"answer questions in at least 8 languages, write high quality code, solve complex reasoning problems, and use tools in a zero-shot way.\" (Dubey et al. 2024) \n",
    "\n",
    "Llama 3.1 includes 8B, 70B, and 405B sizes. While Meta recommends a cluster with at least 8 GPUs to run the 70B and 405B sizes, the 8B model is small enough to run on a single GPU with 20GB of memory. For more technical stats, see [the official Llama 3.1 model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) in GitHub.\n",
    "\n",
    "Over the course of this post, we'll implement a complete text generation pipeline using only the research literature, pre-trained weights from the `Meta-Llama3.1-8B-Instruct` checkpoint, and Meta's reference implementation as a guide. In the next section, we'll review the stages of an end-to-end, text generation pipeline. In the sections that follow, we'll walk through a detailed teardown of each stage—tracing an inference from raw data to the first output token. In the last section, we'll put all the pieces together into a custom chatbot capable of generating long form content.\n",
    "\n",
    "Let the teardown begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bdfe4-94f0-4e7d-a90f-defe1625c130",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d850f817-3a96-44a3-b551-2814dab297a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sys import stdout\n",
    "from textwrap import dedent\n",
    "import warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas import Series\n",
    "from pydantic import BaseModel, validate_call\n",
    "from pytest import approx\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import relu, silu, softmax\n",
    "\n",
    "from llama_models.llama3.api.tokenizer import Tokenizer\n",
    "from llama_models.llama3.reference_impl.model import RMSNorm\n",
    "\n",
    "import stickshift as ss\n",
    "from stickshift import default_arg, take\n",
    "from stickshift.models import llama\n",
    "from stickshift.torch import device as torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c174d1af-fd9e-4d0d-9f08-e59ca4af5f35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure gpu\n",
    "device = torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563bf2a8-5154-49d2-a5bc-83076d4d484f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "figure > img {\n",
       "    display:block;\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "figcaption {\n",
       "    text-align: center;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "figure > img {\n",
    "    display:block;\n",
    "    margin-left: auto !important;\n",
    "    margin-right: auto !important;\n",
    "}\n",
    "figcaption {\n",
    "    text-align: center;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c8599f-dd26-49fc-b185-64611ca709b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ea8fc2-7b83-4bf0-bdce-1f8254f5b789",
   "metadata": {},
   "source": [
    "# Text Generation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7355b475-d266-423d-9fe4-214bfcc9337d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In [the last teardown](https://stickshift.github.io/2024/09/04/transformer-teardown.html), we looked at a text classification Transformer. This time we're going to dissect a *text generation* Transformer. Instead of simply applying a label to the input text, the Head stage will be responsible for *generating* new content. Don't worry—it's not as complicated as it sounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d845533-c472-464c-9c00-275616d30693",
   "metadata": {},
   "source": [
    "Shown below in Figure 1, we can see the stages of a text generation pipeline are the same as the text classification pipeline we saw last time. The Tokenize stage splits raw text into tokens. The Embeddings stage converts individual tokens into embedding vectors. The Context Layers stage augments the input embeddings with contextual signals drawn from the surrounding tokens. Finally, the Head stage converts the contextualized embeddings into task-specific predictions.\n",
    "\n",
    "<figure>\n",
    "<img src=\"transformer-pipeline.svg\" width=\"800\">\n",
    "<figcaption>Figure 1: Text Generation Pipeline</figcaption>\n",
    "</figure>\n",
    "\n",
    "The key differences between text generation and text classification are the *task-specific* predictions in the Head stage. While text classification Transformers predict a label for the raw text, *text generation Transformers predict the next token.*\n",
    "\n",
    "But one token is just the beginning. The magical powers driving the Generative AI explosion come from simply running the predictions in a loop. The next token predicted in each iteration is appended to the end of the input sequence, and the process repeats. Over and over again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4d296-d70f-4597-96c0-7e5dbb448e5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Model Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a509bf-9a2b-4d6a-b4fd-322ed4af31eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We'll start by loading the configuration and pre-trained weights for the `Meta-Llama3.1-8B-Instruct` checkpoint. The weights for all Llama checkpoints can be downloaded directly from [Meta](https://llama.meta.com/) and [Hugging Face](https://huggingface.co/meta-llama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55aa2322-410c-4720-9e45-2c9eb68dd8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checkpoint_path': PosixPath('/Users/andrewyoung/.llama/checkpoints/Meta-Llama3.1-8B-Instruct'), 'vocab_size': 128256, 'd_model': 4096, 'd_head': 128, 'd_ffn': 14336, 'n_layers': 32, 'n_heads': 32, 'n_kv_heads': 8, 'n_kv_groups': 4, 'rms_norm_eps': 1e-05, 'rope_theta': 500000.0, 'max_seq_len': 8192, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9, 'max_output_tokens': 500}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model config\n",
    "config = llama.config(\"Meta-Llama3.1-8B-Instruct\")\n",
    "\n",
    "# Load pre-trained model parameters\n",
    "checkpoint = torch.load(\n",
    "    config.checkpoint_path / \"consolidated.00.pth\", \n",
    "    weights_only=True, \n",
    "    map_location=device,\n",
    ")\n",
    "\n",
    "config.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da1908f-31dd-4fff-a2d5-33aea8c39c52",
   "metadata": {},
   "source": [
    "We'll reference a number of the settings in `config` throughout the teardown. For now, a few interesting ones to note are `d_model`, `d_fnn`, `n_layers`, and `n_heads`. These represent the main differences between the 8B, 70B, and 405B sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "691d01c1-da77-4410-ab37-424024f5556e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "def load_pretrained_state(layer):    \n",
    "    # Load pre-trained state\n",
    "    llama.load_state(\n",
    "        normalize_attention, \"normalize_attention\", \n",
    "        normalize_ffn, \"normalize_ffn\", \n",
    "        w_q, \"w_q\", \n",
    "        w_k, \"w_k\", \n",
    "        w_v, \"w_v\", \n",
    "        attention_outputs, \"attention_outputs\",\n",
    "        ffn_gates, \"ffn_gates\",\n",
    "        ffn_inputs, \"ffn_inputs\",\n",
    "        ffn_outputs, \"ffn_outputs\",\n",
    "        checkpoint=checkpoint,\n",
    "        layer=layer,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f9a785-f3f1-40d0-9665-b32958eb1098",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Raw Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d92a6f-19b2-4355-a68d-d2deb93aff9a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Before we can tear anything down, we need a prompt. Since our goal is to trace an inference from raw text to the first output token, we want to start with a prompt that's specific enough to generate a consistent, one-word answer. If we do everything right, the first output token we predict should be \"Boston\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d167217-0eb0-4408-b5ed-4fdc9f315e2a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "prompt += \"What is the capital of Massachusetts? Answer in one word.\"\n",
    "prompt += \"<|eot_id|>\"\n",
    "prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d644832-0b53-4197-a7e4-fe6669dd07f9",
   "metadata": {},
   "source": [
    "You can see `prompt` already includes a number of special tokens. While the chatbot we build at the end will add these for us automatically, we need to manually inject them for now. You can read more about the Llama 3.1 prompt syntax in the [Llama Prompting Guide](https://www.llama.com/docs/how-to-guides/prompting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab8cd61-85ae-4fa6-acae-de7c7053b786",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdf7ba8-caf5-480b-bc8e-88aef06f3a1b",
   "metadata": {},
   "source": [
    "The Tokenize stage splits raw text into tokens using a fixed vocabulary. Llama uses a vocabulary of 128k tokens built on top of [OpenAI's tiktoken](https://github.com/openai/tiktoken) tokenizer. We'll dig into the gory details in the later stages, but here we'll simply use the off-the-shelf Tokenizer from Meta's [llama-models](https://github.com/meta-llama/llama-models) reference implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e32934a-bf49-46ab-8326-44d10d63e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer model from checkpoint\n",
    "tokenizer = Tokenizer(str(config.checkpoint_path / \"tokenizer.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c17a641c-8673-47cc-b5fd-f1f6f4dec4ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 128006, 882, 128007, 271, 3923, 374, 279, 6864, 315, 22108, 30, 22559, 304, 832, 3492, 13, 128009, 128006, 78191, 128007, 271]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split raw text into tokens\n",
    "token_ids = tokenizer.encode(prompt, bos=True, eos=False, allowed_special=\"all\")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cd1dd6e-66af-4e9d-a3ca-3efc036ac85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a781f891-605e-4819-95bb-413b11986a5b",
   "metadata": {},
   "source": [
    "We see `tokenizer.encode` split our prompt into 22 token ids. These ids represent the index of each token in Llama's 128k token vocabulary. We can always reverse the process with `tokenizer.decode`. If you look closely at the cell output below, you'll notice the tokenizer injected another special token `(128000, '<|begin_of_text|>')` to mark the beginning of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a5a247d-4cdc-47b3-bb66-2344ad22b360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the capital of Massachusetts? Answer in one word.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode token ids back into raw text\n",
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee78eb7-8dce-427e-94f2-71a91fe4da71",
   "metadata": {},
   "source": [
    "Our last step is to convert `token_ids` into a PyTorch tensor. The `x` variable represents our \"current state\". We'll trace `x` through every stage of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ccebd8f-8225-4bc3-8b12-a367620847e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load token_ids into a tensor\n",
    "x = torch.tensor(token_ids, device=device)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa0acf-5be2-44e2-97f4-bbc4ba3e4486",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c9eee8-e93f-48d9-9e9c-ed97bf0e84e3",
   "metadata": {},
   "source": [
    "The Embeddings stage converts individual tokens into embeddings. Embeddings (Bengio et al. 2000) are vectors that represent tokens as unique points in a multi-dimensional vector subspace. Embeddings serve as the fundamental data structure of Transformers. The context layers we'll look at in the next stage take embeddings as input, transform them, and produce embeddings as output. \n",
    "\n",
    "In the following cells, we start by creating a lookup table with one embedding for each of the tokens in the Llama vocabulary. Next, we load the pre-trained embedding values from the model checkpoint. Finally, we use the lookup table to map each token value to an embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a85e9908-06bd-4fe8-b2f4-1c67cd465f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings lookup table\n",
    "embeddings = nn.Embedding(\n",
    "    num_embeddings=config.vocab_size, \n",
    "    embedding_dim=config.d_model,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "llama.load_state(embeddings, \"embeddings\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9018c1ba-d851-48c0-8aec-3362da89cecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map token values to embeddings\n",
    "x = embeddings(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b6a636-ec97-4a6f-9930-8f81216470da",
   "metadata": {},
   "source": [
    "From `x.shape` above, you can see the embeddings are represented as row vectors stacked together into an $n \\times d_{model}$ tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91b63aee-ed54-425c-98cf-1306666eebe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
       "          6.3419e-05,  1.1902e-03],\n",
       "        [-1.6499e-04, -2.4319e-04,  1.6403e-04,  ..., -1.5163e-04,\n",
       "          3.5095e-04,  7.3242e-04],\n",
       "        [ 3.5095e-03,  7.2021e-03,  5.3406e-05,  ..., -7.2479e-04,\n",
       "         -1.0620e-02,  8.2779e-04],\n",
       "        ...,\n",
       "        [-9.7656e-03, -3.4637e-03,  1.8616e-03,  ..., -7.1411e-03,\n",
       "         -4.3030e-03,  8.6060e-03],\n",
       "        [-4.6158e-04, -3.9291e-04, -6.5863e-06,  ..., -6.2561e-04,\n",
       "         -5.0354e-04,  6.6757e-04],\n",
       "        [-2.8687e-03,  3.8910e-03, -1.7357e-04,  ...,  8.0872e-04,\n",
       "          5.0354e-04,  2.3041e-03]], device='mps:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show sample\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d1ae0d-4702-461f-bcde-24f6d5857414",
   "metadata": {},
   "source": [
    "# Context Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690aea2b-ccc5-4480-b835-dcf0b40eabb7",
   "metadata": {},
   "source": [
    "The Context Layers in a Transformer are responsible for infusing each token embedding with contextual signals drawn from the rest of the sequence. The mechanism works by passing the token embeddings through multiple layers of attention and feedforward blocks. The attention blocks focus on relationships between tokens, augmenting each embedding with a weighted combination of the surrounding embeddings. The feedforward blocks capitalize on the extra context, transforming each augmented embedding using a fully connected neural network. This pattern of attention and transformation is repeated over and over again, gradually converting representations of individual words into representations of abstract semantic concepts over a series of small increments.\n",
    "\n",
    "<figure>\n",
    "<img src=\"transformer-layers.svg\" width=\"800\">\n",
    "<figcaption>Figure 2: Context Layers</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e66e1bd-265f-46a4-ae62-33444b61e1b6",
   "metadata": {},
   "source": [
    "Figure 2 illustrates the components of each context layer in more detail. The input embeddings are first passed to the Attention block and then the FFN block. Note the Residual Input and Residual Attention paths that are combined with the Attention and FFN outputs. These residuals are critical for providing a stable path for gradient flow during training (He et al. 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e3b2c-e215-4c92-aa76-d7d94745d6a6",
   "metadata": {},
   "source": [
    "## Position Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf98312-8518-4a6e-987e-065ad462d0fa",
   "metadata": {},
   "source": [
    "The relevance of one token to another often depends on their proximity, making token positions critical to all Transformers. While early Transformer models like Vanilla and BERT encoded the absolute token positions directly in the input representation, more recent models have adopted *relative* position encoding schemes shown to perform better on larger sequences. Inspired by GPTNeo, Llama replaces absolute position encoding with the Rotary Position Embedding (RoPE) approach from Su et al. (2021). (Touvron et al. 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a91de23-d15e-43e9-ae73-c805d9dc24ff",
   "metadata": {},
   "source": [
    "### Rotary Position Embedding (RoPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a9cf2-dfb3-4a9b-b837-1b898d2e8b76",
   "metadata": {},
   "source": [
    "One of the reasons embeddings are so powerful is they represent the semantic meaning of abstract concepts in a form that you can do math with. For example, if you create embeddings for the words \"king\", \"queen\", \"man\", and \"woman\" using Word2Vec, you can show that the closest embedding to $E_{king} - E_{man} + E_{woman}$ is $E_{queen}$. I know that's a bit of a mind bender, but the point of the story is the power of embeddings often comes from the \"distance\" between them.\n",
    "\n",
    "While there are multiple ways to measure distance between vectors, Transformers specifically define distance between embeddings as the angle between their vectors. If the angle is small, the embeddings are close to each other. If the angle is large, the embeddings are further apart.\n",
    "\n",
    "> RoPE works by converting the distance between token positions into angular distance between embedding vectors, fitting perfectly into the Transformer's mental model of \"distance\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d20d0-7100-4fc7-9a75-411e1e4aaae1",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"rope-concept.svg\" width=\"800\">\n",
    "<figcaption>Figure 3: RoPE Concept in 2D</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca583be-99db-4ddf-b9e2-2a941610a58a",
   "metadata": {},
   "source": [
    "Figure 3 illustrates the intuition behind RoPE in 2-dimensions. On the left, we have a sequence of 2-dimensional vectors $\\begin{bmatrix}\\mathbf{x}_0 & \\mathbf{x}_1 & \\mathbf{x}_2\\end{bmatrix}^T$ with their 2-dimensional geometric interpretation plotted on the right. The matrix in the center represents a rotational transformation. Given an token's position $m$, the idea behind RoPE is to rotate the token's embedding a distance of $m \\theta$. Following this idea, $x_0$ would stay the same, $x_1$ would be rotated a distance of $\\theta$, and $x_2$ would be rotated $2 \\theta$.\n",
    "\n",
    "While Figure 3 illustrates RoPE conceptually in 2D, implementing it is a little more complicated. In practice, RoPE splits the token embeddings into pairs, e.g. {::nomarkdown}$\\Set{(\\mathbf{x}_0,\\mathbf{x}_1), (\\mathbf{x}_2,\\mathbf{x}_3), \\dots}${:/}, and then applies 2D rotations to each pair using the rotation matrix $\\mathbf{R}_{\\Theta,m}^d$.\n",
    "\n",
    "Given a hyperparameter $\\Theta$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{R}_{\\Theta,m}^d &= \n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) & -sin(m \\theta_0) & 0 & 0 & \\dots & 0 & 0 \\\\\n",
    "sin(m \\theta_0) & cos(m \\theta_0) & 0 & 0 & \\dots & 0 & 0 \\\\\n",
    "0 & 0 & cos(m \\theta_1) & -sin(m \\theta_1) & \\dots & 0 & 0 \\\\\n",
    "0 & 0 & sin(m \\theta_1) & cos(m \\theta_1) & \\dots & 0 & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & \\dots & 0 & 0 & cos(m \\theta_{d/2-1}) & -sin(m \\theta_{d/2-1}) \\\\\n",
    "0 & 0 & \\dots & 0 & 0 & sin(m \\theta_{d/2-1}) & cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\text{where } \\theta_i &= \\frac{1}{\\Theta^{2i/d_{head}}}, \\text{and } i \\text{ is the embedding index}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Luckily, Su et al. (2021) give us a more efficient approach to apply $\\mathbf{R}_{\\Theta,m}^d$ in practice.\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{\\Theta,m}^d \\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 \\\\\n",
    "\\vdots \\\\\n",
    "x_{d/2-2} \\\\\n",
    "x_{d/2-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "-x_2 \\\\\n",
    "x_1 \\\\\n",
    "-x_4 \\\\\n",
    "x_3 \\\\\n",
    "\\vdots \\\\\n",
    "-x_{d/2-1} \\\\\n",
    "x_{d/2-2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd5fc5f-08e7-4212-a792-c4db96a4d4af",
   "metadata": {},
   "source": [
    "Which finally brings us back to the teardown. Our goal here is to calculate the $cos$ and $sin$ vectors in the equation above. Since these are only a function of the shape of the token embeddings, we will reuse them across all the context layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9cfbaf4-0139-4000-899b-6623927f083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rope(n):\n",
    "    # Hyperparams\n",
    "    base = config.rope_theta\n",
    "    d = config.d_head\n",
    "    \n",
    "    # Compute theta_i = 1 / base^(2i/d) from i = 0 to d/2-1\n",
    "    thetas = 1.0 / base**(2 * torch.arange(d // 2, device=device) / d)\n",
    "    \n",
    "    # Compute m * theta_i for position m in 0 to n\n",
    "    frequencies = torch.stack([m*thetas for m in range(n)])\n",
    "    \n",
    "    # Duplicate each row\n",
    "    frequencies = torch.cat((frequencies, frequencies), dim=-1)\n",
    "    \n",
    "    # Apply cos, sin\n",
    "    cos = torch.cos(frequencies)\n",
    "    sin = torch.sin(frequencies)\n",
    "    \n",
    "    # Sanity check\n",
    "    assert cos.shape[0] == n and cos.shape[1] == config.d_head\n",
    "    assert sin.shape[0] == n and sin.shape[1] == config.d_head\n",
    "\n",
    "    return cos, sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5af2cbd3-8474-4e39-ad38-96d9c0428745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([22, 128]), torch.Size([22, 128]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute RoPE rotation matrices\n",
    "rope_cos, rope_sin = rope(len(x))\n",
    "\n",
    "rope_cos.shape, rope_sin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a29d2e-08b0-4e7a-ab5c-77752b44c271",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "**[Intro attention algorithm]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02279831-454b-44d8-9936-8e6dfc47873c",
   "metadata": {},
   "source": [
    "### Normalize Attention Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a9744-3624-4ce4-a378-d8d19719ed40",
   "metadata": {},
   "source": [
    "One difference between Llama and Vanilla architectures is in the approach to normalization. While Vanilla normalizes the outputs of each block, Llama uses a pre-normalization approach inspired by GPT3 (Touvron et al. 2023). In addition, Llama has replaced the LayerNorm algorithm with the less computationally expensive RMSNorm algorithm from Zhang and Sennrich (2019) (Touvron et al. 2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "744b2da1-20bb-4e52-a6ea-a883e78bf7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention normalization\n",
    "normalize_attention = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(normalize_attention, \"normalize_attention\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae0e97bf-cb15-41dc-a103-2db9fed51076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize attention inputs\n",
    "residual = x\n",
    "x = normalize_attention(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e000d-2f6c-4517-89af-fa162d4c2e57",
   "metadata": {},
   "source": [
    "### Project Queries, Keys, Values\n",
    "\n",
    "**[Introduce GQA]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c80d4879-ba66-449e-82cb-35e366d1cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure query, key, value projections\n",
    "w_q = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "w_k = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "w_v = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(w_q, \"w_q\", w_k, \"w_k\", w_v, \"w_v\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09a586eb-dc5c-482f-83d1-c8f358b2adec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([22, 4096]), torch.Size([22, 1024]), torch.Size([22, 1024]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project embeddings to query, key, value spaces\n",
    "q = w_q(x)\n",
    "k = w_k(x)\n",
    "v = w_v(x)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c240bc2-81c7-4058-af7d-8e80824ac100",
   "metadata": {},
   "source": [
    "**[Discuss GQA]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4fbce-a453-4ebd-b13e-0c014e69c6d6",
   "metadata": {},
   "source": [
    "### Split Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd757d84-d973-456a-b4ed-0dd57b31e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, n_heads):\n",
    "    return x.view(-1, n_heads, config.d_head).transpose(-3, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17d3364c-4647-4d83-bf0f-59f3d0f12c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 22, 128]), torch.Size([8, 22, 128]), torch.Size([8, 22, 128]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split attention heads\n",
    "q = split_heads(q, config.n_heads)\n",
    "k = split_heads(k, config.n_kv_heads)\n",
    "v = split_heads(v, config.n_kv_heads)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1228b160-f021-4529-8936-9daed82a8e27",
   "metadata": {},
   "source": [
    "### Encode Positions (RoPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df4e744d-a49f-4087-81db-b23ea17051c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 22, 128]), torch.Size([8, 22, 128]), torch.Size([8, 22, 128]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode positions by rotating queries and keys\n",
    "q = (q * rope_cos) + (llama.rotate_half(q) * rope_sin)\n",
    "k = (k * rope_cos) + (llama.rotate_half(k) * rope_sin)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e4c08-a857-4ea7-a81d-513f3835ce1d",
   "metadata": {},
   "source": [
    "### Expand Key / Value Groups (GQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9ca6a71-f43a-4b6a-a84a-f0f7c53f5f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 22, 128]), torch.Size([32, 22, 128]), torch.Size([32, 22, 128]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand key/value groups\n",
    "k = k.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "v = v.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dc7802b-8b5a-43a7-9752-59aab46d1b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert q.shape == k.shape == v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21619c-70bf-4bca-bc0c-13704f17ed07",
   "metadata": {},
   "source": [
    "### Calculate Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d3f15e1-5de8-4fb5-a430-dcce398657ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute attention mask M\n",
    "n = len(x)\n",
    "mask = torch.ones(n, n, dtype=torch.bool, device=device).tril(diagonal=0)\n",
    "m = torch.zeros(n, n, device=device).masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18342d97-3e5c-483d-b11b-92dbb3a717d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 22, 128])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute attention for all heads in parallel\n",
    "a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head) + m, dim=-1) @ v\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ae03c0-09d2-435a-9f53-0fd300b965f9",
   "metadata": {},
   "source": [
    "### Recombine Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "176a6b11-d24f-420f-b25e-0362c7f9a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_heads(x):\n",
    "    return x.transpose(-3, -2).contiguous().view(-1, int(config.n_heads * config.d_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05d7e054-822d-43eb-8e0b-8c068b22d941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine attention heads\n",
    "a = combine_heads(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d2d702-8cd1-4f46-b66d-49100a54ec7d",
   "metadata": {},
   "source": [
    "### Project Attention Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca228f80-7a27-4f3d-bd5b-1c7fca3b9c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention output projection\n",
    "attention_outputs = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(attention_outputs, \"attention_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6078b47b-a104-4f05-b84c-161ab12a1fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project attention embeddings back to model space\n",
    "a = attention_outputs(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fc1dc6-7f72-4938-9503-9f02fc7c41ac",
   "metadata": {},
   "source": [
    "### Combine w/ Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c08c7041-cfae-4286-a5c0-a6ac1cf095b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine attention embeddings with residuals\n",
    "x = residual + a\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40394845-477a-4d74-b95b-a5df2b05690c",
   "metadata": {},
   "source": [
    "## FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b07fd-ec9d-493f-ac63-55ade8980d7b",
   "metadata": {},
   "source": [
    "### Normalize FFN Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "282ff39a-0b80-43d9-b48c-282a72d4b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FFN normalization\n",
    "normalize_ffn = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained state\n",
    "llama.load_state(normalize_ffn, \"normalize_ffn\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "373aa399-40ba-423b-9871-25c0cd841489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize FFN inputs\n",
    "residual = x\n",
    "x = normalize_ffn(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7501951b-ba5d-4f58-853b-fb9e017a3cef",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21e42429-bfcb-43c8-b92f-0dbad588a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SwiGLU FFN\n",
    "ffn_gates = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_ffn,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "ffn_inputs = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_ffn,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(ffn_gates, \"ffn_gates\", ffn_inputs, \"ffn_inputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4927418-ffc4-4aac-b3c5-c62cfeb93f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 14336])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply transform\n",
    "f = silu(ffn_gates(x)) * ffn_inputs(x)\n",
    "\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a383c198-a280-4675-ae24-dbc43c838b96",
   "metadata": {},
   "source": [
    "### Project FFN Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26d4ac14-c19b-4102-9b49-824cd0e28de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FFN output projection\n",
    "ffn_outputs = nn.Linear(\n",
    "    in_features=config.d_ffn,\n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(ffn_outputs, \"ffn_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5baad73d-76c8-4cae-80b4-800563f92336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project FFN embeddings back to model space\n",
    "f = ffn_outputs(f)\n",
    "\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c274e4b-8d29-4e14-96ff-0113403b7012",
   "metadata": {},
   "source": [
    "### Combine w/ Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50883355-6553-45de-9e9b-3ad27371ff88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine FFN embeddings with residuals\n",
    "x = residual + f\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4afd793-c7d6-410f-8ea7-a0c87aaa368f",
   "metadata": {},
   "source": [
    "## Stacking the Layers\n",
    "\n",
    "Now that we've gone through each step, let's put all the pieces together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94982f78-8e24-4f60-844e-941b38cca48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_layers(x):\n",
    "    # Compute RoPE rotation matrices\n",
    "    rope_cos, rope_sin = rope(len(x))\n",
    "\n",
    "    # Apply layer logic in a loop\n",
    "    for layer in range(config.n_layers):\n",
    "    \n",
    "        # Load pre-trained state for layer\n",
    "        load_pretrained_state(layer)\n",
    "    \n",
    "        #\n",
    "        # Attention\n",
    "        #\n",
    "    \n",
    "        # Normalize attention inputs\n",
    "        residual = x\n",
    "        x = normalize_attention(x)\n",
    "        \n",
    "        # Project embeddings to query, key, value spaces\n",
    "        q = w_q(x)\n",
    "        k = w_k(x)\n",
    "        v = w_v(x)\n",
    "        \n",
    "        # Split attention heads\n",
    "        q = split_heads(q, config.n_heads)\n",
    "        k = split_heads(k, config.n_kv_heads)\n",
    "        v = split_heads(v, config.n_kv_heads)\n",
    "    \n",
    "        # Encode positions by rotating queries and keys\n",
    "        q = (q * rope_cos) + (llama.rotate_half(q) * rope_sin)\n",
    "        k = (k * rope_cos) + (llama.rotate_half(k) * rope_sin)\n",
    "        \n",
    "        # Expand key/value groups\n",
    "        k = k.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "        v = v.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "    \n",
    "        # Compute masked attention bias M\n",
    "        n = len(x)\n",
    "        mask = torch.ones(n, n, dtype=torch.bool, device=device).tril(diagonal=0)\n",
    "        m = torch.zeros(n, n, device=device).masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "        \n",
    "        # Compute attention for all heads in parallel\n",
    "        a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head) + m, dim=-1) @ v\n",
    "    \n",
    "        # Combine attention heads\n",
    "        a = combine_heads(a)\n",
    "        \n",
    "        # Project attention embeddings back to model space\n",
    "        a = attention_outputs(a)\n",
    "        \n",
    "        # Combine attention embeddings with residuals\n",
    "        x = residual + a\n",
    "        \n",
    "        #\n",
    "        # FFN\n",
    "        #\n",
    "    \n",
    "        # Normalize FFN inputs\n",
    "        residual = x\n",
    "        x = normalize_ffn(x)\n",
    "    \n",
    "        # Apply transform\n",
    "        f = silu(ffn_gates(x)) * ffn_inputs(x)\n",
    "    \n",
    "        # Project FFN embeddings back to model space\n",
    "        f = ffn_outputs(f)\n",
    "        \n",
    "        # Combine FFN embeddings with residuals\n",
    "        x = residual + f\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7caa67da-3466-4427-971e-8b212f33e84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start over from initial tokens\n",
    "x = torch.tensor(token_ids, device=device)\n",
    "\n",
    "# Initial embeddings\n",
    "x = embeddings(x)\n",
    "\n",
    "# Contextualized embeddings\n",
    "x = context_layers(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9d6ac66-280a-488e-b38a-ea74dd8e3b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8842,  1.9047,  1.0641,  ..., -1.3221,  2.1526,  1.3637],\n",
       "        [ 0.5711, -0.4364, -0.1372,  ..., -0.0924, -0.2381, -0.1401],\n",
       "        [-0.2568, -0.5273, -0.4703,  ...,  0.2887,  1.0332, -1.0900],\n",
       "        ...,\n",
       "        [-0.0088, -0.0946, -0.1153,  ...,  0.2482,  0.1270, -0.0051],\n",
       "        [ 0.2867, -0.0427,  0.3964,  ..., -0.1481,  0.2981, -0.3256],\n",
       "        [-0.9179,  0.6267,  0.4772,  ...,  0.0447,  1.8657, -0.3242]],\n",
       "       device='mps:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae7bf87-1b85-45cc-9a5e-94bffa34d9c8",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a556f-ae27-4797-9f91-0330fda4b1e0",
   "metadata": {},
   "source": [
    "## Normalize Head Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "411660dd-e187-4043-90d1-5a7f519f425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure head normalization\n",
    "normalize_head = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(normalize_head, \"normalize_head\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c05af81b-b8b2-4c4e-b589-432316e3e7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 4096])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize head inputs\n",
    "x = normalize_head(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c56884-1f30-4aa4-b83c-b8c8dbcfc0f4",
   "metadata": {},
   "source": [
    "## Project Head Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94679c13-fffa-46b4-aad5-98eee9d8e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure output projection\n",
    "head_outputs = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.vocab_size,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(head_outputs, \"head_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a776956-9e97-4aec-b1ac-6933e7d3338d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128256])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use last embedding to represent the entire sequence\n",
    "x = x[-1]\n",
    "\n",
    "# Project outputs to token space\n",
    "x = head_outputs(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d4b8d-68fd-4ffa-9f74-b86f537508ff",
   "metadata": {},
   "source": [
    "## Top Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8581035-27a1-488e-b19e-eb7a93079c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boston'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select top scoring token\n",
    "token_id = x.argmax()\n",
    "\n",
    "# Decode token\n",
    "token = tokenizer.decode([token_id]).strip()\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd25f8f2-3b5f-4042-8736-2dc8f86418d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify answer\n",
    "assert token == \"Boston\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9fdb8f-7ab8-478d-a202-9f146db1f5e7",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e284c62e-ce76-4ff6-9b4f-121dd2df01ba",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "957cbab2-a730-4648-91c8-2b037496cd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "temperature = config.temperature\n",
    "temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef5b0e6b-f199-40cb-9403-9c6241e607da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply temperature\n",
    "x = x / temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7129c00f-56bb-44a9-a3e4-0c9ed88bf38e",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6e8b207-31c3-45cf-98fb-d11e38508bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to probabilities\n",
    "probs = softmax(x)\n",
    "\n",
    "# Sort probabilities in descending order\n",
    "probs, indices = probs.sort(descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcf8c58-8890-4bfb-a1f9-f7b8fc487a8c",
   "metadata": {},
   "source": [
    "### Top K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14a802ff-fff6-4401-b081-f85ebb5dbf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "top_k = config.top_k\n",
    "top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b4d2fb95-dcf1-47c3-a89a-65e22b94c5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 50 of 128256\n"
     ]
    }
   ],
   "source": [
    "# Retain top k tokens\n",
    "probs = probs[:top_k]\n",
    "print(f\"Retained {len(probs)} of {len(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe60337-1fa1-4b5f-b51d-7090bfc750b2",
   "metadata": {},
   "source": [
    "### Top P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dad0f58f-270c-48d1-be50-7272f991e149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "top_p = config.top_p\n",
    "top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "263f3633-4e36-4704-94f2-56bb278b7cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 2 of 128256\n"
     ]
    }
   ],
   "source": [
    "# Find cutoff where cumulative probability exceeds top_p\n",
    "cumulative_mask = probs.cumsum(dim=-1) > top_p\n",
    "threshold_index = torch.argmax(cumulative_mask).item()\n",
    "\n",
    "# Only apply threshold if top_p was exceeded\n",
    "if cumulative_mask.any():\n",
    "    probs = probs[:threshold_index+1]\n",
    "\n",
    "print(f\"Retained {len(probs)} of {len(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27636666-22eb-48aa-a6e6-d8df8ab7d03e",
   "metadata": {},
   "source": [
    "### Random Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42871662-98b9-40c8-9d0e-0d8628833f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token id 65432, token 'Boston', score 0.661\n",
      "token id 791, token 'The', score 0.338\n"
     ]
    }
   ],
   "source": [
    "# Print remaining token pool\n",
    "for i, prob in enumerate(probs):\n",
    "    print(f\"token id {indices[i]}, token '{tokenizer.decode([indices[i]])}', score {prob:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea7d2a10-8647-408a-8c56-c7e413ad7c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boston'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample from remaining tokens weighted by probability\n",
    "sampled_index = torch.multinomial(probs, 1)\n",
    "\n",
    "# Convert sampled_index to original logits\n",
    "token_id = indices[sampled_index]\n",
    "\n",
    "# Decode token\n",
    "token = tokenizer.decode([token_id]).strip()\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34176fda-b314-4185-a836-972594ca8b45",
   "metadata": {},
   "source": [
    "## Complete Head Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b911c61-42f1-4239-87a3-4f1abfccee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def head(x):\n",
    "    # Normalize head inputs\n",
    "    x = normalize_head(x)\n",
    "    \n",
    "    # Use last embedding to represent the entire sequence\n",
    "    x = x[-1]\n",
    "    \n",
    "    # Project outputs to token space\n",
    "    x = head_outputs(x)\n",
    "\n",
    "    #\n",
    "    # Temperature\n",
    "    #\n",
    "    \n",
    "    # Apply temperature\n",
    "    x = x / config.temperature\n",
    "\n",
    "    #\n",
    "    # Ranking\n",
    "    #\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probs = softmax(x)\n",
    "    \n",
    "    # Sort probabilities in descending order\n",
    "    probs, indices = probs.sort(descending=True)\n",
    "\n",
    "    #\n",
    "    # Top K\n",
    "    #\n",
    "    \n",
    "    # Retain top k tokens\n",
    "    probs = probs[:config.top_k]\n",
    "\n",
    "    #\n",
    "    # Top P\n",
    "    #\n",
    "    \n",
    "    # Find cutoff where cumulative probability exceeds top_p\n",
    "    cumulative_mask = probs.cumsum(dim=-1) > config.top_p\n",
    "    threshold_index = torch.argmax(cumulative_mask).item()\n",
    "    \n",
    "    # Only apply threshold if top_p was exceeded\n",
    "    if cumulative_mask.any():\n",
    "        probs = probs[:threshold_index+1]\n",
    "\n",
    "    #\n",
    "    # Random Selection\n",
    "    #\n",
    "    \n",
    "    # Sample from remaining tokens weighted by probability\n",
    "    sampled_index = torch.multinomial(probs, 1)\n",
    "    \n",
    "    # Convert sampled_index to original logits\n",
    "    token_id = indices[sampled_index]\n",
    "\n",
    "    return token_id.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d6c609-b231-44f3-b019-36f5bb6fadad",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "caf733a9-98de-45c4-83ca-4957521f4277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7b81eebb-7ffd-4a88-9b41-e7adc11ded07",
   "metadata": {},
   "outputs": [],
   "source": [
    "@validate_call\n",
    "def prepare_messages(messages: list[Message]):\n",
    "    # Initialize prompt\n",
    "    prompt = \"\"\n",
    "    \n",
    "    # Format each message\n",
    "    for message in messages:\n",
    "        prompt += f\"<|start_header_id|>{message.role}<|end_header_id|>\\n\\n\"\n",
    "        prompt += message.content\n",
    "        prompt += \"<|eot_id|>\"\n",
    "\n",
    "    # Finish with the assistant role to prime the model's response\n",
    "    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "89131319-b332-4f7d-818a-f6b3dbcae22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@validate_call\n",
    "def generate(messages: list[Message]):\n",
    "    # Format message prompt\n",
    "    prompt = prepare_messages(messages)\n",
    "    \n",
    "    # Split raw text into tokens\n",
    "    token_ids = tokenizer.encode(prompt, bos=True, eos=False, allowed_special=\"all\")\n",
    "    \n",
    "    # Generate output until we get a stop token or we exceed max_output_tokens.\n",
    "    for _ in range(config.max_output_tokens):\n",
    "        \n",
    "        # Start over from initial tokens\n",
    "        x = torch.tensor(token_ids, device=device)\n",
    "        \n",
    "        # Initial embeddings\n",
    "        x = embeddings(x)\n",
    "        \n",
    "        # Contextualized embeddings\n",
    "        x = context_layers(x)\n",
    "        \n",
    "        # Head\n",
    "        token_id = head(x)\n",
    "        \n",
    "        # Check stopping criteria\n",
    "        if token_id in tokenizer.stop_tokens:\n",
    "            break\n",
    "    \n",
    "        # Print token\n",
    "        token = tokenizer.decode([token_id])\n",
    "        stdout.write(token)\n",
    "        \n",
    "        # Append to end of sequence\n",
    "        token_ids.append(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2613f3c3-455e-41cf-84d5-b43dfbf5fe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Massachusetts is Boston."
     ]
    }
   ],
   "source": [
    "generate([\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is capital of Massachusetts?\",\n",
    "    },\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365758e1-c14e-41c3-a1de-870fe894d229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c944e7-561f-43c1-9145-05cc99ba95fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f595bb-4045-4b42-bb50-08a6d61abc15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1fd42f-54ed-4c9f-b4b1-7071d5f10cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730570bc-ea61-491d-a220-c379ca8ce1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0e312-3ec8-4d0b-8f44-c56dbf89f403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077f131-8e3b-4211-b6a0-a351efa14f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59730d66-3c6b-40a4-bc07-7f9d8e570f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "stickshift": {
   "description": "Trace an Inference Through Each Layer in a SOTA Transformer Using Llama 3.1 Open Source Foundation Models",
   "draft": true,
   "title": "Transformer Teardown: Llama 3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
