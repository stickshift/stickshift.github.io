<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Writing a Transformer One Cell at a Time | Pattern Recognition</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Writing a Transformer One Cell at a Time" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="There is something special about Transformers. Since first introduced in 2017, the Transformer architecture has redefined the entire NLP category and is quickly spreading to other types of machine learning. Transformers, and the language embeddings at their core, seem to have hit on some underlying fundamental substrate that transcends boundaries, making them far more generalizable than previous approaches. Anyone who wants to contribute to the future of AI should clearly be studying transformers inside and out. While there are a million papers, blogs, and tutorials written on Transformers, I still find it challenging to map the abstract ideas from the research literature into concrete, actionable steps I can experiment with. My engineer&#39;s brain wants to &quot;see the code&quot; behind high level concepts like embeddings, residuals, and multi-head self-attention. While it&#39;s easy to find open source Transformer implementations, they are often overloaded with configuration settings, conditionalized to support every variation ever imagined, to the point that the main ideas are completely obscured. The goal of this post is to give you a stronger sense for how Transformers work under the hood. We&#39;ll take a single inference—from raw data to final prediction—and break it down into tiny steps, illustrating the main ideas from the Transformer literature with minimal, straightforward, working Python code. You may be surprised by how few steps are required! Setup¶ In [1]: import warnings from matplotlib import pyplot as plt import seaborn as sns import numpy as np from pytest import approx import torch from torch import nn from torch.nn.functional import relu, softmax import transformers from stickshift.models import distilbert In [2]: # Ignore all warnings warnings.filterwarnings(&quot;ignore&quot;) # Configure gpu device = torch.device(&quot;cpu&quot;) if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) elif torch.backends.mps.is_available(): device = torch.device(&quot;mps&quot;) Text Classification with DistilBERT¶If you&#39;ve worked with Transformers at all, I&#39;m sure you&#39;re familiar with Hugging Face&#39;s collection of Python libraries as well as their endless repository of models and datasets. Throughout the post, we&#39;ll be working with Hugging Face&#39;s default text classification model: DistilBERT. DistilBERT is a smaller, faster, lighter-weight version of the original BERT model that&#39;s easier to experiment with. We&#39;ll use the pre-trained model parameters from Hugging Face, but we&#39;ll implement the model&#39;s logic step-by-step in Jupyter using a slightly modified version of the actual DistilBERT PyTorch implementation from Hugging Face&#39;s transformers library. Before we get into the implementation, let&#39;s start by running the entire process end-to-end using Hugging Face&#39;s high level pipeline API. The following cells create a complete text classification pipeline and then apply it to the sentence &quot;I love ice cream&quot;. As you might expect, the model classifies the sentence as overwhelmingly positive. There is a lot happening in very few lines of code here. Over the rest of the post, we&#39;ll break this prediction down and recreate it one step at a time. In [3]: # Create off-the-shelf text classification transformer transformer = transformers.pipeline(&quot;text-classification&quot;, device=device) No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english). Using a pipeline without specifying a model name and revision in production is not recommended. In [4]: transformer(&quot;I love ice cream&quot;) Out[4]: [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9998118281364441}] The code samples throughout this post recreate portions of Hugging Face&#39;s DistilBERT implementation. They&#39;ve been lightly refactored to improve clarity and help illustrate the core ideas of the Transformer architecture. The following cell loads a custom version of the model configuration as well as the pre-trained model parameters. We&#39;ll reference these as we go. In [5]: # Load model config and pre-trained parameters config = distilbert.config(transformer.model) parameters = transformer.model.state_dict() Transformer Pipeline¶The following diagram depicts a Transformer as a multi-stage pipeline. The Context stage at the center of the pipeline is where most of the magic happens. The stages before and after Context provide the extra machinery required to convert raw data into input embeddings and output embeddings into task-specific outputs. While we&#39;ll focus on text data, it&#39;s worth noting that the same stages can be applied to all data modalities including audio and images (Xu et al. 2023). Tokenize¶ The Tokenize stage is responsible for breaking raw data into a sequence of &quot;tokens&quot;. While the word &quot;token&quot; is often associated with text processing, the Transformer literature extends this to other data modalities as well. Examples include patches of an image or segments of an audio recording. In fact, tokenization is seen as a core strength of the Transformer architecture because it allows Transformers to process different types of data using a single, universal approach (Xu et al. 2023). While tokenization is a general concept, the specific algorithms used are modality-specific. In this case, our transformer uses an algorithm known as &quot;word-piece&quot; (Devlin et al. 2019) to split raw text into a sequence of tokens. Next, special tokens are injected to mark the beginning and end of the sequence. Each token is then converted into an integer-encoded categorical value using a fixed token vocabulary, producing the final sequence of &quot;input_ids&quot; that are passed to the next stage. Since our primary interest is in the Transformer layers that come later, we&#39;ll use Hugging Face&#39;s off-the-shelf tokenizer implementation here. In [6]: # Extract tokenizer from transformer tokenizer = transformer.tokenizer In [7]: # Tokenize sentence batch = tokenizer(&quot;I love ice cream&quot;, return_tensors=&quot;pt&quot;) batch Out[7]: {&#39;input_ids&#39;: tensor([[ 101, 1045, 2293, 3256, 6949, 102]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1]])} Tokenizing &quot;I love ice cream&quot; generates the token sequence: [101, 1045, 2293, 3256, 6949, 102]. If we decode the integer-encoded values to see what each one represents, we can see the 4 words are represented by values 1045 to 6949. The values 101 and 102 represent special tokens [CLS] and [SEP] that were added to mark the beginning and end of the sequence respectively. In [8]: [tokenizer.decode(input_id) for input_id in batch.input_ids[0]] Out[8]: [&#39;[CLS]&#39;, &#39;i&#39;, &#39;love&#39;, &#39;ice&#39;, &#39;cream&#39;, &#39;[SEP]&#39;] Embeddings¶ The second stage in the Transformer pipeline converts each of the integer-encoded categorical values into an &quot;embedding&quot;. Embeddings (Bengio et al. 2000) are the fundamental data structure of the Transformer architecture. The Transformer layers we&#39;ll look at in the next stage take embeddings as input, transform them, and produce embeddings as output. Embeddings predated Transformers by almost 2 decades and are a fascinating topic in their own right. But we&#39;ll save the embeddings deep dive for another post. For now, all we need to know is embeddings represent each token as a unique point in an n-dimensional vector space. The vector space coordinates are initialized randomly and then learned during training. Similar to tokenization, the steps required to convert tokens into embeddings depend on the data modality. In text transformers, the Embeddings stage is typically implemented using 2 lookup tables. The first lookup table maps the value of each token to a unique embedding vector. The second lookup table maps the position of each token to a unique embedding vector. The value and position embeddings are then added together to create the initial token embeddings. Let&#39;s start with value embeddings. First, we initialize the value embeddings lookup table. Next, we read the values from the tokenizer output. Finally, we pass the token values to the lookup table to get unique embeddings for each value. In [9]: # Initialize value embeddings lookup table value_embeddings = nn.Embedding( num_embeddings=config.vocab_size, embedding_dim=config.d_model, ) # Load pre-trained state value_embeddings.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.embeddings.word_embeddings.weight&quot;], }) Out[9]: &lt;All keys matched successfully&gt; In [10]: # Calculate token values values = torch.squeeze(batch.input_ids) [tokenizer.decode(input_id) for input_id in values] Out[10]: [&#39;[CLS]&#39;, &#39;i&#39;, &#39;love&#39;, &#39;ice&#39;, &#39;cream&#39;, &#39;[SEP]&#39;] In [11]: # Map token values to embeddings v = value_embeddings(values) v.shape Out[11]: torch.Size([6, 768]) In [12]: # Show sample of value embeddings v Out[12]: tensor([[ 3.9925e-02, -1.0171e-02, -2.0390e-02, ..., 6.1588e-02, 2.1959e-02, 2.2732e-02], [-1.2794e-02, 4.9879e-03, -2.6270e-02, ..., -7.2300e-05, 5.3657e-03, 1.1908e-02], [ 5.9359e-02, -2.3563e-02, -2.0560e-03, ..., -1.0420e-02, 1.4846e-02, -1.2815e-02], [-2.4101e-02, -2.4911e-02, -2.2601e-02, ..., -2.5139e-02, 1.1392e-02, 3.2655e-02], [-8.5466e-02, -5.9276e-02, -5.6659e-02, ..., -1.7192e-02, -8.6179e-02, -4.5105e-02], [-2.1060e-02, -6.4941e-03, -1.0682e-02, ..., -2.3401e-02, 6.1463e-03, -6.4845e-03]], grad_fn=&lt;EmbeddingBackward0&gt;) Next, we&#39;ll follow a similar set of steps for the position embeddings. We&#39;ll start by initializing the position embeddings lookup table. Next, we&#39;ll calculate the positions from the tokenizer output. Finally, we pass the token positions to the lookup table to get unique embeddings for each position. In [13]: # Configure position embeddings lookup table position_embeddings = nn.Embedding( num_embeddings=config.max_sequence_length, embedding_dim=config.d_model, ) # Load pre-trained state position_embeddings.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.embeddings.position_embeddings.weight&quot;], }) Out[13]: &lt;All keys matched successfully&gt; In [14]: # Calculate token positions positions = torch.arange(values.size(0)) positions Out[14]: tensor([0, 1, 2, 3, 4, 5]) In [15]: # Map token positions to embeddings p = position_embeddings(positions) p.shape Out[15]: torch.Size([6, 768]) In [16]: # Show sample of position embeddings p Out[16]: tensor([[ 1.8007e-02, -2.3798e-02, -3.5982e-02, ..., 4.5726e-04, 5.1363e-05, 1.5002e-02], [ 7.8592e-03, 4.8144e-03, -1.6093e-02, ..., 2.9312e-02, 2.7634e-02, -8.5431e-03], [-1.1663e-02, -3.1590e-03, -9.4000e-03, ..., 1.4870e-02, 2.1609e-02, -7.4069e-03], [-4.0848e-03, -1.1123e-02, -2.1704e-02, ..., 1.8962e-02, 4.6763e-03, -1.0220e-03], [-8.2666e-03, -4.1641e-03, -7.5136e-03, ..., 1.9757e-02, -2.2192e-03, 3.8681e-03], [ 4.6293e-04, -1.8499e-02, -1.9709e-02, ..., 5.4042e-03, 1.8076e-02, 2.9490e-03]], grad_fn=&lt;EmbeddingBackward0&gt;) Now that we have value and position embeddings, we add and normalize them to get the final &quot;position-encoded token embeddings&quot;. In [17]: # Configure embeddings normalization normalize_embeddings = nn.LayerNorm( normalized_shape=config.d_model, eps=1e-12, ) # Load pre-trained state normalize_embeddings.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.embeddings.LayerNorm.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.embeddings.LayerNorm.bias&quot;], }) Out[17]: &lt;All keys matched successfully&gt; In [18]: # Add and normalize value and position embeddings x = normalize_embeddings(v + p) x.shape Out[18]: torch.Size([6, 768]) In [19]: # Show sample of token embeddings x Out[19]: tensor([[ 0.3549, -0.1386, -0.2253, ..., 0.1536, 0.0748, 0.1310], [ 0.2282, 0.5511, -0.5092, ..., 0.6421, 0.9541, 0.3192], [ 1.4511, -0.0794, 0.2168, ..., 0.2851, 1.0723, -0.0919], [-0.0564, -0.1761, -0.2870, ..., 0.1442, 0.6767, 1.0396], [-1.1349, -0.5135, -0.4714, ..., 0.3874, -1.0348, -0.2812], [-0.2980, -0.3332, -0.3742, ..., -0.3392, 0.3764, -0.1298]], grad_fn=&lt;NativeLayerNormBackward0&gt;) Congrats! You&#39;ve converted the raw text &quot;I love ice cream&quot; into embeddings that encode both the token values and positions. Context¶ In the previous stage, we mapped the token values and positions to embeddings. But these embeddings represent the tokens in isolation. The Context stage is responsible for infusing each embedding with contextual signals drawn from the entire sequence. At a conceptual level, this should be intuitive. The meaning of the word &quot;ice&quot; changes when you add &quot;cream&quot; after it. The Context stage works by passing the token embeddings through multiple layers of attention and feedforward blocks. The attention blocks focus on relationships between tokens, augmenting each embedding with information drawn from the surrounding embeddings. The feedforward blocks focus on individual tokens, transforming the contextual clues added by attention with the non-linear transformation magic neural networks are famous for. The following diagram illustrates the stack of Transformer layers in the Context stage. The contents of each layer are identical. By arranging the layers in a stack, the model builds context in small increments similar to the hierarchical features in a CNN. The main differences between popular Transformer models such as BERT and GPT come down to how these layers are configured. As illustrated above, given input embeddings $X$, we can define the output embeddings $Z$ as: \begin{align} Y &amp;= Normalize(X + Attention(X)) \\ Z &amp;= Normalize(Y + FNN(Y)) \end{align} Scaled Dot-Product Attention¶The Attention block is the signature component of the Transformer architecture. It&#39;s also one of the most complicated and likely the least familiar when you&#39;re first learning about Transformers. We&#39;ll walk through the core attention algorithm described in the original &quot;All You Need is Attention&quot; paper by Vaswani et al. one step at a time. At the end of the Context section, we&#39;ll put all the pieces together. Vaswani et al. described their attention algorithm as Scaled Dot-Product Attention (SDPA) and defined the standard attention equation everyone cites: \begin{equation} Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_K}})V \end{equation} Queries, Keys, Values¶The $Q$, $K$, and $V$ terms in the SDPA equation are &quot;query&quot;, &quot;key&quot;, and &quot;value&quot; matrices respectively. Each row in $Q$, $K$, and $V$ represents a token embedding that has been projected to distinct representation subspaces. Query embeddings represent selection criteria for the surrounding tokens that would add context to the current token definition. Key embeddings represent characteristics that satisfy the selection criteria. Value embeddings represent the contextual information one token transfers to another. Together, queries, keys, and values allow the attention mechanism to refine the representation of each token based on the surrounding tokens. Given an $n \times d_{model}$ matrix of input embeddings $X$, we can expand on the SDPA equation by defining linear projections $Queries$, $Keys$, and $Values$: \begin{align} SDPA(Q, K, V) &amp;= softmax(\frac{QK^T}{\sqrt{d_K}})V \\ \text{where } Q &amp;= Queries(X) \\ K &amp;= Keys(X) \\ V &amp;= Values(X) \end{align} Enough LaTeX, let&#39;s see what this looks like in Python. In [20]: # Configure query, key, value projections queries = nn.Linear( in_features=config.d_model, out_features=config.d_model, ) keys = nn.Linear( in_features=config.d_model, out_features=config.d_model, ) values = nn.Linear( in_features=config.d_model, out_features=config.d_model, ) # Load pre-trained state queries.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.q_lin.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.q_lin.bias&quot;], }) keys.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.k_lin.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.k_lin.bias&quot;], }) values.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.v_lin.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.k_lin.bias&quot;], }) Out[20]: &lt;All keys matched successfully&gt; In [21]: # Project token embeddings to query, key, and value spaces q = queries(x) k = keys(x) v = values(x) q.shape, k.shape, v.shape Out[21]: (torch.Size([6, 768]), torch.Size([6, 768]), torch.Size([6, 768])) We can see the projections generated unique query, key, and value embeddings for each of the 6 tokens [&#39;[CLS]&#39;, &#39;i&#39;, &#39;love&#39;, &#39;ice&#39;, &#39;cream&#39;, &#39;[SEP]&#39;]. Attention Weights¶ Now that we have $Q$, $K$, and $V$, we can delve into the SDPA equation itself. For each input embedding, SDPA calculates a weighted sum of the value projections for all the tokens in the sequence. We already saw the value projections are represented by V. The weights are represented by the softmax term: \begin{align} softmax(\frac{QK^T}{\sqrt{d_K}}) \end{align} I wouldn&#39;t hold it against you if it&#39;s not immediately obvious what we get here. To see what&#39;s happening, let&#39;s break this down even further. First, the $QK^T$ term calculates a $d_Q \times d_K$ matrix of the dot products of each query embedding with every key embedding. To see why, imagine we have 2 token embeddings of length 3. \begin{align} QK^T &amp;= \begin{bmatrix} q_{00} &amp; q_{01} &amp; q_{02} \\ q_{10} &amp; q_{11} &amp; q_{12} \end{bmatrix} \begin{bmatrix} k_{00} &amp; k_{10} \\ k_{01} &amp; k_{11} \\ k_{02} &amp; k_{12} \end{bmatrix} = \begin{bmatrix} w_{00} &amp; w_{01} \\ w_{10} &amp; w_{11} \end{bmatrix} \\ \text{where } w_{ij} &amp;= row(Q, i) \cdot row(K, j) \end{align} In [22]: # Calculate similarity between Q and K w = q @ k.transpose(-2, -1) w.shape Out[22]: torch.Size([6, 6]) Second, the $1/\sqrt{d_K}$ term scales the dot products down to avoid pushing the softmax function into regions with very small gradients. In [23]: w /= np.sqrt(k.size(1)) w.shape Out[23]: torch.Size([6, 6]) Finally, the softmax function normalizes the weights across the keys. In [24]: # Normalize weights across keys w = softmax(w, dim=-1) w Out[24]: tensor([[9.9964e-01, 8.3108e-06, 5.9084e-06, 1.2478e-05, 1.7158e-05, 3.1488e-04], [9.6971e-01, 3.5093e-04, 3.8457e-03, 1.4637e-04, 2.0632e-04, 2.5743e-02], [9.8888e-01, 1.4836e-03, 3.4143e-04, 3.4613e-04, 1.1235e-03, 7.8293e-03], [7.4676e-01, 3.8455e-05, 1.0549e-03, 8.3005e-05, 2.2695e-01, 2.5122e-02], [7.5151e-01, 9.0717e-06, 2.4871e-04, 2.3733e-01, 1.7790e-05, 1.0880e-02], [8.6867e-01, 4.3978e-04, 4.6904e-05, 7.9754e-05, 1.2606e-03, 1.2950e-01]], grad_fn=&lt;SoftmaxBackward0&gt;) In [25]: # Plot weights for each query _, axs = plt.subplots(nrows=2, ncols=3, figsize=(10,5), gridspec_kw={&quot;wspace&quot;: 0.5, &quot;hspace&quot;: 0.5}) for i in range(6): ax = axs[i//3][i%3] sns.scatterplot(x=np.arange(len(w[i])), y=w[i].detach(), ax=ax) ax.set_title(f&quot;Query {i}&quot;) ax.set_xlabel(f&quot;Keys&quot;) ax.set_ylabel(f&quot;Weight&quot;) Attention Output¶ Now that we have the attention weights, we can apply them to the values. This will give us a weighted sum of contextual information. However, the answer is still in &quot;value space&quot;. Before we combine them with the token embeddings, we&#39;ll project them back to &quot;model space&quot;. In [26]: # Compute weighted combination of values a = w @ v a.shape Out[26]: torch.Size([6, 768]) In [27]: # Configure output projection outputs = nn.Linear(in_features=config.d_model, out_features=config.d_model) # Load pre-trained state outputs.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.out_lin.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.out_lin.bias&quot;], }) Out[27]: &lt;All keys matched successfully&gt; In [28]: # Project attention embeddings back to model space a = outputs(a) a.shape Out[28]: torch.Size([6, 768]) Multi-Head Attention¶At this point, we&#39;ve walked through the core SDPA algorithm step-by-step. However, we&#39;re not quite done. Vaswani et al. realized there are more than one set of relationships involved in transferring context across tokens. A single application of SDPA would effectively average these together. The solution is to apply SDPA multiple times on separate query, key, and value embeddings. Each of these is referred to as an &quot;attention head&quot;. Each head is isolated, leaving it free to learn distinct relational structures. In [29]: def split_heads(x): return x.view(-1, config.n_heads, config.d_head).transpose(-3, -2) def combine_heads(x): return x.transpose(-3, -2).contiguous().view(-1, int(config.n_heads * config.d_head)) In [30]: # Render query, key, value dimensions before we split q.shape, k.shape, v.shape Out[30]: (torch.Size([6, 768]), torch.Size([6, 768]), torch.Size([6, 768])) In [31]: # Split queries, keys, values into separate heads q = split_heads(q) k = split_heads(k) v = split_heads(v) q.shape, k.shape, v.shape Out[31]: (torch.Size([12, 6, 64]), torch.Size([12, 6, 64]), torch.Size([12, 6, 64])) We can see that the queries, keys, and values have been split into 12 heads, leaving each of the original 768-element query, key, and value embeddings is now 64 elements long. Next, let&#39;s recompute the attention embeddings. In [32]: # Compute attention for all heads in parallel a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head), dim=-1) @ v a.shape Out[32]: torch.Size([12, 6, 64]) While the attention code is the same, you can see the attention values are still split into heads. Next, we&#39;ll recombine them before applying the final output projection. In [33]: # Recombine heads a = combine_heads(a) a.shape Out[33]: torch.Size([6, 768]) In [34]: # Project attention embeddings back to model space a = outputs(a) a.shape Out[34]: torch.Size([6, 768]) Add and Normalize¶Before we get to the FNN, we&#39;ll combine the attention embeddings with input embeddings the same way we combined the value and position embeddings. In [35]: # Configure attention normalization normalize_attention = nn.LayerNorm(normalized_shape=config.d_model, eps=1e-12) # Load pre-trained state normalize_attention.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.sa_layer_norm.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.sa_layer_norm.bias&quot;], }) Out[35]: &lt;All keys matched successfully&gt; In [36]: # Combine attention with input embeddings y = normalize_attention(x + a) y.shape Out[36]: torch.Size([6, 768]) FNN¶The FNN block is a straightforward fully connected multi-layer perceptron. In [37]: # Configure FNN fnn = nn.Sequential( nn.Linear(in_features=config.d_model, out_features=config.d_fnn), nn.GELU(), nn.Linear(in_features=config.d_fnn, out_features=config.d_model), ) # Load pre-trained state fnn.load_state_dict({ &quot;0.weight&quot;: parameters[&quot;distilbert.transformer.layer.0.ffn.lin1.weight&quot;], &quot;0.bias&quot;: parameters[&quot;distilbert.transformer.layer.0.ffn.lin1.bias&quot;], &quot;2.weight&quot;: parameters[&quot;distilbert.transformer.layer.0.ffn.lin2.weight&quot;], &quot;2.bias&quot;: parameters[&quot;distilbert.transformer.layer.0.ffn.lin2.bias&quot;], }) Out[37]: &lt;All keys matched successfully&gt; In [38]: # Transform attention outputs f = fnn(y) f.shape Out[38]: torch.Size([6, 768]) Add and Normalize¶Next, we combine the FNN outputs with the attention outputs. In [39]: # Configure attention normalization normalize_fnn = nn.LayerNorm(normalized_shape=config.d_model, eps=1e-12) # Load pre-trained state normalize_fnn.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.output_layer_norm.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.output_layer_norm.bias&quot;], }) Out[39]: &lt;All keys matched successfully&gt; In [40]: z = normalize_fnn(y + f) z.shape Out[40]: torch.Size([6, 768]) Full Stack¶Quick recap. Given input embeddings $X$, we added attention embeddings to get $Y$, and added transformed embeddings to get $Z$. Next, we combine all of these elements together and repeat for each layer in the stack. While you would normally create a stack of torch modules, we run the layers in a loop instead to make it easier to see what&#39;s happening. In [41]: def attention(x, *, layer): # Configure query, key, value, and output projections queries = nn.Linear(in_features=config.d_model, out_features=config.d_model) keys = nn.Linear(in_features=config.d_model, out_features=config.d_model) values = nn.Linear(in_features=config.d_model, out_features=config.d_model) outputs = nn.Linear(in_features=config.d_model, out_features=config.d_model) # Load pre-trained state queries.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.q_lin.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.q_lin.bias&quot;], }) keys.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.k_lin.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.k_lin.bias&quot;], }) values.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.v_lin.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.v_lin.bias&quot;], }) outputs.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.out_lin.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.out_lin.bias&quot;], }) # Project x to query, key, and value spaces q = queries(x) k = keys(x) v = values(x) # Split q, k, v into separate heads q = split_heads(q) k = split_heads(k) v = split_heads(v) # Compute attention for all heads in parallel a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head), dim=-1) @ v # Recombine heads a = combine_heads(a) # Project attention embeddings back to model space a = outputs(a) return a def normalize_attention(a, *, layer): # Configure attention normalization xform = nn.LayerNorm(normalized_shape=config.d_model, eps=1e-12) # Load pre-trained state xform.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.sa_layer_norm.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.sa_layer_norm.bias&quot;], }) return xform(a) def fnn(y, *, layer): # Configure FNN xform = nn.Sequential( nn.Linear(in_features=config.d_model, out_features=config.d_fnn), nn.GELU(), nn.Linear(in_features=config.d_fnn, out_features=config.d_model), ) # Load pre-trained state xform.load_state_dict({ &quot;0.weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.ffn.lin1.weight&quot;], &quot;0.bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.ffn.lin1.bias&quot;], &quot;2.weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.ffn.lin2.weight&quot;], &quot;2.bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.ffn.lin2.bias&quot;], }) # Transform attention outputs f = xform(y) return f def normalize_fnn(f, *, layer): # Configure attention normalization xform = nn.LayerNorm(normalized_shape=config.d_model, eps=1e-12) # Load pre-trained state xform.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.output_layer_norm.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.output_layer_norm.bias&quot;], }) return xform(f) In [42]: # Initialize loop z_i = x # Apply layer logic in a loop for layer in range(config.n_layers): # Use previous layer&#39;s outputs as inputs x_i = z_i # Attention y_i = normalize_attention(x_i + attention(x_i, layer=layer), layer=layer) # Transform z_i = normalize_fnn(y_i + fnn(y_i, layer=layer), layer=layer) # Save outputs from last layer z = z_i In [43]: z Out[43]: tensor([[ 3.6173e-01, -1.3168e-01, 3.5340e-02, ..., 4.4015e-01, 1.0666e+00, -1.9293e-01], [ 7.3341e-01, 4.9823e-02, -1.7590e-02, ..., 5.0063e-01, 1.1480e+00, -1.2997e-01], [ 1.1230e+00, 2.7603e-01, 3.2096e-01, ..., 1.8820e-01, 1.0586e+00, -1.2496e-01], [ 4.8728e-01, 1.4863e-02, 4.2930e-01, ..., 4.8993e-01, 7.9435e-01, 1.2331e-01], [ 1.0595e-03, -1.4508e-01, 2.8892e-01, ..., 5.5342e-01, 7.9370e-01, -9.0899e-02], [ 1.1021e+00, 8.6115e-02, 5.7461e-01, ..., 6.8800e-01, 5.6345e-01, -6.6278e-01]], grad_fn=&lt;NativeLayerNormBackward0&gt;) In [44]: # Sanity check hiddens = transformer.model.distilbert(input_ids=batch.input_ids.to(device)).last_hidden_state.squeeze().to(&quot;cpu&quot;) assert torch.allclose(z, hiddens, atol=1e-5) Head¶As the final stage in the Transformer pipeline, the Head stage maps the contextualized embeddings to task-specific predictions. In our case, the Head stage is responsible for turning the contextualized embeddings into a binary classifier that predicts whether the original text contains positive or negative sentiments. This sounds like a straightforward output layer until you realize that instead of one set of features, we have a sequence of features. And the length of the sequence is arbitrary. How do you connect an arbitrary length sequence of feature vectors to an output layer? The trick is hiding in our contextualized embeddings. Each input embedding represents a single token in isolation. But the output embeddings have been infused with information from all of the tokens. This is why the common practice is to simply take the first output embedding and drop the rest. The first embedding represents the start of sequence marker [CLS]. Since the [CLS] marker token is added to every sequence, the first input embedding is always the same. In contrast, the first output embedding uniquely represents one specific sequence. If we let the first output embedding represent the entire sequence, then we have a single feature vector that&#39;s easy to connect to any task-specific output layer we need. In [45]: # Use [CLS] embedding to represent the entire sequence features = z[0] features.shape Out[45]: torch.Size([768]) In [46]: # Configure classifier classifier = nn.Sequential( nn.Linear(in_features=config.d_model, out_features=config.d_model), nn.ReLU(), nn.Linear(in_features=config.d_model, out_features=config.n_labels), ) # Load pre-trained state classifier.load_state_dict({ &quot;0.weight&quot;: parameters[&quot;pre_classifier.weight&quot;], &quot;0.bias&quot;: parameters[&quot;pre_classifier.bias&quot;], &quot;2.weight&quot;: parameters[&quot;classifier.weight&quot;], &quot;2.bias&quot;: parameters[&quot;classifier.bias&quot;], }) Out[46]: &lt;All keys matched successfully&gt; In [47]: # Classify features prediction1 = torch.softmax(classifier(features), dim=-1)[1].item() prediction1 Out[47]: 0.9998118281364441 In [48]: # Verify custom results match off-the-shelf ones prediction2 = transformer(&quot;I love ice cream&quot;)[0][&quot;score&quot;] prediction2 Out[48]: 0.9998118281364441 In [49]: assert prediction1 == approx(prediction2) Discussion¶ In [ ]: References¶Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv, May 24, 2019. https://doi.org/10.48550/arXiv.1810.04805. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” arXiv, August 1, 2023. https://doi.org/10.48550/arXiv.1706.03762. Xu, Peng, Xiatian Zhu, and David A. Clifton. “Multimodal Learning with Transformers: A Survey.” arXiv, May 9, 2023. https://doi.org/10.48550/arXiv.2206.06488." />
<meta property="og:description" content="There is something special about Transformers. Since first introduced in 2017, the Transformer architecture has redefined the entire NLP category and is quickly spreading to other types of machine learning. Transformers, and the language embeddings at their core, seem to have hit on some underlying fundamental substrate that transcends boundaries, making them far more generalizable than previous approaches. Anyone who wants to contribute to the future of AI should clearly be studying transformers inside and out. While there are a million papers, blogs, and tutorials written on Transformers, I still find it challenging to map the abstract ideas from the research literature into concrete, actionable steps I can experiment with. My engineer&#39;s brain wants to &quot;see the code&quot; behind high level concepts like embeddings, residuals, and multi-head self-attention. While it&#39;s easy to find open source Transformer implementations, they are often overloaded with configuration settings, conditionalized to support every variation ever imagined, to the point that the main ideas are completely obscured. The goal of this post is to give you a stronger sense for how Transformers work under the hood. We&#39;ll take a single inference—from raw data to final prediction—and break it down into tiny steps, illustrating the main ideas from the Transformer literature with minimal, straightforward, working Python code. You may be surprised by how few steps are required! Setup¶ In [1]: import warnings from matplotlib import pyplot as plt import seaborn as sns import numpy as np from pytest import approx import torch from torch import nn from torch.nn.functional import relu, softmax import transformers from stickshift.models import distilbert In [2]: # Ignore all warnings warnings.filterwarnings(&quot;ignore&quot;) # Configure gpu device = torch.device(&quot;cpu&quot;) if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) elif torch.backends.mps.is_available(): device = torch.device(&quot;mps&quot;) Text Classification with DistilBERT¶If you&#39;ve worked with Transformers at all, I&#39;m sure you&#39;re familiar with Hugging Face&#39;s collection of Python libraries as well as their endless repository of models and datasets. Throughout the post, we&#39;ll be working with Hugging Face&#39;s default text classification model: DistilBERT. DistilBERT is a smaller, faster, lighter-weight version of the original BERT model that&#39;s easier to experiment with. We&#39;ll use the pre-trained model parameters from Hugging Face, but we&#39;ll implement the model&#39;s logic step-by-step in Jupyter using a slightly modified version of the actual DistilBERT PyTorch implementation from Hugging Face&#39;s transformers library. Before we get into the implementation, let&#39;s start by running the entire process end-to-end using Hugging Face&#39;s high level pipeline API. The following cells create a complete text classification pipeline and then apply it to the sentence &quot;I love ice cream&quot;. As you might expect, the model classifies the sentence as overwhelmingly positive. There is a lot happening in very few lines of code here. Over the rest of the post, we&#39;ll break this prediction down and recreate it one step at a time. In [3]: # Create off-the-shelf text classification transformer transformer = transformers.pipeline(&quot;text-classification&quot;, device=device) No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english). Using a pipeline without specifying a model name and revision in production is not recommended. In [4]: transformer(&quot;I love ice cream&quot;) Out[4]: [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9998118281364441}] The code samples throughout this post recreate portions of Hugging Face&#39;s DistilBERT implementation. They&#39;ve been lightly refactored to improve clarity and help illustrate the core ideas of the Transformer architecture. The following cell loads a custom version of the model configuration as well as the pre-trained model parameters. We&#39;ll reference these as we go. In [5]: # Load model config and pre-trained parameters config = distilbert.config(transformer.model) parameters = transformer.model.state_dict() Transformer Pipeline¶The following diagram depicts a Transformer as a multi-stage pipeline. The Context stage at the center of the pipeline is where most of the magic happens. The stages before and after Context provide the extra machinery required to convert raw data into input embeddings and output embeddings into task-specific outputs. While we&#39;ll focus on text data, it&#39;s worth noting that the same stages can be applied to all data modalities including audio and images (Xu et al. 2023). Tokenize¶ The Tokenize stage is responsible for breaking raw data into a sequence of &quot;tokens&quot;. While the word &quot;token&quot; is often associated with text processing, the Transformer literature extends this to other data modalities as well. Examples include patches of an image or segments of an audio recording. In fact, tokenization is seen as a core strength of the Transformer architecture because it allows Transformers to process different types of data using a single, universal approach (Xu et al. 2023). While tokenization is a general concept, the specific algorithms used are modality-specific. In this case, our transformer uses an algorithm known as &quot;word-piece&quot; (Devlin et al. 2019) to split raw text into a sequence of tokens. Next, special tokens are injected to mark the beginning and end of the sequence. Each token is then converted into an integer-encoded categorical value using a fixed token vocabulary, producing the final sequence of &quot;input_ids&quot; that are passed to the next stage. Since our primary interest is in the Transformer layers that come later, we&#39;ll use Hugging Face&#39;s off-the-shelf tokenizer implementation here. In [6]: # Extract tokenizer from transformer tokenizer = transformer.tokenizer In [7]: # Tokenize sentence batch = tokenizer(&quot;I love ice cream&quot;, return_tensors=&quot;pt&quot;) batch Out[7]: {&#39;input_ids&#39;: tensor([[ 101, 1045, 2293, 3256, 6949, 102]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1]])} Tokenizing &quot;I love ice cream&quot; generates the token sequence: [101, 1045, 2293, 3256, 6949, 102]. If we decode the integer-encoded values to see what each one represents, we can see the 4 words are represented by values 1045 to 6949. The values 101 and 102 represent special tokens [CLS] and [SEP] that were added to mark the beginning and end of the sequence respectively. In [8]: [tokenizer.decode(input_id) for input_id in batch.input_ids[0]] Out[8]: [&#39;[CLS]&#39;, &#39;i&#39;, &#39;love&#39;, &#39;ice&#39;, &#39;cream&#39;, &#39;[SEP]&#39;] Embeddings¶ The second stage in the Transformer pipeline converts each of the integer-encoded categorical values into an &quot;embedding&quot;. Embeddings (Bengio et al. 2000) are the fundamental data structure of the Transformer architecture. The Transformer layers we&#39;ll look at in the next stage take embeddings as input, transform them, and produce embeddings as output. Embeddings predated Transformers by almost 2 decades and are a fascinating topic in their own right. But we&#39;ll save the embeddings deep dive for another post. For now, all we need to know is embeddings represent each token as a unique point in an n-dimensional vector space. The vector space coordinates are initialized randomly and then learned during training. Similar to tokenization, the steps required to convert tokens into embeddings depend on the data modality. In text transformers, the Embeddings stage is typically implemented using 2 lookup tables. The first lookup table maps the value of each token to a unique embedding vector. The second lookup table maps the position of each token to a unique embedding vector. The value and position embeddings are then added together to create the initial token embeddings. Let&#39;s start with value embeddings. First, we initialize the value embeddings lookup table. Next, we read the values from the tokenizer output. Finally, we pass the token values to the lookup table to get unique embeddings for each value. In [9]: # Initialize value embeddings lookup table value_embeddings = nn.Embedding( num_embeddings=config.vocab_size, embedding_dim=config.d_model, ) # Load pre-trained state value_embeddings.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.embeddings.word_embeddings.weight&quot;], }) Out[9]: &lt;All keys matched successfully&gt; In [10]: # Calculate token values values = torch.squeeze(batch.input_ids) [tokenizer.decode(input_id) for input_id in values] Out[10]: [&#39;[CLS]&#39;, &#39;i&#39;, &#39;love&#39;, &#39;ice&#39;, &#39;cream&#39;, &#39;[SEP]&#39;] In [11]: # Map token values to embeddings v = value_embeddings(values) v.shape Out[11]: torch.Size([6, 768]) In [12]: # Show sample of value embeddings v Out[12]: tensor([[ 3.9925e-02, -1.0171e-02, -2.0390e-02, ..., 6.1588e-02, 2.1959e-02, 2.2732e-02], [-1.2794e-02, 4.9879e-03, -2.6270e-02, ..., -7.2300e-05, 5.3657e-03, 1.1908e-02], [ 5.9359e-02, -2.3563e-02, -2.0560e-03, ..., -1.0420e-02, 1.4846e-02, -1.2815e-02], [-2.4101e-02, -2.4911e-02, -2.2601e-02, ..., -2.5139e-02, 1.1392e-02, 3.2655e-02], [-8.5466e-02, -5.9276e-02, -5.6659e-02, ..., -1.7192e-02, -8.6179e-02, -4.5105e-02], [-2.1060e-02, -6.4941e-03, -1.0682e-02, ..., -2.3401e-02, 6.1463e-03, -6.4845e-03]], grad_fn=&lt;EmbeddingBackward0&gt;) Next, we&#39;ll follow a similar set of steps for the position embeddings. We&#39;ll start by initializing the position embeddings lookup table. Next, we&#39;ll calculate the positions from the tokenizer output. Finally, we pass the token positions to the lookup table to get unique embeddings for each position. In [13]: # Configure position embeddings lookup table position_embeddings = nn.Embedding( num_embeddings=config.max_sequence_length, embedding_dim=config.d_model, ) # Load pre-trained state position_embeddings.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.embeddings.position_embeddings.weight&quot;], }) Out[13]: &lt;All keys matched successfully&gt; In [14]: # Calculate token positions positions = torch.arange(values.size(0)) positions Out[14]: tensor([0, 1, 2, 3, 4, 5]) In [15]: # Map token positions to embeddings p = position_embeddings(positions) p.shape Out[15]: torch.Size([6, 768]) In [16]: # Show sample of position embeddings p Out[16]: tensor([[ 1.8007e-02, -2.3798e-02, -3.5982e-02, ..., 4.5726e-04, 5.1363e-05, 1.5002e-02], [ 7.8592e-03, 4.8144e-03, -1.6093e-02, ..., 2.9312e-02, 2.7634e-02, -8.5431e-03], [-1.1663e-02, -3.1590e-03, -9.4000e-03, ..., 1.4870e-02, 2.1609e-02, -7.4069e-03], [-4.0848e-03, -1.1123e-02, -2.1704e-02, ..., 1.8962e-02, 4.6763e-03, -1.0220e-03], [-8.2666e-03, -4.1641e-03, -7.5136e-03, ..., 1.9757e-02, -2.2192e-03, 3.8681e-03], [ 4.6293e-04, -1.8499e-02, -1.9709e-02, ..., 5.4042e-03, 1.8076e-02, 2.9490e-03]], grad_fn=&lt;EmbeddingBackward0&gt;) Now that we have value and position embeddings, we add and normalize them to get the final &quot;position-encoded token embeddings&quot;. In [17]: # Configure embeddings normalization normalize_embeddings = nn.LayerNorm( normalized_shape=config.d_model, eps=1e-12, ) # Load pre-trained state normalize_embeddings.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.embeddings.LayerNorm.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.embeddings.LayerNorm.bias&quot;], }) Out[17]: &lt;All keys matched successfully&gt; In [18]: # Add and normalize value and position embeddings x = normalize_embeddings(v + p) x.shape Out[18]: torch.Size([6, 768]) In [19]: # Show sample of token embeddings x Out[19]: tensor([[ 0.3549, -0.1386, -0.2253, ..., 0.1536, 0.0748, 0.1310], [ 0.2282, 0.5511, -0.5092, ..., 0.6421, 0.9541, 0.3192], [ 1.4511, -0.0794, 0.2168, ..., 0.2851, 1.0723, -0.0919], [-0.0564, -0.1761, -0.2870, ..., 0.1442, 0.6767, 1.0396], [-1.1349, -0.5135, -0.4714, ..., 0.3874, -1.0348, -0.2812], [-0.2980, -0.3332, -0.3742, ..., -0.3392, 0.3764, -0.1298]], grad_fn=&lt;NativeLayerNormBackward0&gt;) Congrats! You&#39;ve converted the raw text &quot;I love ice cream&quot; into embeddings that encode both the token values and positions. Context¶ In the previous stage, we mapped the token values and positions to embeddings. But these embeddings represent the tokens in isolation. The Context stage is responsible for infusing each embedding with contextual signals drawn from the entire sequence. At a conceptual level, this should be intuitive. The meaning of the word &quot;ice&quot; changes when you add &quot;cream&quot; after it. The Context stage works by passing the token embeddings through multiple layers of attention and feedforward blocks. The attention blocks focus on relationships between tokens, augmenting each embedding with information drawn from the surrounding embeddings. The feedforward blocks focus on individual tokens, transforming the contextual clues added by attention with the non-linear transformation magic neural networks are famous for. The following diagram illustrates the stack of Transformer layers in the Context stage. The contents of each layer are identical. By arranging the layers in a stack, the model builds context in small increments similar to the hierarchical features in a CNN. The main differences between popular Transformer models such as BERT and GPT come down to how these layers are configured. As illustrated above, given input embeddings $X$, we can define the output embeddings $Z$ as: \begin{align} Y &amp;= Normalize(X + Attention(X)) \\ Z &amp;= Normalize(Y + FNN(Y)) \end{align} Scaled Dot-Product Attention¶The Attention block is the signature component of the Transformer architecture. It&#39;s also one of the most complicated and likely the least familiar when you&#39;re first learning about Transformers. We&#39;ll walk through the core attention algorithm described in the original &quot;All You Need is Attention&quot; paper by Vaswani et al. one step at a time. At the end of the Context section, we&#39;ll put all the pieces together. Vaswani et al. described their attention algorithm as Scaled Dot-Product Attention (SDPA) and defined the standard attention equation everyone cites: \begin{equation} Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_K}})V \end{equation} Queries, Keys, Values¶The $Q$, $K$, and $V$ terms in the SDPA equation are &quot;query&quot;, &quot;key&quot;, and &quot;value&quot; matrices respectively. Each row in $Q$, $K$, and $V$ represents a token embedding that has been projected to distinct representation subspaces. Query embeddings represent selection criteria for the surrounding tokens that would add context to the current token definition. Key embeddings represent characteristics that satisfy the selection criteria. Value embeddings represent the contextual information one token transfers to another. Together, queries, keys, and values allow the attention mechanism to refine the representation of each token based on the surrounding tokens. Given an $n \times d_{model}$ matrix of input embeddings $X$, we can expand on the SDPA equation by defining linear projections $Queries$, $Keys$, and $Values$: \begin{align} SDPA(Q, K, V) &amp;= softmax(\frac{QK^T}{\sqrt{d_K}})V \\ \text{where } Q &amp;= Queries(X) \\ K &amp;= Keys(X) \\ V &amp;= Values(X) \end{align} Enough LaTeX, let&#39;s see what this looks like in Python. In [20]: # Configure query, key, value projections queries = nn.Linear( in_features=config.d_model, out_features=config.d_model, ) keys = nn.Linear( in_features=config.d_model, out_features=config.d_model, ) values = nn.Linear( in_features=config.d_model, out_features=config.d_model, ) # Load pre-trained state queries.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.q_lin.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.q_lin.bias&quot;], }) keys.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.k_lin.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.k_lin.bias&quot;], }) values.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.v_lin.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.k_lin.bias&quot;], }) Out[20]: &lt;All keys matched successfully&gt; In [21]: # Project token embeddings to query, key, and value spaces q = queries(x) k = keys(x) v = values(x) q.shape, k.shape, v.shape Out[21]: (torch.Size([6, 768]), torch.Size([6, 768]), torch.Size([6, 768])) We can see the projections generated unique query, key, and value embeddings for each of the 6 tokens [&#39;[CLS]&#39;, &#39;i&#39;, &#39;love&#39;, &#39;ice&#39;, &#39;cream&#39;, &#39;[SEP]&#39;]. Attention Weights¶ Now that we have $Q$, $K$, and $V$, we can delve into the SDPA equation itself. For each input embedding, SDPA calculates a weighted sum of the value projections for all the tokens in the sequence. We already saw the value projections are represented by V. The weights are represented by the softmax term: \begin{align} softmax(\frac{QK^T}{\sqrt{d_K}}) \end{align} I wouldn&#39;t hold it against you if it&#39;s not immediately obvious what we get here. To see what&#39;s happening, let&#39;s break this down even further. First, the $QK^T$ term calculates a $d_Q \times d_K$ matrix of the dot products of each query embedding with every key embedding. To see why, imagine we have 2 token embeddings of length 3. \begin{align} QK^T &amp;= \begin{bmatrix} q_{00} &amp; q_{01} &amp; q_{02} \\ q_{10} &amp; q_{11} &amp; q_{12} \end{bmatrix} \begin{bmatrix} k_{00} &amp; k_{10} \\ k_{01} &amp; k_{11} \\ k_{02} &amp; k_{12} \end{bmatrix} = \begin{bmatrix} w_{00} &amp; w_{01} \\ w_{10} &amp; w_{11} \end{bmatrix} \\ \text{where } w_{ij} &amp;= row(Q, i) \cdot row(K, j) \end{align} In [22]: # Calculate similarity between Q and K w = q @ k.transpose(-2, -1) w.shape Out[22]: torch.Size([6, 6]) Second, the $1/\sqrt{d_K}$ term scales the dot products down to avoid pushing the softmax function into regions with very small gradients. In [23]: w /= np.sqrt(k.size(1)) w.shape Out[23]: torch.Size([6, 6]) Finally, the softmax function normalizes the weights across the keys. In [24]: # Normalize weights across keys w = softmax(w, dim=-1) w Out[24]: tensor([[9.9964e-01, 8.3108e-06, 5.9084e-06, 1.2478e-05, 1.7158e-05, 3.1488e-04], [9.6971e-01, 3.5093e-04, 3.8457e-03, 1.4637e-04, 2.0632e-04, 2.5743e-02], [9.8888e-01, 1.4836e-03, 3.4143e-04, 3.4613e-04, 1.1235e-03, 7.8293e-03], [7.4676e-01, 3.8455e-05, 1.0549e-03, 8.3005e-05, 2.2695e-01, 2.5122e-02], [7.5151e-01, 9.0717e-06, 2.4871e-04, 2.3733e-01, 1.7790e-05, 1.0880e-02], [8.6867e-01, 4.3978e-04, 4.6904e-05, 7.9754e-05, 1.2606e-03, 1.2950e-01]], grad_fn=&lt;SoftmaxBackward0&gt;) In [25]: # Plot weights for each query _, axs = plt.subplots(nrows=2, ncols=3, figsize=(10,5), gridspec_kw={&quot;wspace&quot;: 0.5, &quot;hspace&quot;: 0.5}) for i in range(6): ax = axs[i//3][i%3] sns.scatterplot(x=np.arange(len(w[i])), y=w[i].detach(), ax=ax) ax.set_title(f&quot;Query {i}&quot;) ax.set_xlabel(f&quot;Keys&quot;) ax.set_ylabel(f&quot;Weight&quot;) Attention Output¶ Now that we have the attention weights, we can apply them to the values. This will give us a weighted sum of contextual information. However, the answer is still in &quot;value space&quot;. Before we combine them with the token embeddings, we&#39;ll project them back to &quot;model space&quot;. In [26]: # Compute weighted combination of values a = w @ v a.shape Out[26]: torch.Size([6, 768]) In [27]: # Configure output projection outputs = nn.Linear(in_features=config.d_model, out_features=config.d_model) # Load pre-trained state outputs.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.out_lin.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.out_lin.bias&quot;], }) Out[27]: &lt;All keys matched successfully&gt; In [28]: # Project attention embeddings back to model space a = outputs(a) a.shape Out[28]: torch.Size([6, 768]) Multi-Head Attention¶At this point, we&#39;ve walked through the core SDPA algorithm step-by-step. However, we&#39;re not quite done. Vaswani et al. realized there are more than one set of relationships involved in transferring context across tokens. A single application of SDPA would effectively average these together. The solution is to apply SDPA multiple times on separate query, key, and value embeddings. Each of these is referred to as an &quot;attention head&quot;. Each head is isolated, leaving it free to learn distinct relational structures. In [29]: def split_heads(x): return x.view(-1, config.n_heads, config.d_head).transpose(-3, -2) def combine_heads(x): return x.transpose(-3, -2).contiguous().view(-1, int(config.n_heads * config.d_head)) In [30]: # Render query, key, value dimensions before we split q.shape, k.shape, v.shape Out[30]: (torch.Size([6, 768]), torch.Size([6, 768]), torch.Size([6, 768])) In [31]: # Split queries, keys, values into separate heads q = split_heads(q) k = split_heads(k) v = split_heads(v) q.shape, k.shape, v.shape Out[31]: (torch.Size([12, 6, 64]), torch.Size([12, 6, 64]), torch.Size([12, 6, 64])) We can see that the queries, keys, and values have been split into 12 heads, leaving each of the original 768-element query, key, and value embeddings is now 64 elements long. Next, let&#39;s recompute the attention embeddings. In [32]: # Compute attention for all heads in parallel a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head), dim=-1) @ v a.shape Out[32]: torch.Size([12, 6, 64]) While the attention code is the same, you can see the attention values are still split into heads. Next, we&#39;ll recombine them before applying the final output projection. In [33]: # Recombine heads a = combine_heads(a) a.shape Out[33]: torch.Size([6, 768]) In [34]: # Project attention embeddings back to model space a = outputs(a) a.shape Out[34]: torch.Size([6, 768]) Add and Normalize¶Before we get to the FNN, we&#39;ll combine the attention embeddings with input embeddings the same way we combined the value and position embeddings. In [35]: # Configure attention normalization normalize_attention = nn.LayerNorm(normalized_shape=config.d_model, eps=1e-12) # Load pre-trained state normalize_attention.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.sa_layer_norm.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.sa_layer_norm.bias&quot;], }) Out[35]: &lt;All keys matched successfully&gt; In [36]: # Combine attention with input embeddings y = normalize_attention(x + a) y.shape Out[36]: torch.Size([6, 768]) FNN¶The FNN block is a straightforward fully connected multi-layer perceptron. In [37]: # Configure FNN fnn = nn.Sequential( nn.Linear(in_features=config.d_model, out_features=config.d_fnn), nn.GELU(), nn.Linear(in_features=config.d_fnn, out_features=config.d_model), ) # Load pre-trained state fnn.load_state_dict({ &quot;0.weight&quot;: parameters[&quot;distilbert.transformer.layer.0.ffn.lin1.weight&quot;], &quot;0.bias&quot;: parameters[&quot;distilbert.transformer.layer.0.ffn.lin1.bias&quot;], &quot;2.weight&quot;: parameters[&quot;distilbert.transformer.layer.0.ffn.lin2.weight&quot;], &quot;2.bias&quot;: parameters[&quot;distilbert.transformer.layer.0.ffn.lin2.bias&quot;], }) Out[37]: &lt;All keys matched successfully&gt; In [38]: # Transform attention outputs f = fnn(y) f.shape Out[38]: torch.Size([6, 768]) Add and Normalize¶Next, we combine the FNN outputs with the attention outputs. In [39]: # Configure attention normalization normalize_fnn = nn.LayerNorm(normalized_shape=config.d_model, eps=1e-12) # Load pre-trained state normalize_fnn.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.output_layer_norm.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.output_layer_norm.bias&quot;], }) Out[39]: &lt;All keys matched successfully&gt; In [40]: z = normalize_fnn(y + f) z.shape Out[40]: torch.Size([6, 768]) Full Stack¶Quick recap. Given input embeddings $X$, we added attention embeddings to get $Y$, and added transformed embeddings to get $Z$. Next, we combine all of these elements together and repeat for each layer in the stack. While you would normally create a stack of torch modules, we run the layers in a loop instead to make it easier to see what&#39;s happening. In [41]: def attention(x, *, layer): # Configure query, key, value, and output projections queries = nn.Linear(in_features=config.d_model, out_features=config.d_model) keys = nn.Linear(in_features=config.d_model, out_features=config.d_model) values = nn.Linear(in_features=config.d_model, out_features=config.d_model) outputs = nn.Linear(in_features=config.d_model, out_features=config.d_model) # Load pre-trained state queries.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.q_lin.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.q_lin.bias&quot;], }) keys.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.k_lin.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.k_lin.bias&quot;], }) values.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.v_lin.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.v_lin.bias&quot;], }) outputs.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.out_lin.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.out_lin.bias&quot;], }) # Project x to query, key, and value spaces q = queries(x) k = keys(x) v = values(x) # Split q, k, v into separate heads q = split_heads(q) k = split_heads(k) v = split_heads(v) # Compute attention for all heads in parallel a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head), dim=-1) @ v # Recombine heads a = combine_heads(a) # Project attention embeddings back to model space a = outputs(a) return a def normalize_attention(a, *, layer): # Configure attention normalization xform = nn.LayerNorm(normalized_shape=config.d_model, eps=1e-12) # Load pre-trained state xform.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.sa_layer_norm.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.sa_layer_norm.bias&quot;], }) return xform(a) def fnn(y, *, layer): # Configure FNN xform = nn.Sequential( nn.Linear(in_features=config.d_model, out_features=config.d_fnn), nn.GELU(), nn.Linear(in_features=config.d_fnn, out_features=config.d_model), ) # Load pre-trained state xform.load_state_dict({ &quot;0.weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.ffn.lin1.weight&quot;], &quot;0.bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.ffn.lin1.bias&quot;], &quot;2.weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.ffn.lin2.weight&quot;], &quot;2.bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.ffn.lin2.bias&quot;], }) # Transform attention outputs f = xform(y) return f def normalize_fnn(f, *, layer): # Configure attention normalization xform = nn.LayerNorm(normalized_shape=config.d_model, eps=1e-12) # Load pre-trained state xform.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.output_layer_norm.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.output_layer_norm.bias&quot;], }) return xform(f) In [42]: # Initialize loop z_i = x # Apply layer logic in a loop for layer in range(config.n_layers): # Use previous layer&#39;s outputs as inputs x_i = z_i # Attention y_i = normalize_attention(x_i + attention(x_i, layer=layer), layer=layer) # Transform z_i = normalize_fnn(y_i + fnn(y_i, layer=layer), layer=layer) # Save outputs from last layer z = z_i In [43]: z Out[43]: tensor([[ 3.6173e-01, -1.3168e-01, 3.5340e-02, ..., 4.4015e-01, 1.0666e+00, -1.9293e-01], [ 7.3341e-01, 4.9823e-02, -1.7590e-02, ..., 5.0063e-01, 1.1480e+00, -1.2997e-01], [ 1.1230e+00, 2.7603e-01, 3.2096e-01, ..., 1.8820e-01, 1.0586e+00, -1.2496e-01], [ 4.8728e-01, 1.4863e-02, 4.2930e-01, ..., 4.8993e-01, 7.9435e-01, 1.2331e-01], [ 1.0595e-03, -1.4508e-01, 2.8892e-01, ..., 5.5342e-01, 7.9370e-01, -9.0899e-02], [ 1.1021e+00, 8.6115e-02, 5.7461e-01, ..., 6.8800e-01, 5.6345e-01, -6.6278e-01]], grad_fn=&lt;NativeLayerNormBackward0&gt;) In [44]: # Sanity check hiddens = transformer.model.distilbert(input_ids=batch.input_ids.to(device)).last_hidden_state.squeeze().to(&quot;cpu&quot;) assert torch.allclose(z, hiddens, atol=1e-5) Head¶As the final stage in the Transformer pipeline, the Head stage maps the contextualized embeddings to task-specific predictions. In our case, the Head stage is responsible for turning the contextualized embeddings into a binary classifier that predicts whether the original text contains positive or negative sentiments. This sounds like a straightforward output layer until you realize that instead of one set of features, we have a sequence of features. And the length of the sequence is arbitrary. How do you connect an arbitrary length sequence of feature vectors to an output layer? The trick is hiding in our contextualized embeddings. Each input embedding represents a single token in isolation. But the output embeddings have been infused with information from all of the tokens. This is why the common practice is to simply take the first output embedding and drop the rest. The first embedding represents the start of sequence marker [CLS]. Since the [CLS] marker token is added to every sequence, the first input embedding is always the same. In contrast, the first output embedding uniquely represents one specific sequence. If we let the first output embedding represent the entire sequence, then we have a single feature vector that&#39;s easy to connect to any task-specific output layer we need. In [45]: # Use [CLS] embedding to represent the entire sequence features = z[0] features.shape Out[45]: torch.Size([768]) In [46]: # Configure classifier classifier = nn.Sequential( nn.Linear(in_features=config.d_model, out_features=config.d_model), nn.ReLU(), nn.Linear(in_features=config.d_model, out_features=config.n_labels), ) # Load pre-trained state classifier.load_state_dict({ &quot;0.weight&quot;: parameters[&quot;pre_classifier.weight&quot;], &quot;0.bias&quot;: parameters[&quot;pre_classifier.bias&quot;], &quot;2.weight&quot;: parameters[&quot;classifier.weight&quot;], &quot;2.bias&quot;: parameters[&quot;classifier.bias&quot;], }) Out[46]: &lt;All keys matched successfully&gt; In [47]: # Classify features prediction1 = torch.softmax(classifier(features), dim=-1)[1].item() prediction1 Out[47]: 0.9998118281364441 In [48]: # Verify custom results match off-the-shelf ones prediction2 = transformer(&quot;I love ice cream&quot;)[0][&quot;score&quot;] prediction2 Out[48]: 0.9998118281364441 In [49]: assert prediction1 == approx(prediction2) Discussion¶ In [ ]: References¶Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv, May 24, 2019. https://doi.org/10.48550/arXiv.1810.04805. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” arXiv, August 1, 2023. https://doi.org/10.48550/arXiv.1706.03762. Xu, Peng, Xiatian Zhu, and David A. Clifton. “Multimodal Learning with Transformers: A Survey.” arXiv, May 9, 2023. https://doi.org/10.48550/arXiv.2206.06488." />
<link rel="canonical" href="http://localhost:4000/2024/09/04/transformer-from-scratch.html" />
<meta property="og:url" content="http://localhost:4000/2024/09/04/transformer-from-scratch.html" />
<meta property="og:site_name" content="Pattern Recognition" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-09-04T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Writing a Transformer One Cell at a Time" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-09-04T00:00:00-04:00","datePublished":"2024-09-04T00:00:00-04:00","description":"There is something special about Transformers. Since first introduced in 2017, the Transformer architecture has redefined the entire NLP category and is quickly spreading to other types of machine learning. Transformers, and the language embeddings at their core, seem to have hit on some underlying fundamental substrate that transcends boundaries, making them far more generalizable than previous approaches. Anyone who wants to contribute to the future of AI should clearly be studying transformers inside and out. While there are a million papers, blogs, and tutorials written on Transformers, I still find it challenging to map the abstract ideas from the research literature into concrete, actionable steps I can experiment with. My engineer&#39;s brain wants to &quot;see the code&quot; behind high level concepts like embeddings, residuals, and multi-head self-attention. While it&#39;s easy to find open source Transformer implementations, they are often overloaded with configuration settings, conditionalized to support every variation ever imagined, to the point that the main ideas are completely obscured. The goal of this post is to give you a stronger sense for how Transformers work under the hood. We&#39;ll take a single inference—from raw data to final prediction—and break it down into tiny steps, illustrating the main ideas from the Transformer literature with minimal, straightforward, working Python code. You may be surprised by how few steps are required! Setup¶ In [1]: import warnings from matplotlib import pyplot as plt import seaborn as sns import numpy as np from pytest import approx import torch from torch import nn from torch.nn.functional import relu, softmax import transformers from stickshift.models import distilbert In [2]: # Ignore all warnings warnings.filterwarnings(&quot;ignore&quot;) # Configure gpu device = torch.device(&quot;cpu&quot;) if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) elif torch.backends.mps.is_available(): device = torch.device(&quot;mps&quot;) Text Classification with DistilBERT¶If you&#39;ve worked with Transformers at all, I&#39;m sure you&#39;re familiar with Hugging Face&#39;s collection of Python libraries as well as their endless repository of models and datasets. Throughout the post, we&#39;ll be working with Hugging Face&#39;s default text classification model: DistilBERT. DistilBERT is a smaller, faster, lighter-weight version of the original BERT model that&#39;s easier to experiment with. We&#39;ll use the pre-trained model parameters from Hugging Face, but we&#39;ll implement the model&#39;s logic step-by-step in Jupyter using a slightly modified version of the actual DistilBERT PyTorch implementation from Hugging Face&#39;s transformers library. Before we get into the implementation, let&#39;s start by running the entire process end-to-end using Hugging Face&#39;s high level pipeline API. The following cells create a complete text classification pipeline and then apply it to the sentence &quot;I love ice cream&quot;. As you might expect, the model classifies the sentence as overwhelmingly positive. There is a lot happening in very few lines of code here. Over the rest of the post, we&#39;ll break this prediction down and recreate it one step at a time. In [3]: # Create off-the-shelf text classification transformer transformer = transformers.pipeline(&quot;text-classification&quot;, device=device) No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english). Using a pipeline without specifying a model name and revision in production is not recommended. In [4]: transformer(&quot;I love ice cream&quot;) Out[4]: [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9998118281364441}] The code samples throughout this post recreate portions of Hugging Face&#39;s DistilBERT implementation. They&#39;ve been lightly refactored to improve clarity and help illustrate the core ideas of the Transformer architecture. The following cell loads a custom version of the model configuration as well as the pre-trained model parameters. We&#39;ll reference these as we go. In [5]: # Load model config and pre-trained parameters config = distilbert.config(transformer.model) parameters = transformer.model.state_dict() Transformer Pipeline¶The following diagram depicts a Transformer as a multi-stage pipeline. The Context stage at the center of the pipeline is where most of the magic happens. The stages before and after Context provide the extra machinery required to convert raw data into input embeddings and output embeddings into task-specific outputs. While we&#39;ll focus on text data, it&#39;s worth noting that the same stages can be applied to all data modalities including audio and images (Xu et al. 2023). Tokenize¶ The Tokenize stage is responsible for breaking raw data into a sequence of &quot;tokens&quot;. While the word &quot;token&quot; is often associated with text processing, the Transformer literature extends this to other data modalities as well. Examples include patches of an image or segments of an audio recording. In fact, tokenization is seen as a core strength of the Transformer architecture because it allows Transformers to process different types of data using a single, universal approach (Xu et al. 2023). While tokenization is a general concept, the specific algorithms used are modality-specific. In this case, our transformer uses an algorithm known as &quot;word-piece&quot; (Devlin et al. 2019) to split raw text into a sequence of tokens. Next, special tokens are injected to mark the beginning and end of the sequence. Each token is then converted into an integer-encoded categorical value using a fixed token vocabulary, producing the final sequence of &quot;input_ids&quot; that are passed to the next stage. Since our primary interest is in the Transformer layers that come later, we&#39;ll use Hugging Face&#39;s off-the-shelf tokenizer implementation here. In [6]: # Extract tokenizer from transformer tokenizer = transformer.tokenizer In [7]: # Tokenize sentence batch = tokenizer(&quot;I love ice cream&quot;, return_tensors=&quot;pt&quot;) batch Out[7]: {&#39;input_ids&#39;: tensor([[ 101, 1045, 2293, 3256, 6949, 102]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1]])} Tokenizing &quot;I love ice cream&quot; generates the token sequence: [101, 1045, 2293, 3256, 6949, 102]. If we decode the integer-encoded values to see what each one represents, we can see the 4 words are represented by values 1045 to 6949. The values 101 and 102 represent special tokens [CLS] and [SEP] that were added to mark the beginning and end of the sequence respectively. In [8]: [tokenizer.decode(input_id) for input_id in batch.input_ids[0]] Out[8]: [&#39;[CLS]&#39;, &#39;i&#39;, &#39;love&#39;, &#39;ice&#39;, &#39;cream&#39;, &#39;[SEP]&#39;] Embeddings¶ The second stage in the Transformer pipeline converts each of the integer-encoded categorical values into an &quot;embedding&quot;. Embeddings (Bengio et al. 2000) are the fundamental data structure of the Transformer architecture. The Transformer layers we&#39;ll look at in the next stage take embeddings as input, transform them, and produce embeddings as output. Embeddings predated Transformers by almost 2 decades and are a fascinating topic in their own right. But we&#39;ll save the embeddings deep dive for another post. For now, all we need to know is embeddings represent each token as a unique point in an n-dimensional vector space. The vector space coordinates are initialized randomly and then learned during training. Similar to tokenization, the steps required to convert tokens into embeddings depend on the data modality. In text transformers, the Embeddings stage is typically implemented using 2 lookup tables. The first lookup table maps the value of each token to a unique embedding vector. The second lookup table maps the position of each token to a unique embedding vector. The value and position embeddings are then added together to create the initial token embeddings. Let&#39;s start with value embeddings. First, we initialize the value embeddings lookup table. Next, we read the values from the tokenizer output. Finally, we pass the token values to the lookup table to get unique embeddings for each value. In [9]: # Initialize value embeddings lookup table value_embeddings = nn.Embedding( num_embeddings=config.vocab_size, embedding_dim=config.d_model, ) # Load pre-trained state value_embeddings.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.embeddings.word_embeddings.weight&quot;], }) Out[9]: &lt;All keys matched successfully&gt; In [10]: # Calculate token values values = torch.squeeze(batch.input_ids) [tokenizer.decode(input_id) for input_id in values] Out[10]: [&#39;[CLS]&#39;, &#39;i&#39;, &#39;love&#39;, &#39;ice&#39;, &#39;cream&#39;, &#39;[SEP]&#39;] In [11]: # Map token values to embeddings v = value_embeddings(values) v.shape Out[11]: torch.Size([6, 768]) In [12]: # Show sample of value embeddings v Out[12]: tensor([[ 3.9925e-02, -1.0171e-02, -2.0390e-02, ..., 6.1588e-02, 2.1959e-02, 2.2732e-02], [-1.2794e-02, 4.9879e-03, -2.6270e-02, ..., -7.2300e-05, 5.3657e-03, 1.1908e-02], [ 5.9359e-02, -2.3563e-02, -2.0560e-03, ..., -1.0420e-02, 1.4846e-02, -1.2815e-02], [-2.4101e-02, -2.4911e-02, -2.2601e-02, ..., -2.5139e-02, 1.1392e-02, 3.2655e-02], [-8.5466e-02, -5.9276e-02, -5.6659e-02, ..., -1.7192e-02, -8.6179e-02, -4.5105e-02], [-2.1060e-02, -6.4941e-03, -1.0682e-02, ..., -2.3401e-02, 6.1463e-03, -6.4845e-03]], grad_fn=&lt;EmbeddingBackward0&gt;) Next, we&#39;ll follow a similar set of steps for the position embeddings. We&#39;ll start by initializing the position embeddings lookup table. Next, we&#39;ll calculate the positions from the tokenizer output. Finally, we pass the token positions to the lookup table to get unique embeddings for each position. In [13]: # Configure position embeddings lookup table position_embeddings = nn.Embedding( num_embeddings=config.max_sequence_length, embedding_dim=config.d_model, ) # Load pre-trained state position_embeddings.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.embeddings.position_embeddings.weight&quot;], }) Out[13]: &lt;All keys matched successfully&gt; In [14]: # Calculate token positions positions = torch.arange(values.size(0)) positions Out[14]: tensor([0, 1, 2, 3, 4, 5]) In [15]: # Map token positions to embeddings p = position_embeddings(positions) p.shape Out[15]: torch.Size([6, 768]) In [16]: # Show sample of position embeddings p Out[16]: tensor([[ 1.8007e-02, -2.3798e-02, -3.5982e-02, ..., 4.5726e-04, 5.1363e-05, 1.5002e-02], [ 7.8592e-03, 4.8144e-03, -1.6093e-02, ..., 2.9312e-02, 2.7634e-02, -8.5431e-03], [-1.1663e-02, -3.1590e-03, -9.4000e-03, ..., 1.4870e-02, 2.1609e-02, -7.4069e-03], [-4.0848e-03, -1.1123e-02, -2.1704e-02, ..., 1.8962e-02, 4.6763e-03, -1.0220e-03], [-8.2666e-03, -4.1641e-03, -7.5136e-03, ..., 1.9757e-02, -2.2192e-03, 3.8681e-03], [ 4.6293e-04, -1.8499e-02, -1.9709e-02, ..., 5.4042e-03, 1.8076e-02, 2.9490e-03]], grad_fn=&lt;EmbeddingBackward0&gt;) Now that we have value and position embeddings, we add and normalize them to get the final &quot;position-encoded token embeddings&quot;. In [17]: # Configure embeddings normalization normalize_embeddings = nn.LayerNorm( normalized_shape=config.d_model, eps=1e-12, ) # Load pre-trained state normalize_embeddings.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.embeddings.LayerNorm.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.embeddings.LayerNorm.bias&quot;], }) Out[17]: &lt;All keys matched successfully&gt; In [18]: # Add and normalize value and position embeddings x = normalize_embeddings(v + p) x.shape Out[18]: torch.Size([6, 768]) In [19]: # Show sample of token embeddings x Out[19]: tensor([[ 0.3549, -0.1386, -0.2253, ..., 0.1536, 0.0748, 0.1310], [ 0.2282, 0.5511, -0.5092, ..., 0.6421, 0.9541, 0.3192], [ 1.4511, -0.0794, 0.2168, ..., 0.2851, 1.0723, -0.0919], [-0.0564, -0.1761, -0.2870, ..., 0.1442, 0.6767, 1.0396], [-1.1349, -0.5135, -0.4714, ..., 0.3874, -1.0348, -0.2812], [-0.2980, -0.3332, -0.3742, ..., -0.3392, 0.3764, -0.1298]], grad_fn=&lt;NativeLayerNormBackward0&gt;) Congrats! You&#39;ve converted the raw text &quot;I love ice cream&quot; into embeddings that encode both the token values and positions. Context¶ In the previous stage, we mapped the token values and positions to embeddings. But these embeddings represent the tokens in isolation. The Context stage is responsible for infusing each embedding with contextual signals drawn from the entire sequence. At a conceptual level, this should be intuitive. The meaning of the word &quot;ice&quot; changes when you add &quot;cream&quot; after it. The Context stage works by passing the token embeddings through multiple layers of attention and feedforward blocks. The attention blocks focus on relationships between tokens, augmenting each embedding with information drawn from the surrounding embeddings. The feedforward blocks focus on individual tokens, transforming the contextual clues added by attention with the non-linear transformation magic neural networks are famous for. The following diagram illustrates the stack of Transformer layers in the Context stage. The contents of each layer are identical. By arranging the layers in a stack, the model builds context in small increments similar to the hierarchical features in a CNN. The main differences between popular Transformer models such as BERT and GPT come down to how these layers are configured. As illustrated above, given input embeddings $X$, we can define the output embeddings $Z$ as: \\begin{align} Y &amp;= Normalize(X + Attention(X)) \\\\ Z &amp;= Normalize(Y + FNN(Y)) \\end{align} Scaled Dot-Product Attention¶The Attention block is the signature component of the Transformer architecture. It&#39;s also one of the most complicated and likely the least familiar when you&#39;re first learning about Transformers. We&#39;ll walk through the core attention algorithm described in the original &quot;All You Need is Attention&quot; paper by Vaswani et al. one step at a time. At the end of the Context section, we&#39;ll put all the pieces together. Vaswani et al. described their attention algorithm as Scaled Dot-Product Attention (SDPA) and defined the standard attention equation everyone cites: \\begin{equation} Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_K}})V \\end{equation} Queries, Keys, Values¶The $Q$, $K$, and $V$ terms in the SDPA equation are &quot;query&quot;, &quot;key&quot;, and &quot;value&quot; matrices respectively. Each row in $Q$, $K$, and $V$ represents a token embedding that has been projected to distinct representation subspaces. Query embeddings represent selection criteria for the surrounding tokens that would add context to the current token definition. Key embeddings represent characteristics that satisfy the selection criteria. Value embeddings represent the contextual information one token transfers to another. Together, queries, keys, and values allow the attention mechanism to refine the representation of each token based on the surrounding tokens. Given an $n \\times d_{model}$ matrix of input embeddings $X$, we can expand on the SDPA equation by defining linear projections $Queries$, $Keys$, and $Values$: \\begin{align} SDPA(Q, K, V) &amp;= softmax(\\frac{QK^T}{\\sqrt{d_K}})V \\\\ \\text{where } Q &amp;= Queries(X) \\\\ K &amp;= Keys(X) \\\\ V &amp;= Values(X) \\end{align} Enough LaTeX, let&#39;s see what this looks like in Python. In [20]: # Configure query, key, value projections queries = nn.Linear( in_features=config.d_model, out_features=config.d_model, ) keys = nn.Linear( in_features=config.d_model, out_features=config.d_model, ) values = nn.Linear( in_features=config.d_model, out_features=config.d_model, ) # Load pre-trained state queries.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.q_lin.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.q_lin.bias&quot;], }) keys.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.k_lin.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.k_lin.bias&quot;], }) values.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.v_lin.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.k_lin.bias&quot;], }) Out[20]: &lt;All keys matched successfully&gt; In [21]: # Project token embeddings to query, key, and value spaces q = queries(x) k = keys(x) v = values(x) q.shape, k.shape, v.shape Out[21]: (torch.Size([6, 768]), torch.Size([6, 768]), torch.Size([6, 768])) We can see the projections generated unique query, key, and value embeddings for each of the 6 tokens [&#39;[CLS]&#39;, &#39;i&#39;, &#39;love&#39;, &#39;ice&#39;, &#39;cream&#39;, &#39;[SEP]&#39;]. Attention Weights¶ Now that we have $Q$, $K$, and $V$, we can delve into the SDPA equation itself. For each input embedding, SDPA calculates a weighted sum of the value projections for all the tokens in the sequence. We already saw the value projections are represented by V. The weights are represented by the softmax term: \\begin{align} softmax(\\frac{QK^T}{\\sqrt{d_K}}) \\end{align} I wouldn&#39;t hold it against you if it&#39;s not immediately obvious what we get here. To see what&#39;s happening, let&#39;s break this down even further. First, the $QK^T$ term calculates a $d_Q \\times d_K$ matrix of the dot products of each query embedding with every key embedding. To see why, imagine we have 2 token embeddings of length 3. \\begin{align} QK^T &amp;= \\begin{bmatrix} q_{00} &amp; q_{01} &amp; q_{02} \\\\ q_{10} &amp; q_{11} &amp; q_{12} \\end{bmatrix} \\begin{bmatrix} k_{00} &amp; k_{10} \\\\ k_{01} &amp; k_{11} \\\\ k_{02} &amp; k_{12} \\end{bmatrix} = \\begin{bmatrix} w_{00} &amp; w_{01} \\\\ w_{10} &amp; w_{11} \\end{bmatrix} \\\\ \\text{where } w_{ij} &amp;= row(Q, i) \\cdot row(K, j) \\end{align} In [22]: # Calculate similarity between Q and K w = q @ k.transpose(-2, -1) w.shape Out[22]: torch.Size([6, 6]) Second, the $1/\\sqrt{d_K}$ term scales the dot products down to avoid pushing the softmax function into regions with very small gradients. In [23]: w /= np.sqrt(k.size(1)) w.shape Out[23]: torch.Size([6, 6]) Finally, the softmax function normalizes the weights across the keys. In [24]: # Normalize weights across keys w = softmax(w, dim=-1) w Out[24]: tensor([[9.9964e-01, 8.3108e-06, 5.9084e-06, 1.2478e-05, 1.7158e-05, 3.1488e-04], [9.6971e-01, 3.5093e-04, 3.8457e-03, 1.4637e-04, 2.0632e-04, 2.5743e-02], [9.8888e-01, 1.4836e-03, 3.4143e-04, 3.4613e-04, 1.1235e-03, 7.8293e-03], [7.4676e-01, 3.8455e-05, 1.0549e-03, 8.3005e-05, 2.2695e-01, 2.5122e-02], [7.5151e-01, 9.0717e-06, 2.4871e-04, 2.3733e-01, 1.7790e-05, 1.0880e-02], [8.6867e-01, 4.3978e-04, 4.6904e-05, 7.9754e-05, 1.2606e-03, 1.2950e-01]], grad_fn=&lt;SoftmaxBackward0&gt;) In [25]: # Plot weights for each query _, axs = plt.subplots(nrows=2, ncols=3, figsize=(10,5), gridspec_kw={&quot;wspace&quot;: 0.5, &quot;hspace&quot;: 0.5}) for i in range(6): ax = axs[i//3][i%3] sns.scatterplot(x=np.arange(len(w[i])), y=w[i].detach(), ax=ax) ax.set_title(f&quot;Query {i}&quot;) ax.set_xlabel(f&quot;Keys&quot;) ax.set_ylabel(f&quot;Weight&quot;) Attention Output¶ Now that we have the attention weights, we can apply them to the values. This will give us a weighted sum of contextual information. However, the answer is still in &quot;value space&quot;. Before we combine them with the token embeddings, we&#39;ll project them back to &quot;model space&quot;. In [26]: # Compute weighted combination of values a = w @ v a.shape Out[26]: torch.Size([6, 768]) In [27]: # Configure output projection outputs = nn.Linear(in_features=config.d_model, out_features=config.d_model) # Load pre-trained state outputs.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.out_lin.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.attention.out_lin.bias&quot;], }) Out[27]: &lt;All keys matched successfully&gt; In [28]: # Project attention embeddings back to model space a = outputs(a) a.shape Out[28]: torch.Size([6, 768]) Multi-Head Attention¶At this point, we&#39;ve walked through the core SDPA algorithm step-by-step. However, we&#39;re not quite done. Vaswani et al. realized there are more than one set of relationships involved in transferring context across tokens. A single application of SDPA would effectively average these together. The solution is to apply SDPA multiple times on separate query, key, and value embeddings. Each of these is referred to as an &quot;attention head&quot;. Each head is isolated, leaving it free to learn distinct relational structures. In [29]: def split_heads(x): return x.view(-1, config.n_heads, config.d_head).transpose(-3, -2) def combine_heads(x): return x.transpose(-3, -2).contiguous().view(-1, int(config.n_heads * config.d_head)) In [30]: # Render query, key, value dimensions before we split q.shape, k.shape, v.shape Out[30]: (torch.Size([6, 768]), torch.Size([6, 768]), torch.Size([6, 768])) In [31]: # Split queries, keys, values into separate heads q = split_heads(q) k = split_heads(k) v = split_heads(v) q.shape, k.shape, v.shape Out[31]: (torch.Size([12, 6, 64]), torch.Size([12, 6, 64]), torch.Size([12, 6, 64])) We can see that the queries, keys, and values have been split into 12 heads, leaving each of the original 768-element query, key, and value embeddings is now 64 elements long. Next, let&#39;s recompute the attention embeddings. In [32]: # Compute attention for all heads in parallel a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head), dim=-1) @ v a.shape Out[32]: torch.Size([12, 6, 64]) While the attention code is the same, you can see the attention values are still split into heads. Next, we&#39;ll recombine them before applying the final output projection. In [33]: # Recombine heads a = combine_heads(a) a.shape Out[33]: torch.Size([6, 768]) In [34]: # Project attention embeddings back to model space a = outputs(a) a.shape Out[34]: torch.Size([6, 768]) Add and Normalize¶Before we get to the FNN, we&#39;ll combine the attention embeddings with input embeddings the same way we combined the value and position embeddings. In [35]: # Configure attention normalization normalize_attention = nn.LayerNorm(normalized_shape=config.d_model, eps=1e-12) # Load pre-trained state normalize_attention.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.sa_layer_norm.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.sa_layer_norm.bias&quot;], }) Out[35]: &lt;All keys matched successfully&gt; In [36]: # Combine attention with input embeddings y = normalize_attention(x + a) y.shape Out[36]: torch.Size([6, 768]) FNN¶The FNN block is a straightforward fully connected multi-layer perceptron. In [37]: # Configure FNN fnn = nn.Sequential( nn.Linear(in_features=config.d_model, out_features=config.d_fnn), nn.GELU(), nn.Linear(in_features=config.d_fnn, out_features=config.d_model), ) # Load pre-trained state fnn.load_state_dict({ &quot;0.weight&quot;: parameters[&quot;distilbert.transformer.layer.0.ffn.lin1.weight&quot;], &quot;0.bias&quot;: parameters[&quot;distilbert.transformer.layer.0.ffn.lin1.bias&quot;], &quot;2.weight&quot;: parameters[&quot;distilbert.transformer.layer.0.ffn.lin2.weight&quot;], &quot;2.bias&quot;: parameters[&quot;distilbert.transformer.layer.0.ffn.lin2.bias&quot;], }) Out[37]: &lt;All keys matched successfully&gt; In [38]: # Transform attention outputs f = fnn(y) f.shape Out[38]: torch.Size([6, 768]) Add and Normalize¶Next, we combine the FNN outputs with the attention outputs. In [39]: # Configure attention normalization normalize_fnn = nn.LayerNorm(normalized_shape=config.d_model, eps=1e-12) # Load pre-trained state normalize_fnn.load_state_dict({ &quot;weight&quot;: parameters[&quot;distilbert.transformer.layer.0.output_layer_norm.weight&quot;], &quot;bias&quot;: parameters[&quot;distilbert.transformer.layer.0.output_layer_norm.bias&quot;], }) Out[39]: &lt;All keys matched successfully&gt; In [40]: z = normalize_fnn(y + f) z.shape Out[40]: torch.Size([6, 768]) Full Stack¶Quick recap. Given input embeddings $X$, we added attention embeddings to get $Y$, and added transformed embeddings to get $Z$. Next, we combine all of these elements together and repeat for each layer in the stack. While you would normally create a stack of torch modules, we run the layers in a loop instead to make it easier to see what&#39;s happening. In [41]: def attention(x, *, layer): # Configure query, key, value, and output projections queries = nn.Linear(in_features=config.d_model, out_features=config.d_model) keys = nn.Linear(in_features=config.d_model, out_features=config.d_model) values = nn.Linear(in_features=config.d_model, out_features=config.d_model) outputs = nn.Linear(in_features=config.d_model, out_features=config.d_model) # Load pre-trained state queries.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.q_lin.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.q_lin.bias&quot;], }) keys.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.k_lin.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.k_lin.bias&quot;], }) values.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.v_lin.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.v_lin.bias&quot;], }) outputs.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.out_lin.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.attention.out_lin.bias&quot;], }) # Project x to query, key, and value spaces q = queries(x) k = keys(x) v = values(x) # Split q, k, v into separate heads q = split_heads(q) k = split_heads(k) v = split_heads(v) # Compute attention for all heads in parallel a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head), dim=-1) @ v # Recombine heads a = combine_heads(a) # Project attention embeddings back to model space a = outputs(a) return a def normalize_attention(a, *, layer): # Configure attention normalization xform = nn.LayerNorm(normalized_shape=config.d_model, eps=1e-12) # Load pre-trained state xform.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.sa_layer_norm.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.sa_layer_norm.bias&quot;], }) return xform(a) def fnn(y, *, layer): # Configure FNN xform = nn.Sequential( nn.Linear(in_features=config.d_model, out_features=config.d_fnn), nn.GELU(), nn.Linear(in_features=config.d_fnn, out_features=config.d_model), ) # Load pre-trained state xform.load_state_dict({ &quot;0.weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.ffn.lin1.weight&quot;], &quot;0.bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.ffn.lin1.bias&quot;], &quot;2.weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.ffn.lin2.weight&quot;], &quot;2.bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.ffn.lin2.bias&quot;], }) # Transform attention outputs f = xform(y) return f def normalize_fnn(f, *, layer): # Configure attention normalization xform = nn.LayerNorm(normalized_shape=config.d_model, eps=1e-12) # Load pre-trained state xform.load_state_dict({ &quot;weight&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.output_layer_norm.weight&quot;], &quot;bias&quot;: parameters[f&quot;distilbert.transformer.layer.{layer}.output_layer_norm.bias&quot;], }) return xform(f) In [42]: # Initialize loop z_i = x # Apply layer logic in a loop for layer in range(config.n_layers): # Use previous layer&#39;s outputs as inputs x_i = z_i # Attention y_i = normalize_attention(x_i + attention(x_i, layer=layer), layer=layer) # Transform z_i = normalize_fnn(y_i + fnn(y_i, layer=layer), layer=layer) # Save outputs from last layer z = z_i In [43]: z Out[43]: tensor([[ 3.6173e-01, -1.3168e-01, 3.5340e-02, ..., 4.4015e-01, 1.0666e+00, -1.9293e-01], [ 7.3341e-01, 4.9823e-02, -1.7590e-02, ..., 5.0063e-01, 1.1480e+00, -1.2997e-01], [ 1.1230e+00, 2.7603e-01, 3.2096e-01, ..., 1.8820e-01, 1.0586e+00, -1.2496e-01], [ 4.8728e-01, 1.4863e-02, 4.2930e-01, ..., 4.8993e-01, 7.9435e-01, 1.2331e-01], [ 1.0595e-03, -1.4508e-01, 2.8892e-01, ..., 5.5342e-01, 7.9370e-01, -9.0899e-02], [ 1.1021e+00, 8.6115e-02, 5.7461e-01, ..., 6.8800e-01, 5.6345e-01, -6.6278e-01]], grad_fn=&lt;NativeLayerNormBackward0&gt;) In [44]: # Sanity check hiddens = transformer.model.distilbert(input_ids=batch.input_ids.to(device)).last_hidden_state.squeeze().to(&quot;cpu&quot;) assert torch.allclose(z, hiddens, atol=1e-5) Head¶As the final stage in the Transformer pipeline, the Head stage maps the contextualized embeddings to task-specific predictions. In our case, the Head stage is responsible for turning the contextualized embeddings into a binary classifier that predicts whether the original text contains positive or negative sentiments. This sounds like a straightforward output layer until you realize that instead of one set of features, we have a sequence of features. And the length of the sequence is arbitrary. How do you connect an arbitrary length sequence of feature vectors to an output layer? The trick is hiding in our contextualized embeddings. Each input embedding represents a single token in isolation. But the output embeddings have been infused with information from all of the tokens. This is why the common practice is to simply take the first output embedding and drop the rest. The first embedding represents the start of sequence marker [CLS]. Since the [CLS] marker token is added to every sequence, the first input embedding is always the same. In contrast, the first output embedding uniquely represents one specific sequence. If we let the first output embedding represent the entire sequence, then we have a single feature vector that&#39;s easy to connect to any task-specific output layer we need. In [45]: # Use [CLS] embedding to represent the entire sequence features = z[0] features.shape Out[45]: torch.Size([768]) In [46]: # Configure classifier classifier = nn.Sequential( nn.Linear(in_features=config.d_model, out_features=config.d_model), nn.ReLU(), nn.Linear(in_features=config.d_model, out_features=config.n_labels), ) # Load pre-trained state classifier.load_state_dict({ &quot;0.weight&quot;: parameters[&quot;pre_classifier.weight&quot;], &quot;0.bias&quot;: parameters[&quot;pre_classifier.bias&quot;], &quot;2.weight&quot;: parameters[&quot;classifier.weight&quot;], &quot;2.bias&quot;: parameters[&quot;classifier.bias&quot;], }) Out[46]: &lt;All keys matched successfully&gt; In [47]: # Classify features prediction1 = torch.softmax(classifier(features), dim=-1)[1].item() prediction1 Out[47]: 0.9998118281364441 In [48]: # Verify custom results match off-the-shelf ones prediction2 = transformer(&quot;I love ice cream&quot;)[0][&quot;score&quot;] prediction2 Out[48]: 0.9998118281364441 In [49]: assert prediction1 == approx(prediction2) Discussion¶ In [ ]: References¶Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv, May 24, 2019. https://doi.org/10.48550/arXiv.1810.04805. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” arXiv, August 1, 2023. https://doi.org/10.48550/arXiv.1706.03762. Xu, Peng, Xiatian Zhu, and David A. Clifton. “Multimodal Learning with Transformers: A Survey.” arXiv, May 9, 2023. https://doi.org/10.48550/arXiv.2206.06488.","headline":"Writing a Transformer One Cell at a Time","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2024/09/04/transformer-from-scratch.html"},"url":"http://localhost:4000/2024/09/04/transformer-from-scratch.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
  <link rel="stylesheet" href="/assets/css/stickshift.css">
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Pattern Recognition</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Writing a Transformer One Cell at a Time</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-09-04T00:00:00-04:00" itemprop="datePublished">Sep 4, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <div class="cell border-box-sizing text_cell rendered" id="cell-id=893c043a-54c5-4983-addb-769769139220"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There is something special about Transformers. Since first introduced in 2017, the Transformer architecture has redefined the entire NLP category and is quickly spreading to other types of machine learning. Transformers, and the language embeddings at their core, seem to have hit on some underlying fundamental substrate that transcends boundaries, making them far more generalizable than previous approaches. Anyone who wants to contribute to the future of AI should clearly be studying transformers inside and out.</p>
<p>While there are a million papers, blogs, and tutorials written on Transformers, I still find it challenging to map the abstract ideas from the research literature into concrete, actionable steps I can experiment with. My engineer's brain wants to "see the code" behind high level concepts like embeddings, residuals, and multi-head self-attention. While it's easy to find open source Transformer implementations, they are often overloaded with configuration settings, conditionalized to support every variation ever imagined, to the point that the main ideas are completely obscured.</p>
<p>The goal of this post is to give you a stronger sense for how Transformers work under the hood. We'll take a single inference—from raw data to final prediction—and break it down into tiny steps, illustrating the main ideas from the Transformer literature with minimal, straightforward, working Python code. You may be surprised by how few steps are required!</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=973f5496-2f6a-46b5-bd9b-f874e68ac4f7"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Setup">Setup<a class="anchor-link" href="#Setup">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=f4b52cfc-e02e-4b70-aecf-c0962354fee8">
<div class="input">
<div class="prompt input_prompt">In [1]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pytest</span> <span class="kn">import</span> <span class="n">approx</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="kn">import</span> <span class="n">relu</span><span class="p">,</span> <span class="n">softmax</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="kn">from</span> <span class="nn">stickshift.models</span> <span class="kn">import</span> <span class="n">distilbert</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=5f68eaaf-2117-48d2-8623-f90d5a035452">
<div class="input">
<div class="prompt input_prompt">In [2]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Ignore all warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">"ignore"</span><span class="p">)</span>

<span class="c1"># Configure gpu</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"mps"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=ae509a70-4087-47a7-b587-3aaef0ab9b71"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Text-Classification-with-DistilBERT">Text Classification with DistilBERT<a class="anchor-link" href="#Text-Classification-with-DistilBERT">¶</a></h2><p>If you've worked with Transformers at all, I'm sure you're familiar with Hugging Face's collection of Python libraries as well as their endless repository of models and datasets. Throughout the post, we'll be working with Hugging Face's default text classification model: DistilBERT. DistilBERT is a smaller, faster, lighter-weight version of the original BERT model that's easier to experiment with. We'll use the pre-trained model parameters from Hugging Face, but we'll implement the model's logic step-by-step in Jupyter using a slightly modified version of the actual DistilBERT PyTorch implementation from Hugging Face's <code>transformers</code> library.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=18a45264-ad56-4656-bdc2-bcc55836ebd4"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Before we get into the implementation, let's start by running the entire process end-to-end using Hugging Face's high level <code>pipeline</code> API. The following cells create a complete text classification pipeline and then apply it to the sentence "I love ice cream". As you might expect, the model classifies the sentence as overwhelmingly positive. There is a lot happening in very few lines of code here. Over the rest of the post, we'll break this prediction down and recreate it one step at a time.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=14efe2e6-d089-4abb-8726-5fe4edb8d451">
<div class="input">
<div class="prompt input_prompt">In [3]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Create off-the-shelf text classification transformer</span>
<span class="n">transformer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-classification"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in production is not recommended.
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=78cc7338-7f84-45fa-a659-5f3bd8cc2169">
<div class="input">
<div class="prompt input_prompt">In [4]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">transformer</span><span class="p">(</span><span class="s2">"I love ice cream"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[4]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>[{'label': 'POSITIVE', 'score': 0.9998118281364441}]</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=93ac32b3-6d87-4887-bf69-57e6e3a72b7a"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The code samples throughout this post recreate portions of Hugging Face's DistilBERT implementation. They've been lightly refactored to improve clarity and help illustrate the core ideas of the Transformer architecture. The following cell loads a custom version of the model configuration as well as the pre-trained model parameters. We'll reference these as we go.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=fc2126e9-eec2-458a-9215-b6fb927d7e83">
<div class="input">
<div class="prompt input_prompt">In [5]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Load model config and pre-trained parameters</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">distilbert</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=c85b4579-fb46-481c-8c62-abf66dd022b2"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Transformer-Pipeline">Transformer Pipeline<a class="anchor-link" href="#Transformer-Pipeline">¶</a></h2><p>The following diagram depicts a Transformer as a multi-stage pipeline. The Context stage at the center of the pipeline is where most of the magic happens. The stages before and after Context provide the extra machinery required to convert raw data into input embeddings and output embeddings into task-specific outputs. While we'll focus on text data, it's worth noting that the same stages can be applied to all data modalities including audio and images (Xu et al. 2023).</p>
<p><img alt="Transformer Pipeline" src="transformer-pipeline.svg"/></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=9796e5d2-ca36-4b34-a48d-de2a4e475e8f"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Tokenize">Tokenize<a class="anchor-link" href="#Tokenize">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=0f495b6e-a916-4e1d-9a29-90adf2548d1b"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Tokenize stage is responsible for breaking raw data into a sequence of "tokens". While the word "token" is often associated with text processing, the Transformer literature extends this to other data modalities as well. Examples include patches of an image or segments of an audio recording. In fact, tokenization is seen as a core strength of the Transformer architecture because it allows Transformers to process different types of data using a single, universal approach (Xu et al. 2023).</p>
<p>While tokenization is a general concept, the specific algorithms used are modality-specific. In this case, our transformer uses an algorithm known as "word-piece" (Devlin et al. 2019) to split raw text into a sequence of tokens. Next, special tokens are injected to mark the beginning and end of the sequence. Each token is then converted into an integer-encoded categorical value using a fixed token vocabulary, producing the final sequence of "input_ids" that are passed to the next stage.</p>
<p>Since our primary interest is in the Transformer layers that come later, we'll use Hugging Face's off-the-shelf tokenizer implementation here.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=98ae8a9e-db57-4d0f-96d4-17c8e0175823">
<div class="input">
<div class="prompt input_prompt">In [6]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Extract tokenizer from transformer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">tokenizer</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=c73c7451-d942-4645-abb3-f77dbac14db9">
<div class="input">
<div class="prompt input_prompt">In [7]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Tokenize sentence</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"I love ice cream"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>

<span class="n">batch</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[7]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>{'input_ids': tensor([[ 101, 1045, 2293, 3256, 6949,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=b6c372b2-d7c8-44b3-ba78-4fbc78b785ad"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Tokenizing "I love ice cream" generates the token sequence: <code>[101, 1045, 2293, 3256, 6949, 102]</code>. If we decode the integer-encoded values to see what each one represents, we can see the 4 words are represented by values 1045 to 6949. The values 101 and 102 represent special tokens <code>[CLS]</code> and <code>[SEP]</code> that were added to mark the beginning and end of the sequence respectively.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=516cc2da-dae9-4746-8132-2b7f16cfab8a">
<div class="input">
<div class="prompt input_prompt">In [8]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">input_id</span><span class="p">)</span> <span class="k">for</span> <span class="n">input_id</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[8]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>['[CLS]', 'i', 'love', 'ice', 'cream', '[SEP]']</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=abfacce5-17f9-46ee-8414-053274f8f2df"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Embeddings">Embeddings<a class="anchor-link" href="#Embeddings">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=9df54d4b-9cd9-4044-a6aa-5d08ae5582a0"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The second stage in the Transformer pipeline converts each of the integer-encoded categorical values into an "embedding". Embeddings (Bengio et al. 2000) are the fundamental data structure of the Transformer architecture. The Transformer layers we'll look at in the next stage take embeddings as input, <em>transform</em> them, and produce embeddings as output. Embeddings predated Transformers by almost 2 decades and are a fascinating topic in their own right. But we'll save the embeddings deep dive for another post. For now, all we need to know is embeddings represent each token as a unique point in an n-dimensional vector space. The vector space coordinates are initialized randomly and then learned during training.</p>
<p>Similar to tokenization, the steps required to convert tokens into embeddings depend on the data modality. In text transformers, the Embeddings stage is typically implemented using 2 lookup tables. The first lookup table maps the value of each token to a unique embedding vector. The second lookup table maps the position of each token to a unique embedding vector. The value and position embeddings are then added together to create the initial token embeddings.</p>
<p><img alt="Embeddings" src="embeddings.svg"/></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=ec8f05cb-1525-48aa-a001-dddba54d5659"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's start with value embeddings. First, we initialize the value embeddings lookup table. Next, we read the values from the tokenizer output. Finally, we pass the token values to the lookup table to get unique embeddings for each value.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=4a17a31c-f505-4191-9fab-d718216474a8">
<div class="input">
<div class="prompt input_prompt">In [9]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Initialize value embeddings lookup table</span>
<span class="n">value_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
    <span class="n">num_embeddings</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> 
    <span class="n">embedding_dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Load pre-trained state</span>
<span class="n">value_embeddings</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
    <span class="s2">"weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.embeddings.word_embeddings.weight"</span><span class="p">],</span>
<span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[9]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>&lt;All keys matched successfully&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=acbd0034-07f6-4e20-bae1-139310c8c18e">
<div class="input">
<div class="prompt input_prompt">In [10]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Calculate token values</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>

<span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">input_id</span><span class="p">)</span> <span class="k">for</span> <span class="n">input_id</span> <span class="ow">in</span> <span class="n">values</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[10]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>['[CLS]', 'i', 'love', 'ice', 'cream', '[SEP]']</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=18970299-0558-4419-b89e-9879595e2390">
<div class="input">
<div class="prompt input_prompt">In [11]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Map token values to embeddings</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">value_embeddings</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>

<span class="n">v</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[11]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([6, 768])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=7a5fd210-4c1e-4cc1-923c-4af984a8f8dc">
<div class="input">
<div class="prompt input_prompt">In [12]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Show sample of value embeddings</span>
<span class="n">v</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[12]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 3.9925e-02, -1.0171e-02, -2.0390e-02,  ...,  6.1588e-02,
          2.1959e-02,  2.2732e-02],
        [-1.2794e-02,  4.9879e-03, -2.6270e-02,  ..., -7.2300e-05,
          5.3657e-03,  1.1908e-02],
        [ 5.9359e-02, -2.3563e-02, -2.0560e-03,  ..., -1.0420e-02,
          1.4846e-02, -1.2815e-02],
        [-2.4101e-02, -2.4911e-02, -2.2601e-02,  ..., -2.5139e-02,
          1.1392e-02,  3.2655e-02],
        [-8.5466e-02, -5.9276e-02, -5.6659e-02,  ..., -1.7192e-02,
         -8.6179e-02, -4.5105e-02],
        [-2.1060e-02, -6.4941e-03, -1.0682e-02,  ..., -2.3401e-02,
          6.1463e-03, -6.4845e-03]], grad_fn=&lt;EmbeddingBackward0&gt;)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=9c203d1a-fd6d-43aa-83a0-82e4a3544dd1"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we'll follow a similar set of steps for the position embeddings. We'll start by initializing the position embeddings lookup table. Next, we'll calculate the positions from the tokenizer output. Finally, we pass the token positions to the lookup table to get unique embeddings for each position.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=740a3ecc-d180-445f-a1fe-f2e157e9b1fa">
<div class="input">
<div class="prompt input_prompt">In [13]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Configure position embeddings lookup table</span>
<span class="n">position_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
    <span class="n">num_embeddings</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">max_sequence_length</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Load pre-trained state</span>
<span class="n">position_embeddings</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
    <span class="s2">"weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.embeddings.position_embeddings.weight"</span><span class="p">],</span>
<span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[13]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>&lt;All keys matched successfully&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=547facad-a7b5-480b-bd9e-22a21650a838">
<div class="input">
<div class="prompt input_prompt">In [14]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Calculate token positions</span>
<span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">values</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="n">positions</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[14]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>tensor([0, 1, 2, 3, 4, 5])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=49cbeef1-f5e4-4e5c-8e15-562c585f289c">
<div class="input">
<div class="prompt input_prompt">In [15]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Map token positions to embeddings</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">position_embeddings</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span>

<span class="n">p</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[15]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([6, 768])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=0f4f9448-ef27-46a0-964d-c7e80d367947">
<div class="input">
<div class="prompt input_prompt">In [16]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Show sample of position embeddings</span>
<span class="n">p</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[16]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 1.8007e-02, -2.3798e-02, -3.5982e-02,  ...,  4.5726e-04,
          5.1363e-05,  1.5002e-02],
        [ 7.8592e-03,  4.8144e-03, -1.6093e-02,  ...,  2.9312e-02,
          2.7634e-02, -8.5431e-03],
        [-1.1663e-02, -3.1590e-03, -9.4000e-03,  ...,  1.4870e-02,
          2.1609e-02, -7.4069e-03],
        [-4.0848e-03, -1.1123e-02, -2.1704e-02,  ...,  1.8962e-02,
          4.6763e-03, -1.0220e-03],
        [-8.2666e-03, -4.1641e-03, -7.5136e-03,  ...,  1.9757e-02,
         -2.2192e-03,  3.8681e-03],
        [ 4.6293e-04, -1.8499e-02, -1.9709e-02,  ...,  5.4042e-03,
          1.8076e-02,  2.9490e-03]], grad_fn=&lt;EmbeddingBackward0&gt;)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=867f8e9d-b332-4672-9aee-8ebf472e7076"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have value and position embeddings, we add and normalize them to get the final "position-encoded token embeddings".</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=21482619-56f4-4352-9842-1fd1441dc3e7">
<div class="input">
<div class="prompt input_prompt">In [17]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Configure embeddings normalization</span>
<span class="n">normalize_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span>
    <span class="n">normalized_shape</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> 
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Load pre-trained state</span>
<span class="n">normalize_embeddings</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
    <span class="s2">"weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.embeddings.LayerNorm.weight"</span><span class="p">],</span> 
    <span class="s2">"bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.embeddings.LayerNorm.bias"</span><span class="p">],</span>
<span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[17]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>&lt;All keys matched successfully&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=9a2a90c3-c770-4a33-8603-a0fa1cdfcbde">
<div class="input">
<div class="prompt input_prompt">In [18]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Add and normalize value and position embeddings</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">normalize_embeddings</span><span class="p">(</span><span class="n">v</span> <span class="o">+</span> <span class="n">p</span><span class="p">)</span>

<span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[18]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([6, 768])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=9d963258-fc89-4326-984e-8cd40a31b131">
<div class="input">
<div class="prompt input_prompt">In [19]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Show sample of token embeddings</span>
<span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[19]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 0.3549, -0.1386, -0.2253,  ...,  0.1536,  0.0748,  0.1310],
        [ 0.2282,  0.5511, -0.5092,  ...,  0.6421,  0.9541,  0.3192],
        [ 1.4511, -0.0794,  0.2168,  ...,  0.2851,  1.0723, -0.0919],
        [-0.0564, -0.1761, -0.2870,  ...,  0.1442,  0.6767,  1.0396],
        [-1.1349, -0.5135, -0.4714,  ...,  0.3874, -1.0348, -0.2812],
        [-0.2980, -0.3332, -0.3742,  ..., -0.3392,  0.3764, -0.1298]],
       grad_fn=&lt;NativeLayerNormBackward0&gt;)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=880c934b-a53a-4c6f-8eb1-c2810aeb27fc"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Congrats! You've converted the raw text "I love ice cream" into embeddings that encode both the token values and positions.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=3ef34552-3fc7-4a00-856c-705a78e0f0d5"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Context">Context<a class="anchor-link" href="#Context">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=e41880d9-cc37-4d32-a8a6-cb0ad60b74a6"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the previous stage, we mapped the token values and positions to embeddings. But these embeddings represent the tokens in <em>isolation</em>. The Context stage is responsible for infusing each embedding with contextual signals drawn from the entire sequence. At a conceptual level, this should be intuitive. The meaning of the word "ice" changes when you add "cream" after it.</p>
<center><img alt="No description has been provided for this image" src="contextualized-embeddings.svg" width="632"/></center>
<p>The Context stage works by passing the token embeddings through multiple layers of attention and feedforward blocks. The attention blocks focus on relationships between tokens, augmenting each embedding with information drawn from the surrounding embeddings. The feedforward blocks focus on individual tokens, transforming the contextual clues added by attention with the non-linear transformation magic neural networks are famous for.</p>
<p>The following diagram illustrates the stack of Transformer layers in the Context stage. The contents of each layer are identical. By arranging the layers in a stack, the model builds context in small increments similar to the hierarchical features in a CNN. The main differences between popular Transformer models such as BERT and GPT come down to how these layers are configured.</p>
<center><img alt="No description has been provided for this image" src="transformer-layers.svg" width="80%"/></center>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=510dc3aa-2911-46f9-b5fa-1dbea77335ea"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As illustrated above, given input embeddings $X$, we can define the output embeddings $Z$ as:</p>
<p>\begin{align}
Y &amp;= Normalize(X + Attention(X)) \\
Z &amp;= Normalize(Y + FNN(Y))
\end{align}</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=49dcb2ce-05ff-4837-a40d-b320ff65dc5d"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Scaled-Dot-Product-Attention">Scaled Dot-Product Attention<a class="anchor-link" href="#Scaled-Dot-Product-Attention">¶</a></h3><p>The Attention block is the signature component of the Transformer architecture. It's also one of the most complicated and likely the least familiar when you're first learning about Transformers. We'll walk through the core attention algorithm described in the original "All You Need is Attention" paper by Vaswani et al. one step at a time. At the end of the Context section, we'll put all the pieces together.</p>
<p>Vaswani et al. described their attention algorithm as Scaled Dot-Product Attention (SDPA) and defined the standard attention equation everyone cites:</p>
<p>\begin{equation}
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_K}})V
\end{equation}</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=0c22fe94-4120-4161-8359-451572eec2b7"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Queries,-Keys,-Values">Queries, Keys, Values<a class="anchor-link" href="#Queries,-Keys,-Values">¶</a></h3><p>The $Q$, $K$, and $V$ terms in the SDPA equation are "query", "key", and "value" matrices respectively. Each row in $Q$, $K$, and $V$ represents a token embedding that has been projected to distinct representation subspaces. Query embeddings represent selection criteria for the surrounding tokens that would add context to the current token definition. Key embeddings represent characteristics that satisfy the selection criteria. Value embeddings represent the contextual information one token transfers to another. Together, queries, keys, and values allow the attention mechanism to refine the representation of each token based on the surrounding tokens.</p>
<p>Given an $n \times d_{model}$ matrix of input embeddings $X$, we can expand on the SDPA equation by defining linear projections $Queries$, $Keys$, and $Values$:</p>
<p>\begin{align}
SDPA(Q, K, V) &amp;= softmax(\frac{QK^T}{\sqrt{d_K}})V \\
\text{where } Q &amp;= Queries(X) \\
K &amp;= Keys(X) \\
V &amp;= Values(X)
\end{align}</p>
<p>Enough LaTeX, let's see what this looks like in Python.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=38d246a9-bdfc-45e8-97b1-dd2efcf3de9d">
<div class="input">
<div class="prompt input_prompt">In [20]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Configure query, key, value projections</span>
<span class="n">queries</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
    <span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
    <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">keys</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
    <span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> 
    <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
    <span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> 
    <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Load pre-trained state</span>
<span class="n">queries</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
    <span class="s2">"weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.transformer.layer.0.attention.q_lin.weight"</span><span class="p">],</span>
    <span class="s2">"bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.transformer.layer.0.attention.q_lin.bias"</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">keys</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
    <span class="s2">"weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.transformer.layer.0.attention.k_lin.weight"</span><span class="p">],</span>
    <span class="s2">"bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.transformer.layer.0.attention.k_lin.bias"</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">values</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
    <span class="s2">"weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.transformer.layer.0.attention.v_lin.weight"</span><span class="p">],</span>
    <span class="s2">"bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.transformer.layer.0.attention.k_lin.bias"</span><span class="p">],</span>
<span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[20]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>&lt;All keys matched successfully&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=33e69dfc-ccc1-4f52-b6b9-65a9207f5285">
<div class="input">
<div class="prompt input_prompt">In [21]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Project token embeddings to query, key, and value spaces</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">queries</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">keys</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">values</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[21]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([6, 768]), torch.Size([6, 768]), torch.Size([6, 768]))</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=90a8d05c-3e01-454f-a6d8-51d7df87c452"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see the projections generated unique query, key, and value embeddings for each of the 6 tokens <code>['[CLS]', 'i', 'love', 'ice', 'cream', '[SEP]']</code>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=a27d541e-2284-4599-90d3-d0854005054f"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Attention-Weights">Attention Weights<a class="anchor-link" href="#Attention-Weights">¶</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=9ab030dc-bcd5-494a-9aae-341945664a42"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have $Q$, $K$, and $V$, we can delve into the SDPA equation itself. For each input embedding, SDPA calculates a weighted sum of the value projections for all the tokens in the sequence. We already saw the value projections are represented by <code>V</code>. The weights are represented by the softmax term:</p>
<p>\begin{align}
softmax(\frac{QK^T}{\sqrt{d_K}})
\end{align}</p>
<p>I wouldn't hold it against you if it's not immediately obvious what we get here. To see what's happening, let's break this down even further.</p>
<p>First, the $QK^T$ term calculates a $d_Q \times d_K$ matrix of the dot products of each query embedding with every key embedding. To see why, imagine we have 2 token embeddings of length 3.</p>
<p>\begin{align}
QK^T
&amp;=
\begin{bmatrix}
q_{00} &amp; q_{01} &amp; q_{02} \\
q_{10} &amp; q_{11} &amp; q_{12}
\end{bmatrix}
\begin{bmatrix}
k_{00} &amp; k_{10} \\
k_{01} &amp; k_{11} \\
k_{02} &amp; k_{12}
\end{bmatrix}
=
\begin{bmatrix}
w_{00} &amp; w_{01} \\
w_{10} &amp; w_{11}
\end{bmatrix} \\
\text{where } w_{ij} &amp;= row(Q, i) \cdot row(K, j)
\end{align}</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=7db10512-59cb-42fb-8dd1-334e6dec0c03">
<div class="input">
<div class="prompt input_prompt">In [22]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Calculate similarity between Q and K</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">w</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[22]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([6, 6])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=2c069a27-6b05-42ba-9448-d53e2c149745"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Second, the $1/\sqrt{d_K}$ term scales the dot products down to avoid pushing the softmax function into regions with very small gradients.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=5c4d8df1-ae31-4d2c-a0a1-158d7b823950">
<div class="input">
<div class="prompt input_prompt">In [23]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">w</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="n">w</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[23]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([6, 6])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=835d7d85-1457-4994-8714-7dbd370a0487"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, the softmax function normalizes the weights across the keys.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=c6b71776-7b28-434a-9437-62769469dea9">
<div class="input">
<div class="prompt input_prompt">In [24]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Normalize weights across keys</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">w</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[24]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>tensor([[9.9964e-01, 8.3108e-06, 5.9084e-06, 1.2478e-05, 1.7158e-05, 3.1488e-04],
        [9.6971e-01, 3.5093e-04, 3.8457e-03, 1.4637e-04, 2.0632e-04, 2.5743e-02],
        [9.8888e-01, 1.4836e-03, 3.4143e-04, 3.4613e-04, 1.1235e-03, 7.8293e-03],
        [7.4676e-01, 3.8455e-05, 1.0549e-03, 8.3005e-05, 2.2695e-01, 2.5122e-02],
        [7.5151e-01, 9.0717e-06, 2.4871e-04, 2.3733e-01, 1.7790e-05, 1.0880e-02],
        [8.6867e-01, 4.3978e-04, 4.6904e-05, 7.9754e-05, 1.2606e-03, 1.2950e-01]],
       grad_fn=&lt;SoftmaxBackward0&gt;)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=4f3fffa5-44e4-4064-b749-1128710360bb">
<div class="input">
<div class="prompt input_prompt">In [25]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Plot weights for each query</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">gridspec_kw</span><span class="o">=</span><span class="p">{</span><span class="s2">"wspace"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">"hspace"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">})</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="o">//</span><span class="mi">3</span><span class="p">][</span><span class="n">i</span><span class="o">%</span><span class="k">3</span>]
    <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">])),</span> <span class="n">y</span><span class="o">=</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Query </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Keys"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Weight"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_png output_subarea">
<img alt="No description has been provided for this image" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA1cAAAHWCAYAAACbsXOkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABqeElEQVR4nO3de3gU9d3//1cSSEIgJ4xJCEQSFEXKIQiSG1AOkhKtCtzVFlE5VbGlQbHRbyW9C0hVAqIYK9QgFZB6gJaDJxSlkXBQBAsihwICglBhA8GSE5LQ5PP7wx9b1mxCAruZ2ezzcV17aWY+M/ue3cyLvGdmZwOMMUYAAAAAgEsSaHUBAAAAANAY0FwBAAAAgAfQXAEAAACAB9BcAQAAAIAH0FwBAAAAgAfQXAEAAACAB9BcAQAAAIAH0FwBAAAAgAfQXAEAAACAB9BcAQAAAIAH0Fzhgnbt2qV7771XrVu3VkhIiBISEnTvvffqn//8p9WlXbJvvvlGP//5zxUVFaWIiAgNGTJEX331ldVlAfiBxppDe/fu1W9+8xv17t1boaGhCggI0KFDh6wuC0ANGmsWLV++XMOGDVO7du0UFhama665Ro888ohOnTpldWk+J8AYY6wuAva1fPlyDR8+XC1bttR9992n5ORkHTp0SC+//LK+/fZbLVmyREOGDLG6zItSWlqq6667TkVFRXrkkUfUtGlTPffcczLGaNu2bbrsssusLhGAGncOLVy4UPfdd586duyoJk2aaNu2bTp48KCSkpKsLg3ADzTmLIqJiVFCQoKGDh2qK664Qjt27FBubq7atWunrVu3qlmzZlaX6DsMUIP9+/ebsLAw06FDB3P8+HGXeSdOnDAdOnQwLVq0MF999VWD11ZWVnbJ65gxY4aRZDZv3uyctnv3bhMUFGSysrIuef0ALl1jz6GTJ0+a4uJiY4wxM2fONJLMwYMHL3m9ADyrsWfRmjVrqk175ZVXjCQzb968S16/P+GyQNRo5syZOn36tF566SVdfvnlLvNiYmI0d+5clZaWaubMmc7po0ePdnvE9fHHH1dAQEC16a+++qq6d++uZs2aqWXLlrrrrrt05MgRlzH9+/dXp06dtGXLFvXt21dhYWH63e9+p1GjRikmJkZnz56ttt5BgwbpmmuuqXX7li5dquuvv17XX3+9c1qHDh00cOBA/fWvf611WQANo7HnUMuWLRUeHl7rGADWa+xZ1L9//2rT/vd//1eStHv37lqXhSuaK9TonXfeUVJSkm688Ua38/v27aukpCS98847F7X+p556SiNHjlT79u01a9YsPfzww8rLy1Pfvn2rXeN78uRJ3XLLLUpJSVFOTo4GDBigESNG6OTJk/rggw9cxjocDn300Ue69957a3zuqqoqbd++XT169Kg2r2fPnjpw4IBKSkouarsAeE5jziEAvsMfs8jhcEj6vnlEPVh96gz2dOrUKSPJDBkypNZxgwcPNpKcl7WMGjXKtG3bttq4KVOmmPN/3Q4dOmSCgoLMU0895TJux44dpkmTJi7T+/XrZySZ3Nxcl7GVlZWmTZs2ZtiwYS7TZ82aZQICAmo9NX/ixAkjyfzhD3+oNm/OnDlGktmzZ0/NGw7A6xp7Dv0QlwUC9uRvWXTOfffdZ4KCgsyXX35Z72X9GWeu4Na5szYXulzl3Pz6nuVZvny5qqqq9POf/1yFhYXOR3x8vNq3b681a9a4jA8JCdGYMWNcpgUGBuqee+7R22+/7fL8r732mnr37q3k5OQan/+7775zrveHQkNDXcYAsEZjzyEAvsEfs+j111/Xyy+/rEceeUTt27ev17L+juYKbtU1IEpKShQQEFDvU8b79u2TMUbt27fX5Zdf7vLYvXu3jh8/7jK+devWCg4OrraekSNH6rvvvtOKFSskfX9b4y1btmjEiBG1Pv+5u96Ul5dXm3fmzBmXMQCs0dhzCIBv8LcsWr9+ve677z6lp6frqaeeqteykJpYXQDsKTIyUgkJCdq+fXut47Zv3642bdo4d3J3H9CUpMrKSpefq6qqFBAQoPfff19BQUHVxrdo0cLl55oanY4dO6p79+569dVXNXLkSL366qsKDg7Wz3/+81rrbtmypUJCQnTs2LFq885NS0hIqHUdALyrsecQAN/gT1n0xRdfaPDgwerUqZOWLl2qJk1oFeqLVww1uv322zV37lxt2LBBN9xwQ7X569ev16FDh5SZmemcFh0d7fYL577++muXn6+88koZY5ScnKyrr776kuocOXKkMjMzdezYMb3++uu69dZbFR0dXesygYGB6ty5s/7xj39Um7dp0ya1a9eOO3gBNtCYcwiA7/CHLDpw4IBuvvlmxcbG6r333qvW1KGOrPzAF+xt3759JiwszHTs2NEUFha6zDt58qTp2LGjiYiIcPmQ5OzZs40k88UXXzinHT161LRo0cLlw5v79+83QUFB5u677zZVVVUu666qqnJ5vn79+pkf/ehHNdZ5/Phx06RJE/Ozn/3MSDLLli2r0/ZNnz7dSDKfffaZc9qePXtMUFCQeeyxx+q0DgDe1dhz6Hzc0AKwr8aeRceOHTPt2rUzCQkJZNAlCjDGGIv6OviAZcuWafjw4YqJian2beT//ve/tXjxYg0ePNg5/uTJk2rbtq3i4uL00EMP6fTp03rxxRd1+eWXa+vWrTr/12369OnKyspS7969NXToUIWHh+vgwYNasWKFHnjgAT366KOSvv/uhcLCQu3cubPGOm+//Xa9++67ioqKksPhcHujih8qKSlRt27dVFJSokcffVRNmzbVrFmzVFlZqW3btlX7HgsA1mjMOVRUVKQXXnhBkvTxxx9r1apVeuSRRxQVFaWoqCiNHz/+Yl82AB7WmLMoJSVFX3zxhX7729+qc+fOLvPi4uL04x//uL4vl/+ytLWDT9ixY4e5++67TXx8vAkMDDSSTGhoqNm1a5fb8R9++KHp1KmTCQ4ONtdcc4159dVXq9129Jxly5aZG264wTRv3tw0b97cdOjQwWRkZJi9e/c6x1zoKI0xxvz1r381kswDDzxQr207cuSIufPOO01ERIRp0aKFue2228y+ffvqtQ4A3tdYc+jgwYNGktuHu1s4A7BWY82imnJIkunXr1+d1wPOXOEiLFq0SKNHj9a9996rRYsWWV2OJOmtt97S0KFDtW7duhq/4A9A40EOAbADsgg/xA0tUG8jR47UsWPHNHHiRLVp00bTpk2zuiTNmzdP7dq1c/shUwCNDzkEwA7IIvwQZ67g0xYvXqzt27crOztbzz//vB566CGrSwLgZ8ghAHZAFtkDzRV8WkBAgFq0aKFhw4YpNzeX72MA0ODIIQB2QBbZA80VAAAAAHhAoNUFAAAAAEBjQHMFwO+tW7dOt99+uxISEhQQEKA333zzgsvk5+fruuuuU0hIiK666iotXLiw2pg5c+YoKSlJoaGhSk1N1ebNmz1fPAAAsA0uxnSjqqpKR48eVXh4uAICAqwuB7CEMUYlJSVKSEhQYGDjPg5TVlamrl276he/+IV++tOfXnD8wYMHdeutt+pXv/qVXnvtNeXl5en+++9Xq1atlJ6eLklasmSJMjMzlZubq9TUVOXk5Cg9PV179+5VbGxsneoii+Dv/CmH7IocAuqXRXzmyo1//etfSkxMtLoMwBaOHDmiNm3aWF1GgwkICNCKFSs0dOjQGsc89thjWrlypXbu3Omcdtddd+nUqVNatWqVJCk1NVXXX3+9Zs+eLen7P1ASExP14IMPauLEiXWqhSwCvudvOWQn5BDwX3XJIs5cuREeHi7p+xcwIiLC4moAaxQXFysxMdG5P+C/Nm7cqLS0NJdp6enpevjhhyVJFRUV2rJli7KyspzzAwMDlZaWpo0bN9a43vLycpWXlzt/PnfsiyyCv/KnHFq3bp1mzpypLVu26NixYxc8yCN9f3lyZmamdu3apcTERP3+97/X6NGjXcbMmTNHM2fOlMPhUNeuXfXCCy+oZ8+eda6Lv4mA+mURzZUb5057R0REECTwe1wGUp3D4VBcXJzLtLi4OBUXF+u7777Tv//9b1VWVrods2fPnhrXm52dralTp1abThbB3/lDDtn18mT+JgL+qy5ZZOkFzL76IfKi0xU6cLxUnx/+tw6cKFXR6QqPrh+Af8rKylJRUZHzceTIkRrHkkNA43LLLbfoySef1P/+7//WaXxubq6Sk5P17LPP6tprr9X48eN155136rnnnnOOmTVrlsaOHasxY8aoY8eOys3NVVhYmObPn++xuskiwJWlZ67sepSmNkdPfafHlm3X+n2Fzml928do+h1dlBDV7JLXD8D+4uPjVVBQ4DKtoKBAERERatasmYKCghQUFOR2THx8fI3rDQkJUUhIyAWfnxwC0FCXJxcXF9c4liwCqrP0zJWvHaUpOl1RLUQkad2+Qk1ctp2jNYCf6NWrl/Ly8lymrV69Wr169ZIkBQcHq3v37i5jqqqqlJeX5xxzscghANKFL08uLCys8fJkh8NR43qzs7MVGRnpfNR0MwuyCHDPp+5rWtNRmnNHYM4dpTl/TF2P0hQXF7s83CksragWIues21eowlKCBPBFpaWl2rZtm7Zt2ybp+7Pk27Zt0+HDhyV9f7neyJEjneN/9atf6auvvtJvf/tb7dmzR3/605/017/+Vb/5zW+cYzIzMzVv3jy98sor2r17t8aNG6eysjKNGTPmkmolhwB4U10vTyaLAPd86oYWDf0h8h8qPnO21vklF5gPwJ7+8Y9/aMCAAc6fMzMzJUmjRo3SwoULdezYMWejJUnJyclauXKlfvOb3+j5559XmzZt9Oc//9l5ebIkDRs2TCdOnNDkyZPlcDiUkpKiVatWVcun+iKHAEjWX55MFgHu+VRz5S1ZWVnOP6ak/95u8YciQpvWup7wC8wHYE/9+/dXbV/55+7GOf3799fnn39e63rHjx+v8ePHX2p5LsghANL3lye/9957LtNqujz53C3dz12e7IlcIosA93zqssALHaWJiYm56KM0524xWtutRmNaBKtv+xi38/q2j1FMi+B6bhEA1A85BDROvnR5skQWATXxqebKyg+RS1JkWLCm39GlWpj0bR+jGXd0UWQYQQLAu8ghoHH6xz/+oW7duqlbt26Svm+MunXrpsmTJ0tSjZcnr169Wl27dtWzzz7r9vLkZ555RpMnT1ZKSoq2bdvmkcuTJbIIqEmAqe1aGC8rLS3V/v37JUndunXTrFmzNGDAALVs2VJXXHGFsrKy9M0332jRokWSvj+K06lTJ2VkZOgXv/iFPvroIz300ENauXKly63YR40apblz56pnz57KycnRX//6V+3Zs6fOYVJcXKzIyEgVFRW5PYtVdLpChaUVKjlzVuGhTRXTIpgQQaNzof0A3lfbe0AOwR+QQ9bjbyKgfllk6WeufOlD5OeLDCM4AFiLHAJgB2QR4MrSM1d2xZEygP3ADngP4O/YB6zHewDUbz/wqc9cAQAAAIBd0VwBAAAAgAfQXAEAAACAB9BcAQAAAIAH0FwBAAAAgAfQXAEAAACAB9BcAQAAAIAH0FwBAAAAgAfQXAEAAACAB9BcAQAAAIAH0FwBAAAAgAfQXAEAAACAB9BcAQAAAIAH0FwBAAAAgAfQXAEAAACAB9BcAQAAAIAH0FwBAAAAgAfQXAEAAACAB9BcAQAAAIAH0FwBAAAAgAfQXAEAAACAB9BcAYCkOXPmKCkpSaGhoUpNTdXmzZtrHNu/f38FBARUe9x6663OMaNHj642/+abb26ITQEAABZpYnUBAGC1JUuWKDMzU7m5uUpNTVVOTo7S09O1d+9excbGVhu/fPlyVVRUOH8+efKkunbtqp/97Gcu426++WYtWLDA+XNISIj3NgIAAFiOM1cA/N6sWbM0duxYjRkzRh07dlRubq7CwsI0f/58t+Nbtmyp+Ph452P16tUKCwur1lyFhIS4jIuOjq61jvLychUXF7s8APgXzqIDvs0WzRVBAsAqFRUV2rJli9LS0pzTAgMDlZaWpo0bN9ZpHS+//LLuuusuNW/e3GV6fn6+YmNjdc0112jcuHE6efJkrevJzs5WZGSk85GYmFj/DQLgs86dRZ8yZYq2bt2qrl27Kj09XcePH3c7fvny5Tp27JjzsXPnTgUFBbk9i37+uDfeeKMhNgfwS5Y3VwQJACsVFhaqsrJScXFxLtPj4uLkcDguuPzmzZu1c+dO3X///S7Tb775Zi1atEh5eXmaMWOG1q5dq1tuuUWVlZU1risrK0tFRUXOx5EjRy5uowD4JLucRQdw8Sz/zNX5QSJJubm5WrlypebPn6+JEydWG9+yZUuXnxcvXlxrkACAN7388svq3Lmzevbs6TL9rrvucv5/586d1aVLF1155ZXKz8/XwIED3a4rJCSEz2UBfurcWfSsrCznNE+fRY+OjtZNN92kJ598UpdddpnbdZSXl6u8vNz5M5cnA/Vj6Zkru1yOw+ccAP8VExOjoKAgFRQUuEwvKCi44AGasrIyLV68WPfdd98Fn6ddu3aKiYnR/v37L6leAI2TXc6ic3kycGksba4IEgBWCw4OVvfu3ZWXl+ecVlVVpby8PPXq1avWZf/2t7+pvLxc99577wWf51//+pdOnjypVq1aXXLNAPBDtZ1FHzx4sDp37qyhQ4fq3Xff1Weffab8/Hy36+HyZODSWP6Zq0tBkADwhMzMTM2bN0+vvPKKdu/erXHjxqmsrMx5ufLIkSNdLtU55+WXX9bQoUOrXV5TWlqq//f//p8+/fRTHTp0SHl5eRoyZIiuuuoqpaenN8g2AfAtdjmLHhISooiICJcHgLqztLkiSADYwbBhw/TMM89o8uTJSklJ0bZt27Rq1SrnWfXDhw/r2LFjLsvs3btXGzZscJtBQUFB2r59uwYPHqyrr75a9913n7p3767169fzmSoAbnEWHWgcLL2hxflBMnToUEn/DZLx48fXuixBAsCTxo8fX2PuuDvrfc0118gY43Z8s2bN9MEHH3iyPAB+IDMzU6NGjVKPHj3Us2dP5eTkVDuL3rp1a2VnZ7ssV9tZ9KlTp+qOO+5QfHy8Dhw4oN/+9recRQe8yPK7BRIkAAAA359FP3HihCZPniyHw6GUlJRqZ9EDA10vOjp3Fv3DDz+str5zZ9FfeeUVnTp1SgkJCRo0aJCeeOIJzqIDXmJ5c0WQAAAAfI+z6IBvCzA17ZF+rLi4WJGRkSoqKuLzV/Bb7AfW4z2Av2MfsB7vAVC//cCn7xYIAAAAAHZBcwUAAAAAHkBzBQAAAAAeQHMFAAAAAB5AcwUAAAAAHkBzBQAAAAAeUO/m6vDhw26/T8EYo8OHD3ukKAC4ELIIgB2QRQDOV+/mKjk5WSdOnKg2/dtvv1VycrJHigKACyGLANgBWQTgfPVurowxCggIqDa9tLRUoaGhHikKAC6ELAJgB2QRgPM1qevAzMxMSVJAQIAmTZqksLAw57zKykpt2rRJKSkpHi8QAM5HFgGwA7IIgDt1bq4+//xzSd8fodmxY4eCg4Od84KDg9W1a1c9+uijnq8QAM5DFgGwA7IIgDt1bq7WrFkjSRozZoyef/55RUREeK0oAKgJWQTADsgiAO7Uubk6Z8GCBd6oAwDqhSwCYAdkEYDz1bu5Kisr0/Tp05WXl6fjx4+rqqrKZf5XX33lseIAoCZkEQA7IIsAnK/ezdX999+vtWvXasSIEWrVqpXbO+QAgLeRRQDsgCwCcL56N1fvv/++Vq5cqT59+nijHgCoE7IIgB2QRQDOV+/vuYqOjlbLli29UQsA1BlZBMAOyCIA56t3c/XEE09o8uTJOn36tDfqAYA6IYsA2AFZBOB8dbossFu3bi7XEO/fv19xcXFKSkpS06ZNXcZu3brVsxUCwP+PLAJgB2QRgJrUqbkaOnSol8sAgAsjiwDYAVkEoCYBxhhjdRF2U1xcrMjISBUVFfGlgPBb7AfW4z2Av2MfsB7vAVC//aDen7kCAAAAAFR30XcL/OHjsssuU+vWrdWvXz++rRyA13k6i+bMmaOkpCSFhoYqNTVVmzdvrnHswoULFRAQ4PIIDQ11GWOM0eTJk9WqVSs1a9ZMaWlp2rdv30VvLwB74u8iAOerd3M1efJkBQYG6tZbb9XUqVM1depU3XrrrQoMDFRGRoauvvpqjRs3TvPmzfNGvQAgybNZtGTJEmVmZmrKlCnaunWrunbtqvT0dB0/frzGZSIiInTs2DHn4+uvv3aZ//TTT+uPf/yjcnNztWnTJjVv3lzp6ek6c+bMJW87APvw9N9FHOgBfJypp5/+9KfmxRdfrDY9NzfX/PSnPzXGGPPHP/7RdOrUqc7rnD17tmnbtq0JCQkxPXv2NJs2bapx7IIFC4wkl0dISIjLmKqqKjNp0iQTHx9vQkNDzcCBA82XX35Z53qKioqMJFNUVFTnZYDGxu77gSezqGfPniYjI8P5c2VlpUlISDDZ2dluxy9YsMBERkbWuL6qqioTHx9vZs6c6Zx26tQpExISYt54440L1nOO3d8DwNt8YR/wZBYtXrzYBAcHm/nz55tdu3aZsWPHmqioKFNQUOB2/IIFC0xERIQ5duyY8+FwOFzGTJ8+3URGRpo333zTfPHFF2bw4MEmOTnZfPfdd3XaPl94DwBvq89+UO/mqnnz5mbfvn3Vpu/bt880b97cGGPM/v37TVhYWJ3WR5AA9mT3/cBTWVReXm6CgoLMihUrXKaPHDnSDB482O0yCxYsMEFBQeaKK64wbdq0MYMHDzY7d+50zj9w4ICRZD7//HOX5fr27WseeuihGms5c+aMKSoqcj6OHDli6/cA8Da755Axnv27yI4HenzhPQC8rT77Qb0vC2zZsqXeeeedatPfeecd5zeUl5WVKTw8vE7rmzVrlsaOHasxY8aoY8eOys3NVVhYmObPn1/jMgEBAYqPj3c+4uLinPOMMcrJydHvf/97DRkyRF26dNGiRYt09OhRvfnmm/XbWAC25aksKiwsVGVlpUuOSFJcXJwcDofbZa655hrNnz9fb731ll599VVVVVWpd+/e+te//iVJzuXqs05Jys7OVmRkpPORmJhYa+0ArOepLKqoqNCWLVuUlpbmnBYYGKi0tDRt3LixxuVKS0vVtm1bJSYmasiQIdq1a5dz3sGDB+VwOFzWGRkZqdTU1BrXWV5eruLiYpcHgLqr0/dcnW/SpEkaN26c1qxZo549e0qSPvvsM7333nvKzc2VJK1evVr9+vW74LrOBUlWVpZzWn2CpKqqStddd52mTZumH/3oR5IuHCR33XVXtfWVl5ervLzc+TNBAtifJ7Oovnr16qVevXo5f+7du7euvfZazZ07V0888cRFrzcrK0uZmZnOn4uLi2mwAJvzVBbVdqBnz549bpc5d6CnS5cuKioq0jPPPKPevXtr165datOmzUUd6MnOztbUqVMvvOEA3Kp3czV27Fh17NhRs2fP1vLlyyV9v3OvXbtWvXv3liQ98sgjdVoXQQLgYnkqi2JiYhQUFKSCggKX6QUFBYqPj69TLU2bNlW3bt20f/9+SXIuV1BQoFatWrmsMyUlpcb1hISEKCQkpE7PCcAePPl3UX1540APB3mAS1Pv5kqS+vTpoz59+ni6ljohSACc44ksCg4OVvfu3ZWXl6ehQ4dKkqqqqpSXl6fx48fXaR2VlZXasWOHfvKTn0iSkpOTFR8fr7y8PGczVVxcrE2bNmncuHGXVC8A+/FEFtnlQA8HeYBLU6fPXJ1/mdwPr8O9lOtyvR0kdV1nSEiIIiIiXB4A7MdbWZSZmal58+bplVde0e7duzVu3DiVlZVpzJgxkqSRI0e6XL78hz/8QR9++KG++uorbd26Vffee6++/vpr3X///ZK+/1zoww8/rCeffFJvv/22duzYoZEjRyohIcHZwAHwXd7IovMP9Jxz7kDP+QeVa3PuQM+5Rur8Az3n17tp06Y6rxNA/dTpzFV0dLSOHTum2NhYRUVFKSAgoNoYY4wCAgJUWVlZ5yfniDGA+vBWFg0bNkwnTpzQ5MmT5XA4lJKSolWrVjkvLz58+LACA/97LOrf//63xo4dK4fDoejoaHXv3l2ffPKJOnbs6Bzz29/+VmVlZXrggQd06tQp3XDDDVq1alW176AB4Hu8lUWZmZkaNWqUevTooZ49eyonJ6fagZ7WrVsrOztb0vcHev7nf/5HV111lU6dOqWZM2fWeKCnffv2Sk5O1qRJkzjQA3hRnZqrjz76yHnHmzVr1ni0AIIEQF15M4vGjx9f40Gd/Px8l5+fe+45Pffcc7WuLyAgQH/4wx/0hz/8wVMlArAJb2URB3oA3xdgjDFWFzF79mzNnDnTGSR//OMflZqaKknq37+/kpKStHDhQknSb37zGy1fvtwlSJ588kl169bNuT5jjKZMmaKXXnrJGSR/+tOfdPXVV9epnuLiYkVGRqqoqIhLBOG32A+sx3sAf8c+YD3eA6B++8FFNVfr16/X3Llz9dVXX+lvf/ubWrdurb/85S9KTk7WDTfccNGF2wVBAvjGfkAWAY2br+wDjTmLfOU9ALypPvtBvb9EeNmyZUpPT1ezZs20detW5/dDFRUVadq0aRdXMQDUE1kEwA7IIgDnq3dz9eSTTyo3N1fz5s1T06ZNndP79OmjrVu3erQ4AKgJWQTADsgiAOerd3O1d+9e9e3bt9r0yMhInTp1yhM1AcAFkUUA7IAsAnC+ejdX8fHxzu+UOt+GDRvUrl07jxQFABdCFgGwA7IIwPnq3VyNHTtWEyZM0KZNmxQQEKCjR4/qtdde06OPPsr3SAFoMGQRADsgiwCcr07fcyVJBw8eVHJysiZOnKiqqioNHDhQp0+fVt++fRUSEqJHH31UDz74oDdrBQCyCIAtkEUA3Klzc3XllVeqbdu2GjBggAYMGKDdu3erpKREpaWl6tixo1q0aOHNOgFAElkEwB7IIgDu1Lm5+uijj5Sfn6/8/Hy98cYbqqioULt27XTTTTfppptuUv/+/Z3fIA4A3kIWAbADsgiAOxf1JcJnzpzRJ5984gyVzZs36+zZs+rQoYN27drljTobFF+YB/jGfkAWAY2br+wDjTmLfOU9ALypPvvBRTVX51RUVOjjjz/W+++/r7lz56q0tFSVlZUXuzrbIEgA39oPyCKgcfK1faAxZpGvvQeAN9RnP6jzZYHS96Hx6aefas2aNcrPz9emTZuUmJiovn37avbs2erXr98lFQ4AdUEWAbADsgjAD9W5ubrpppu0adMmJScnq1+/fvrlL3+p119/Xa1atfJmfQDggiwCYAdkEQB36txcrV+/Xq1atXJ+SLNfv3667LLLvFkbAFRDFgGwA7IIgDt1/hLhU6dO6aWXXlJYWJhmzJihhIQEde7cWePHj9fSpUt14sQJb9YJAJLIIgD2QBYBcOeib2hRUlKiDRs2OK8z/uKLL9S+fXvt3LnT0zU2OD68CfjOfkAWAY2XL+0DjTWLfOk9ALylPvtBnc9c/VDz5s3VsmVLtWzZUtHR0WrSpIl27959sasDgItCFgGwA7IIgFSPz1xVVVXpH//4h/Lz87VmzRp9/PHHKisrU+vWrTVgwADNmTNHAwYM8GatAEAWAbAFsgiAO3VurqKiolRWVqb4+HgNGDBAzz33nPr3768rr7zSm/UBgAuyCIAdkEUA3KlzczVz5kwNGDBAV199tTfrAYBakUUA7IAsAuBOnZurX/7yl96sAwDqhCwCYAdkEQB3LvqGFgAAAACA/6K5AgAAAAAPoLkCAAAAAA+guQIASXPmzFFSUpJCQ0OVmpqqzZs31zh23rx5uvHGGxUdHa3o6GilpaVVGz969GgFBAS4PG6++WZvbwYAH0cWAb7NFs0VQQLASkuWLFFmZqamTJmirVu3qmvXrkpPT9fx48fdjs/Pz9fw4cO1Zs0abdy4UYmJiRo0aJC++eYbl3E333yzjh075ny88cYbDbE5AHwUWQT4PsubK4IEgNVmzZqlsWPHasyYMerYsaNyc3MVFham+fPnux3/2muv6de//rVSUlLUoUMH/fnPf1ZVVZXy8vJcxoWEhCg+Pt75iI6ObojNAeCjyCLA91neXBEkAKxUUVGhLVu2KC0tzTktMDBQaWlp2rhxY53Wcfr0aZ09e1YtW7Z0mZ6fn6/Y2Fhdc801GjdunE6ePFnresrLy1VcXOzyAOAf7JJF5BBwaSxtrggSAFYrLCxUZWWl4uLiXKbHxcXJ4XDUaR2PPfaYEhISXLLs5ptv1qJFi5SXl6cZM2Zo7dq1uuWWW1RZWVnjerKzsxUZGel8JCYmXtxGAfA5dskicgi4NHX+EmFvqC1I9uzZU6d11BQkP/3pT5WcnKwDBw7od7/7nW655RZt3LhRQUFB1daRnZ2tqVOnXtrGAPBL06dP1+LFi5Wfn6/Q0FDn9Lvuusv5/507d1aXLl105ZVXKj8/XwMHDnS7rqysLGVmZjp/Li4u5g8bAHXiqSwih4BLY2lzdakIEgCXKiYmRkFBQSooKHCZXlBQoPj4+FqXfeaZZzR9+nT9/e9/V5cuXWod265dO8XExGj//v01NlchISEKCQmp3wYAaBTskkXkEHBpLL0s0BNB8uGHH9YrSNwJCQlRRESEywOAfwgODlb37t1dPrd57nOcvXr1qnG5p59+Wk888YRWrVqlHj16XPB5/vWvf+nkyZNq1aqVR+oG0LiQRUDjYGlzRZAAsIPMzEzNmzdPr7zyinbv3q1x48aprKxMY8aMkSSNHDlSWVlZzvEzZszQpEmTNH/+fCUlJcnhcMjhcKi0tFSSVFpaqv/3//6fPv30Ux06dEh5eXkaMmSIrrrqKqWnp1uyjQDsjywCGgFjscWLF5uQkBCzcOFC889//tM88MADJioqyjgcDmOMMSNGjDATJ050jp8+fboJDg42S5cuNceOHXM+SkpKjDHGlJSUmEcffdRs3LjRHDx40Pz973831113nWnfvr05c+ZMnWoqKioykkxRUZHnNxjwEf62H7zwwgvmiiuuMMHBwaZnz57m008/dc7r16+fGTVqlPPntm3bGknVHlOmTDHGGHP69GkzaNAgc/nll5umTZuatm3bmrFjxzpzra787T0Afsgf9wG7ZZE/vgfAD9VnPwgwxpiGb+lczZ49WzNnzpTD4VBKSor++Mc/KjU1VZLUv39/JSUlaeHChZKkpKQkff3119XWMWXKFD3++OP67rvvNHToUH3++ec6deqUEhISNGjQID3xxBPVbpxRk+LiYkVGRqqoqIhLBOG32A+sx3sAf8c+YD3eA6B++4Etmiu7IUgA9gM74D2Av2MfsB7vAVC//cDyLxEGAAAAgMaA5goAAAAAPIDmCgAAAAA8gOYKAAAAADyA5goAAAAAPIDmCgAAAAA8gOYKAAAAADyA5goAAAAAPIDmCgAAAAA8gOYKAAAAADyA5goAAAAAPIDmCgAAAAA8gOYKAAAAADyA5goAAAAAPIDmCgAAAAA8gOYKAAAAADyA5goAAAAAPIDmCgAAAAA8gOYKAAAAADyA5goAAAAAPKCJ1QUAAAD/U3S6QoWlFSo+c1YRzZoqpnmwIsOCrS4LAC4JzRUAAGhQR099p8eWbdf6fYXOaX3bx2j6HV2UENXMwsoA4NJwWSAAAGgwRacrqjVWkrRuX6EmLtuuotMVFlUGAJeOM1cAgEaFy83srbC0olpjdc66fYUqLK3g/QLQILzx74UtzlzNmTNHSUlJCg0NVWpqqjZv3lzr+L/97W/q0KGDQkND1blzZ7333nsu840xmjx5slq1aqVmzZopLS1N+/bt8+YmWKbodIUOHC/V54f/rQMnSi094menWuxWD7XYHzl0aezye3X01Hca/8bnGjhrrf73T59o4LNr9eAbn+voqe8sqccur4udaik+c7bW+SUXmN/YkUUXzy6/43ash1qq89a/F5afuVqyZIkyMzOVm5ur1NRU5eTkKD09XXv37lVsbGy18Z988omGDx+u7Oxs3XbbbXr99dc1dOhQbd26VZ06dZIkPf300/rjH/+oV155RcnJyZo0aZLS09P1z3/+U6GhoQ29iV5jp2vW7VSL3eqhFvsjhy6NXX6vLnS52QvDuzXoGRG7vC52qyUitGmt88MvML8xI4sunp1+x+1WD7VU581/LwKMMcYTRV6s1NRUXX/99Zo9e7YkqaqqSomJiXrwwQc1ceLEauOHDRumsrIyvfvuu85p//M//6OUlBTl5ubKGKOEhAQ98sgjevTRRyVJRUVFiouL08KFC3XXXXddsKbi4mJFRkaqqKhIERERHtpSzyo6XaHxb3zu9tKKvu1jGvSPCDvVYrd6fLkWX9gPPMWOOST5xntgp9/xA8dLNXDW2hrn52X205WxLRqkFju9Lnaq5Vw9D77xudbVoR5f2Ac8yY5Z5AvvgR1/x+1SD7W4V99/L+qzH1h6WWBFRYW2bNmitLQ057TAwEClpaVp48aNbpfZuHGjy3hJSk9Pd44/ePCgHA6Hy5jIyEilpqbWuM7y8nIVFxe7POyuLtes+2MtdquHWuzPLjkkkUWXyk6Xm9npdbFTLZIUGRas6Xd0Ud/2MS7T+7aP0Yw7uvjt563skkXkUOOqh1rc8+a/F5ZeFlhYWKjKykrFxcW5TI+Li9OePXvcLuNwONyOdzgczvnnptU05oeys7M1derUi9oGq9jpjwg71SLZqx5qsT+75JBEFl0qO11uZqfXxU61nJMQ1UwvDO+mwtIKlZw5q/DQpopp4d83HrFLFpFDl85O9VCLe97898IWN7SwWlZWloqKipyPI0eOWF3SBdnpjwg71SLZqx5qQX2QRZcmpkVwtbMh5/RtH6OYFg33h7udXhc71XK+yLBgXRnbQilXROvK2BZ+3VjZCTl06exUD7W4581/LyxtrmJiYhQUFKSCggKX6QUFBYqPj3e7THx8fK3jz/23PusMCQlRRESEy8Pu7PRHhJ1qsVs91GJ/dskhiSy6VHa63MxOr4udakHN7JJF5FDjqoda3PPmvxeWNlfBwcHq3r278vLynNOqqqqUl5enXr16uV2mV69eLuMlafXq1c7xycnJio+PdxlTXFysTZs21bhOX2SnPyLsVIvd6qEW+yOHLo3dfq/OXW6Wl9lPb/66t/Iy++mF4d3UqoHviGWn18VOtaBmZNHFs9vvuJ3qoZaaee3fC2OxxYsXm5CQELNw4ULzz3/+0zzwwAMmKirKOBwOY4wxI0aMMBMnTnSO//jjj02TJk3MM888Y3bv3m2mTJlimjZtanbs2OEcM336dBMVFWXeeusts337djNkyBCTnJxsvvvuuzrVVFRUZCSZoqIiz26sF5wqKzf7C0rM519/a/YXlJhTZeXUYsN6fLEWX9oPLpUdc8gY33oP7PQ7bid2el3sVEtd+dI+4Al2zCJfeg/s9jtup3qo5dLUZz+wvLkyxpgXXnjBXHHFFSY4ONj07NnTfPrpp855/fr1M6NGjXIZ/9e//tVcffXVJjg42PzoRz8yK1eudJlfVVVlJk2aZOLi4kxISIgZOHCg2bt3b53r8aUgAbzF3/YDu+WQMf73HgA/5I/7gN2yyB/fA+CH6rMfWP49V3ZUVFSkqKgoHTlyxCeuNQa8obi4WImJiTp16pQiIyOtLscvkUXwd+SQ9cghoH5ZZOmt2O2qpKREkpSYmGhxJYD1SkpK+KPGImQR8D1yyDrkEPBfdckizly5UVVVpaNHjyo8PFwBAQFux5zrYDmS44rXpWa+9toYY1RSUqKEhAQFBvKtDVa4UBb52u9UQ+K1cc/XXhdyyHr8TXRpeG3c87XXpT5ZxJkrNwIDA9WmTZs6jfWV25Q2NF6XmvnSa8ORYmvVNYt86XeqofHauOdLrws5ZC3+JvIMXhv3fOl1qWsWcRgIAAAAADyA5goAAAAAPIDm6iKFhIRoypQpCgkJsboUW+F1qRmvDTyN36ma8dq4x+sCb+D3qma8Nu415teFG1oAAAAAgAdw5goAAAAAPIDmCgAAAAA8gOYKAAAAADyA5goAAAAAPIDm6iLMmTNHSUlJCg0NVWpqqjZv3mx1SZbLzs7W9ddfr/DwcMXGxmro0KHau3ev1WXZzvTp0xUQEKCHH37Y6lLQCJBF1ZFFdUMWwZPIIlfkUN001hyiuaqnJUuWKDMzU1OmTNHWrVvVtWtXpaen6/jx41aXZqm1a9cqIyNDn376qVavXq2zZ89q0KBBKisrs7o02/jss880d+5cdenSxepS0AiQRe6RRRdGFsGTyKLqyKELa9Q5ZFAvPXv2NBkZGc6fKysrTUJCgsnOzrawKvs5fvy4kWTWrl1rdSm2UFJSYtq3b29Wr15t+vXrZyZMmGB1SfBxZFHdkEWuyCJ4Gll0YeSQq8aeQ5y5qoeKigpt2bJFaWlpzmmBgYFKS0vTxo0bLazMfoqKiiRJLVu2tLgSe8jIyNCtt97q8rsDXCyyqO7IIldkETyJLKobcshVY8+hJlYX4EsKCwtVWVmpuLg4l+lxcXHas2ePRVXZT1VVlR5++GH16dNHnTp1srocyy1evFhbt27VZ599ZnUpaCTIorohi1yRRfA0sujCyCFX/pBDNFfwuIyMDO3cuVMbNmywuhTLHTlyRBMmTNDq1asVGhpqdTmAXyGL/ossAqxBDv2Xv+QQzVU9xMTEKCgoSAUFBS7TCwoKFB8fb1FV9jJ+/Hi9++67Wrdundq0aWN1OZbbsmWLjh8/ruuuu845rbKyUuvWrdPs2bNVXl6uoKAgCyuELyKLLowsckUWwRvIotqRQ678JYf4zFU9BAcHq3v37srLy3NOq6qqUl5ennr16mVhZdYzxmj8+PFasWKFPvroIyUnJ1tdki0MHDhQO3bs0LZt25yPHj166J577tG2bdsaRYig4ZFFNSOL3COL4A1kkXvkkHv+kkOcuaqnzMxMjRo1Sj169FDPnj2Vk5OjsrIyjRkzxurSLJWRkaHXX39db731lsLDw+VwOCRJkZGRatasmcXVWSc8PLzaNdbNmzfXZZddxrXXuCRkkXtkkXtkEbyFLKqOHHLPX3KI5qqehg0bphMnTmjy5MlyOBxKSUnRqlWrqn2Y09+8+OKLkqT+/fu7TF+wYIFGjx7d8AUBjRxZ5B5ZBDQssqg6csi/BRhjjNVFAAAAAICv4zNXAAAAAOABNFcAAAAA4AE0VwAAAADgATRXAAAAAOABNFcAAAAA4AE0VwAAAADgATRXAAAAAOABNFcAAAAA4AE0VwAAAADgATRX8KrRo0dr6NChLtOWLl2q0NBQPfvss9YUBcDvkEUArEYO+YcmVhcA//LnP/9ZGRkZys3N1ZgxY6wuB4CfIosAWI0capw4c4UG8/TTT+vBBx/U4sWLnSHy1ltv6brrrlNoaKjatWunqVOn6j//+Y8k6Re/+IVuu+02l3WcPXtWsbGxevnllyV9f8Snc+fOatasmS677DKlpaWprKysYTcMgE8hiwBYjRxqxAzgRaNGjTJDhgwxv/3tb02LFi3M3//+d+e8devWmYiICLNw4UJz4MAB8+GHH5qkpCTz+OOPG2OM+fjjj01QUJA5evSoc5nly5eb5s2bm5KSEnP06FHTpEkTM2vWLHPw4EGzfft2M2fOHFNSUtLg2wnA3sgiAFYjh/wDzRW8atSoUSY4ONhIMnl5eS7zBg4caKZNm+Yy7S9/+Ytp1aqV8+eOHTuaGTNmOH++/fbbzejRo40xxmzZssVIMocOHfLiFgBoDMgiAFYjh/xDgDHGWHnmDI3b6NGjtWvXLhUWFqpNmzZ6//331aJFC0nS5ZdfrtLSUgUFBTnHV1ZW6syZMyorK1NYWJiee+45vfTSS9q9e7cKCgrUpk0bffTRR7rxxhtVWVmp9PR0bd68Wenp6Ro0aJDuvPNORUdHW7W5AGyKLAJgNXLIP9BcwatGjx6tU6dO6fnnn9eAAQOUkJCg999/X+Hh4WrWrJmmTp2qn/70p9WWa9eunQIDA3Xy5EklJCQoPz9fn3zyiebOnasvv/zSOc4Yo08++UQffvihVqxYIYfDoU2bNik5ObkhNxOAzZFFAKxGDvkHbmiBBtG2bVutXbtWDodDN998s0pKSnTddddp7969uuqqq6o9AgO//9W87LLLNHToUC1YsEALFy6sdjedgIAA9enTR1OnTtXnn3+u4OBgrVixwopNBOADyCIAViOHGjduxY4Gk5iYqPz8fA0YMEDp6el67LHHdOedd+qKK67QnXfeqcDAQH3xxRfauXOnnnzySedy999/v2677TZVVlZq1KhRzumbNm1SXl6eBg0apNjYWG3atEknTpzQtddea8XmAfARZBEAq5FDjRfNFRpUmzZtnGEyffp0LV26VE8//bRmzJihpk2bqkOHDrr//vtdlklLS1OrVq30ox/9SAkJCc7pERERWrdunXJyclRcXKy2bdvq2Wef1S233NLQmwXAx5BFAKxGDjVOfOYKtldaWqrWrVtrwYIFbq9FBoCGQBYBsBo5ZH+cuYJtVVVVqbCwUM8++6yioqI0ePBgq0sC4IfIIgBWI4d8B80VbOvw4cNKTk5WmzZttHDhQjVpwq8rgIZHFgGwGjnkO7gsEAAAAAA8gFuxAwAAAIAH0FwBAAAAgAfQXAEAAACAB9BcAQAAAIAH0FwBAAAAgAfQXAEAAACAB9BcAQAAAIAH0FwBAAAAgAfQXAEAAACAB9BcAQAAAIAH0FwBAAAAgAfQXOGCdu3apXvvvVetW7dWSEiIEhISdO+99+qf//yn1aVdkhUrVig9PV0JCQkKCQlRmzZtdOedd2rnzp1WlwbgBxprDv3Qj3/8YwUEBGj8+PFWlwLAjcaaRY8//rgCAgKqPUJDQ60uzec0sboA2Nvy5cs1fPhwtWzZUvfdd5+Sk5N16NAhvfzyy1q6dKmWLFmiIUOGWF3mRdmxY4eio6M1YcIExcTEyOFwaP78+erZs6c2btyorl27Wl0iADXuHDrf8uXLtXHjRqvLAFADf8iiF198US1atHD+HBQUZGE1vinAGGOsLgL2dODAAXXp0kVXXHGF1q1bp8svv9w5r7CwUDfeeKP+9a9/afv27UpOTm7Q2k6fPq2wsDCPr7egoEBt2rTRfffdp9zcXI+vH0D9+EsOnTlzRtdee61+8YtfaPLkycrIyNDs2bM9sm4Al66xZ9Hjjz+uqVOn6sSJE4qJifFQZf6JywJRo5kzZ+r06dN66aWXXEJEkmJiYjR37lyVlpZq5syZzumjR49WUlJStXWdO938Q6+++qq6d++uZs2aqWXLlrrrrrt05MgRlzH9+/dXp06dtGXLFvXt21dhYWH63e9+p1GjRikmJkZnz56ttt5Bgwbpmmuuqfc2x8bGKiwsTKdOnar3sgA8z19y6Omnn1ZVVZUeffTROo0H0LD8JYuMMSouLhbnXi4ezRVq9M477ygpKUk33nij2/l9+/ZVUlKS3nnnnYta/1NPPaWRI0eqffv2mjVrlh5++GHl5eWpb9++1ZqbkydP6pZbblFKSopycnI0YMAAjRgxQidPntQHH3zgMtbhcOijjz7SvffeW6c6Tp06pRMnTmjHjh26//77VVxcrIEDB17UNgHwLH/IocOHD2v69OmaMWOGmjVrdlHbAcC7/CGLJKldu3aKjIxUeHi47r33XhUUFFzU9vg1A7hx6tQpI8kMGTKk1nGDBw82kkxxcbExxphRo0aZtm3bVhs3ZcoUc/6v26FDh0xQUJB56qmnXMbt2LHDNGnSxGV6v379jCSTm5vrMraystK0adPGDBs2zGX6rFmzTEBAgPnqq6/qsqnmmmuuMZKMJNOiRQvz+9//3lRWVtZpWQDe4y85dOedd5revXs7f5ZkMjIyLrgcgIbhD1mUk5Njxo8fb1577TWzdOlSM2HCBNOkSRPTvn17U1RUVOuycMWZK7hVUlIiSQoPD6913Ln558bX1fLly1VVVaWf//znKiwsdD7i4+PVvn17rVmzxmV8SEiIxowZ4zItMDBQ99xzj95++22X53/ttdfUu3fvOl/zvGDBAq1atUp/+tOfdO211+q7775TZWVlvbYHgOf5Qw6tWbNGy5YtU05OTr1qB9Bw/CGLJkyYoBdeeEF333237rjjDuXk5OiVV17Rvn379Kc//ale2+PvaK7gVl0DoqSkRAEBAfX+8OO+fftkjFH79u11+eWXuzx2796t48ePu4xv3bq1goODq61n5MiR+u6777RixQpJ0t69e7VlyxaNGDGizrX06tVL6enpGjdunD744AO9+uqrysrKqtf2APC8xp5D//nPf/TQQw9pxIgRuv766+tVO4CG09izqCZ333234uPj9fe///2ilvdX3IodbkVGRiohIUHbt2+vddz27dvVpk0b507u7gOakqqdCaqqqlJAQIDef/99t7f5PP82oJJq/BxCx44d1b17d7366qsaOXKkXn31VQUHB+vnP/95rXXXJDo6WjfddJNee+01PfPMMxe1DgCe0dhzaNGiRdq7d6/mzp2rQ4cOucwrKSnRoUOHnDfZAWCdxp5FtUlMTNS333570cv7I5or1Oj222/X3LlztWHDBt1www3V5q9fv16HDh1SZmamc1p0dLTbO+19/fXXLj9feeWVMsYoOTlZV1999SXVOXLkSGVmZurYsWN6/fXXdeuttyo6Ovqi1/fdd9+pqKjokmoC4BmNOYcOHz6ss2fPqk+fPtXmLVq0SIsWLdKKFSs0dOjQS6oNwKVrzFlUE2OMDh06pG7dul1STX7Hws97web27dtnwsLCTMeOHU1hYaHLvJMnT5qOHTuaiIgIlw9Jzp4920gyX3zxhXPa0aNHTYsWLVw+vLl//34TFBRk7r77blNVVeWy7qqqKpfn69evn/nRj35UY53Hjx83TZo0MT/72c+MJLNs2bI6bV9BQUG1aQcPHjTh4eHmxhtvrNM6AHhXY86h3bt3mxUrVlR7SDI/+clPzIoVK8zRo0cvuB4A3teYs+jccj80Z84cI8nMmjWrTuvA9/gSYdRq2bJlGj58uGJiYqp9G/m///1vLV68WIMHD3aOP3nypNq2bau4uDg99NBDOn36tF588UVdfvnl2rp1q8v3JkyfPl1ZWVnq3bu3hg4dqvDwcB08eFArVqzQAw884Py+l/79+6uwsFA7d+6ssc7bb79d7777rqKiouRwOBQSEnLBbYuLi9PAgQOVkpKi6Oho7du3Ty+//LJOnz6tvLw89e7d+xJeOQCe0phzyJ2AgAC+RBiwocacRWFhYRo2bJg6d+6s0NBQbdiwQYsXL1bXrl318ccfc3lyfVja2sEn7Nixw9x9990mPj7eBAYGGkkmNDTU7Nq1y+34Dz/80HTq1MkEBweba665xrz66qvVbjt6zrJly8wNN9xgmjdvbpo3b246dOhgMjIyzN69e51jLnSUxhhj/vrXvxpJ5oEHHqjzdk2ZMsX06NHDREdHmyZNmpiEhARz1113me3bt9d5HQAaRmPNIXfErdgB22qsWXT//febjh07mvDwcNO0aVNz1VVXmccee8x5W3nUHWeuUG+LFi3S6NGjde+992rRokVWlyNJeuuttzR06FCtW7euxi/4A9B4kEMA7IAswg9xQwvU28iRI3Xs2DFNnDhRbdq00bRp06wuSfPmzVO7du3cfsgUQONDDgGwA7IIP8SZK/i0xYsXa/v27crOztbzzz+vhx56yOqSAPgZcgiAHZBF9kBzBZ8WEBCgFi1aaNiwYcrNzVWTJpyMBdCwyCEAdkAW2QPNFQAAAAB4QKDVBQAAAABAY0BzBQAAAAAewMWYblRVVeno0aMKDw9XQECA1eUAljDGqKSkRAkJCQoM5DiMFcgi+DtyyHrkEFC/LKK5cuPo0aNKTEy0ugzAFo4cOaI2bdpYXYZfIouA75FD1iGHgP+qSxbRXLkRHh4u6fsXMCIiwuJqAGsUFxcrMTHRuT+g4ZFF8HfkkPXIIaB+WURz5ca5094REREECfwel4FYhywCvkcOWYccAv6rLllEc3URik5XqLC0QsVnziqiWVPFNA9WZFiw1WUB8CPkEAA7IIsAVzRX9XT01Hd6bNl2rd9X6JzWt32Mpt/RRQlRzSysDIC/IIcA2AFZBFTHrXfqoeh0RbUQkaR1+wo1cdl2FZ2usKgyAP6CHAJgB2QR4B7NVT0UllZUC5Fz1u0rVGEpQQLAu8ghAHZAFgHu0VzVQ/GZs7XOL7nAfAC4VOQQADsgiwD3aK7qISK0aa3zwy8wHwAuFTkEwA7IIsA9mqt6iGkRrL7tY9zO69s+RjEtuDsOAO8ihwDYAVkEuEdzVQ+RYcGafkeXamHSt32MZtzRhVuPAvA6cgiAHZBFgHvcir2eEqKa6YXh3VRYWqGSM2cVHtpUMS34TgcADYccAmAHZBFQHc3VRYgMIzgAWIscAmAHZBHgissCAQAAAMADaK4AAAB8xJw5c5SUlKTQ0FClpqZq8+bNtY7PycnRNddco2bNmikxMVG/+c1vdObMmQaqFvA/NFcAAAA+YMmSJcrMzNSUKVO0detWde3aVenp6Tp+/Ljb8a+//romTpyoKVOmaPfu3Xr55Ze1ZMkS/e53v2vgygH/QXMFAADgA2bNmqWxY8dqzJgx6tixo3JzcxUWFqb58+e7Hf/JJ5+oT58+uvvuu5WUlKRBgwZp+PDhFzzbBeDi0VwBAADYXEVFhbZs2aK0tDTntMDAQKWlpWnjxo1ul+ndu7e2bNnibKa++uorvffee/rJT35S4/OUl5eruLjY5QGg7rhbIAAAgM0VFhaqsrJScXFxLtPj4uK0Z88et8vcfffdKiws1A033CBjjP7zn//oV7/6Va2XBWZnZ2vq1KkerR3wJ5y5AgAAaITy8/M1bdo0/elPf9LWrVu1fPlyrVy5Uk888USNy2RlZamoqMj5OHLkSANWDPg+zlwBAADYXExMjIKCglRQUOAyvaCgQPHx8W6XmTRpkkaMGKH7779fktS5c2eVlZXpgQce0P/93/8pMLD6MfaQkBCFhIR4fgMAP8GZKwAAAJsLDg5W9+7dlZeX55xWVVWlvLw89erVy+0yp0+frtZABQUFSZKMMd4rFvBjnLkCAADwAZmZmRo1apR69Oihnj17KicnR2VlZRozZowkaeTIkWrdurWys7MlSbfffrtmzZqlbt26KTU1Vfv379ekSZN0++23O5ssAJ5FcwUAAOADhg0bphMnTmjy5MlyOBxKSUnRqlWrnDe5OHz4sMuZqt///vcKCAjQ73//e33zzTe6/PLLdfvtt+upp56yahOARi/AcF64muLiYkVGRqqoqEgRERFWlwNYgv3AerwH8HfsA9bjPQDqtx/wmSsAAAAA8ACaKwAAAADwAJorAAAAAPAAmisAAAAA8ACaKwAAAADwAJ9orubMmaOkpCSFhoYqNTVVmzdvrnX8qVOnlJGRoVatWikkJERXX3213nvvvQaqFgAAAIA/sn1ztWTJEmVmZmrKlCnaunWrunbtqvT0dB0/ftzt+IqKCv34xz/WoUOHtHTpUu3du1fz5s1T69atG7hyAI0NB3oAAEBtbP8lwrNmzdLYsWOd3z6em5urlStXav78+Zo4cWK18fPnz9e3336rTz75RE2bNpUkJSUlNWTJABqhcwd6cnNzlZqaqpycHKWnp2vv3r2KjY2tNv7cgZ7Y2FgtXbpUrVu31tdff62oqKiGLx4AADQIW5+5qqio0JYtW5SWluacFhgYqLS0NG3cuNHtMm+//bZ69eqljIwMxcXFqVOnTpo2bZoqKytrfJ7y8nIVFxe7PADgfOcf6OnYsaNyc3MVFham+fPnux1/7kDPm2++qT59+igpKUn9+vVT165dG7hyAADQUGzdXBUWFqqyslJxcXEu0+Pi4uRwONwu89VXX2np0qWqrKzUe++9p0mTJunZZ5/Vk08+WePzZGdnKzIy0vlITEz06HYA8G0c6AEAAHVh6+bqYlRVVSk2NlYvvfSSunfvrmHDhun//u//lJubW+MyWVlZKioqcj6OHDnSgBUDsDsO9AAAgLqw9WeuYmJiFBQUpIKCApfpBQUFio+Pd7tMq1at1LRpUwUFBTmnXXvttXI4HKqoqFBwcHC1ZUJCQhQSEuLZ4gH4tfMP9AQFBal79+765ptvNHPmTE2ZMsXtMllZWcrMzHT+XFxcTIMFAIAPsfWZq+DgYHXv3l15eXnOaVVVVcrLy1OvXr3cLtOnTx/t379fVVVVzmlffvmlWrVq5baxAoALudgDPVdffXWNB3rcCQkJUUREhMsDAAD4Dls3V5KUmZmpefPm6ZVXXtHu3bs1btw4lZWVOe8eOHLkSGVlZTnHjxs3Tt9++60mTJigL7/8UitXrtS0adOUkZFh1SYA8HEc6AEAAHVh68sCJWnYsGE6ceKEJk+eLIfDoZSUFK1atcr52YfDhw8rMPC/PWJiYqI++OAD/eY3v1GXLl3UunVrTZgwQY899phVmwCgEcjMzNSoUaPUo0cP9ezZUzk5OdUO9LRu3VrZ2dmSvj/QM3v2bE2YMEEPPvig9u3bp2nTpumhhx6ycjMAAIAX2b65kqTx48dr/Pjxbufl5+dXm9arVy99+umnXq4KgD/hQA8AALiQAGOMsboIuykuLlZkZKSKior4zAP8FvuB9XgP4O/YB6zHewDUbz+w/WeuAAAAAMAX0FwBAAAAgAfQXAEAAACAB9BcAQAAAIAH0FwBAAD4iDlz5igpKUmhoaFKTU3V5s2bax1/6tQpZWRkqFWrVgoJCdHVV1+t9957r4GqBfyPT9yKHQAAwN8tWbJEmZmZys3NVWpqqnJycpSenq69e/cqNja22viKigr9+Mc/VmxsrJYuXarWrVvr66+/VlRUVMMXD/gJmisAAAAfMGvWLI0dO9b55eW5ublauXKl5s+fr4kTJ1YbP3/+fH377bf65JNP1LRpU0lSUlJSQ5YM+B0uCwQAALC5iooKbdmyRWlpac5pgYGBSktL08aNG90u8/bbb6tXr17KyMhQXFycOnXqpGnTpqmysrLG5ykvL1dxcbHLA0Dd0VwBAADYXGFhoSorKxUXF+cyPS4uTg6Hw+0yX331lZYuXarKykq99957mjRpkp599lk9+eSTNT5Pdna2IiMjnY/ExESPbgfQ2NFcAQAANEJVVVWKjY3VSy+9pO7du2vYsGH6v//7P+Xm5ta4TFZWloqKipyPI0eONGDFgO/jM1cAAAA2FxMTo6CgIBUUFLhMLygoUHx8vNtlWrVqpaZNmyooKMg57dprr5XD4VBFRYWCg4OrLRMSEqKQkBDPFg/4Ec5cAQAA2FxwcLC6d++uvLw857Sqqirl5eWpV69ebpfp06eP9u/fr6qqKue0L7/8Uq1atXLbWAG4dDRXAAAAPiAzM1Pz5s3TK6+8ot27d2vcuHEqKytz3j1w5MiRysrKco4fN26cvv32W02YMEFffvmlVq5cqWnTpikjI8OqTQAaPa80V4cPH5Yxptp0Y4wOHz7sjacEgGrIIgBW82QODRs2TM8884wmT56slJQUbdu2TatWrXLe5OLw4cM6duyYc3xiYqI++OADffbZZ+rSpYseeughTZgwwe1t2wF4RoBxt8dfoqCgIB07dqzaF9qdPHlSsbGxtd4C1A6Ki4sVGRmpoqIiRUREWF0OYInGsB+QRYBvawz7ADkE+L767AdeOXNljFFAQEC16aWlpQoNDfXGUwJANWQRAKuRQ4B/8ejdAjMzMyVJAQEBmjRpksLCwpzzKisrtWnTJqWkpHjyKQGgGrIIgNXIIcA/ebS5+vzzzyV9f5Rmx44dLneiCQ4OVteuXfXoo4968ikBoBqyCIDVyCHAP3m0uVqzZo0kacyYMXr++ee5NheAJcgiAFYjhwD/5JUvEV6wYIE3VgsA9UIWAbAaOQT4F680V2VlZZo+fbry8vJ0/Phxly+vk6SvvvrKG08LAC7IIgBWI4cA/+KV5ur+++/X2rVrNWLECLVq1crtXXIAwNvIIgBWI4cA/+KV5ur999/XypUr1adPH2+sHgDqhCwCYDVyCPAvXvmeq+joaLVs2dIbqwaAOiOLAFiNHAL8i1eaqyeeeEKTJ0/W6dOnvbF6AKgTsgiA1cghwL947LLAbt26uVxHvH//fsXFxSkpKUlNmzZ1Gbt161ZPPS0AuCCLAFiNHAL8l8eaq6FDh3pqVdXMmTNHM2fOlMPhUNeuXfXCCy+oZ8+eF1xu8eLFGj58uIYMGaI333zTa/UBsA+yCIDVvJlDAOwtwBhjrC6iNkuWLNHIkSOVm5ur1NRU5eTk6G9/+5v27t2r2NjYGpc7dOiQbrjhBrVr104tW7as1x80xcXFioyMVFFREV/6B7/FfuCKLAIaHvuA9XgPgPrtB175zJUnzZo1S2PHjtWYMWPUsWNH5ebmKiwsTPPnz69xmcrKSt1zzz2aOnWq2rVr14DVAmisyCIAAHAhXrkVe3R0tNvvcQgICFBoaKiuuuoqjR49WmPGjKl1PRUVFdqyZYuysrKc0wIDA5WWlqaNGzfWuNwf/vAHxcbG6r777tP69esvWG95ebnKy8udPxcXF19wGQD2RxYBsJqncgiAb/BKczV58mQ99dRTuuWWW5yfR9i8ebNWrVqljIwMHTx4UOPGjdN//vMfjR07tsb1FBYWqrKyUnFxcS7T4+LitGfPHrfLbNiwQS+//LK2bdtW53qzs7M1derUOo8H4BvIIgBW81QOAfANXmmuNmzYoCeffFK/+tWvXKbPnTtXH374oZYtW6YuXbroj3/8o0eDpKSkRCNGjNC8efMUExNT5+WysrKUmZnp/Lm4uFiJiYkeqwuANcgiAFazKocAWMMrn7n64IMPlJaWVm36wIED9cEHH0iSfvKTn+irr76qdT0xMTEKCgpSQUGBy/SCggLFx8dXG3/gwAEdOnRIt99+u5o0aaImTZpo0aJFevvtt9WkSRMdOHDA7fOEhIQoIiLC5QHA95FFAKzmqRwC4Bu80ly1bNlS77zzTrXp77zzjvNbysvKyhQeHl7reoKDg9W9e3fl5eU5p1VVVSkvL0+9evWqNr5Dhw7asWOHtm3b5nwMHjxYAwYM0LZt2zgCDPgZsgiA1TyVQwB8g1cuC5w0aZLGjRunNWvWOK8v/uyzz/Tee+8pNzdXkrR69Wr169fvguvKzMzUqFGj1KNHD/Xs2VM5OTkqKytzfvBz5MiRat26tbKzsxUaGqpOnTq5LB8VFSVJ1aYDaPzIIgBW82QOAbA/rzRXY8eOVceOHTV79mwtX75cknTNNddo7dq16t27tyTpkUceqdO6hg0bphMnTmjy5MlyOBxKSUnRqlWrnB8sP3z4sAIDbX9HeQAWIIsAWM2TOQTA/mz/JcJW4AvzAPYDO+A9gL9jH3Bvzpw5mjlzphwOh7p27aoXXnjBeVasNosXL9bw4cM1ZMiQOn+hOe8BUL/9wGNnroqLi51PdqHvZmHnBOAtZBEAq3kzh5YsWaLMzEzl5uYqNTVVOTk5Sk9P1969exUbG1vjcocOHdKjjz6qG2+8sV7PB6B+PNZcRUdH69ixY4qNjVVUVJTbL8wzxiggIECVlZWeeloAcEEWAbCaN3No1qxZGjt2rPPznrm5uVq5cqXmz5+viRMnul2msrJS99xzj6ZOnar169fr1KlT9d4mAHXjsebqo48+ct71Zs2aNZ5aLQDUC1kEwGreyqGKigpt2bJFWVlZzmmBgYFKS0vTxo0ba1zuD3/4g2JjY3Xfffdp/fr1tT5HeXm5ysvLnT9f6MwbAFcea67Ov8sNd7wBYBWyCIDVvJVDhYWFqqysdN5I55y4uDjt2bPH7TIbNmzQyy+/rG3bttXpObKzszV16tRLLRXwW167tdX69et17733qnfv3vrmm28kSX/5y1+0YcMGbz0lAFRDFgGwmlU5VFJSohEjRmjevHmKiYmp0zJZWVkqKipyPo4cOeLVGoHGxivN1bJly5Senq5mzZpp69atztPLRUVFmjZtmjeeEgCqIYsAWM2TORQTE6OgoCAVFBS4TC8oKFB8fHy18QcOHNChQ4d0++23q0mTJmrSpIkWLVqkt99+W02aNNGBAweqLRMSEqKIiAiXB4C680pz9eSTTyo3N1fz5s1T06ZNndP79OmjrVu3euMpAaAasgiA1TyZQ8HBwerevbvy8vKc06qqqpSXl6devXpVG9+hQwft2LFD27Ztcz4GDx6sAQMGaNu2bUpMTLz4DQPglle+RHjv3r3q27dvtemRkZHcoQZAgyGLAFjN0zmUmZmpUaNGqUePHurZs6dycnJUVlbmvHvgyJEj1bp1a2VnZys0NFSdOnVyWT4qKkqSqk0H4Bleaa7i4+O1f/9+JSUluUzfsGGD2rVr542nBIBqyCIAVvN0Dg0bNkwnTpzQ5MmT5XA4lJKSolWrVjlvcnH48GEFBnrtI/UALsArzdXYsWM1YcIEzZ8/XwEBATp69Kg2btyoRx99VJMmTfLGUwJANWQRAKt5I4fGjx+v8ePHu52Xn59f67ILFy68qOcEUDceba4OHjyo5ORkTZw4UVVVVRo4cKBOnz6tvn37KiQkRI8++qgefPBBTz4lAFRDFgGwGjkE+CePNldXXnml2rZtqwEDBmjAgAHavXu3SkpKVFpaqo4dO6pFixaefDoAcIssAmA1cgjwTx5trj766CPl5+crPz9fb7zxhioqKtSuXTvddNNNuummm9S/f/9qX3wHAJ5GFgGwGjkE+KcAY4zxxorPnDmjTz75xBksmzdv1tmzZ9WhQwft2rXLG0/pMcXFxYqMjFRRURHf7wC/1Vj2A7II8F2NZR8ghwDfVp/9wGvN1TkVFRX6+OOP9f7772vu3LkqLS1VZWWlN5/ykhEkQOPbD8giwPc0tn2AHAJ8U332A4/fLbCiokKffvqp1qxZo/z8fG3atEmJiYnq27evZs+erX79+nn6KQGgGrIIgNXIIcD/eLS5uummm7Rp0yYlJyerX79++uUvf6nXX39drVq18uTTAECtyCIAViOHAP/k0eZq/fr1atWqlfODmv369dNll13myacAgAsiiwBYjRwC/JNHv8L71KlTeumllxQWFqYZM2YoISFBnTt31vjx47V06VKdOHHCk08HAG6RRQCsRg4B/smrN7QoKSnRhg0bnNcaf/HFF2rfvr127tzpraf0CD68CTSu/YAsAnxTY9oHyCHAd1l6Q4vzNW/eXC1btlTLli0VHR2tJk2aaPfu3d58SgCohiyCVYpOV6iwtELFZ84qollTxTQPVmRYsNVlwQLkEOAfPNpcVVVV6R//+Ify8/O1Zs0affzxxyorK1Pr1q01YMAAzZkzRwMGDPDkUwI+gT+wGhZZBDs4euo7PbZsu9bvK3RO69s+RtPv6KKEqGYWVoaGQA4B/smjzVVUVJTKysoUHx+vAQMG6LnnnlP//v115ZVXevJpAJ/CH1gNjyyC1YpOV1Tb7yVp3b5CTVy2XS8M78YBlkaOHAL8k0ebq5kzZ2rAgAG6+uqrPblawGfxB5Y1yCJYrbC0otp+f866fYUqLK1g32/kyCHAP3m0ufrlL3/pydUBPo8/sKxBFsFqxWfO1jq/5ALz4fvIIcA/efRW7ABc8QcW4J8iQpvWOj/8AvMBAL7JJ5qrOXPmKCkpSaGhoUpNTdXmzZtrHDtv3jzdeOONio6OVnR0tNLS0modD3gTf2AB/immRbD6to9xO69v+xjFtOCMNQA0RrZvrpYsWaLMzExNmTJFW7duVdeuXZWenq7jx4+7HZ+fn6/hw4drzZo12rhxoxITEzVo0CB98803DVw5wB9YjQ0HelBXkWHBmn5Hl2r7f9/2MZpxRxcuBwaARsqrXyLsCampqbr++us1e/ZsSd/f2jQxMVEPPvigJk6ceMHlKysrFR0drdmzZ2vkyJF1ek6+MA+edPTUd5q4bLvW/eBugTPu6KJWNr5bIPuBqyVLlmjkyJHKzc1VamqqcnJy9Le//U179+5VbGxstfH33HOP+vTpo969eys0NFQzZszQihUrtGvXLrVu3bpOz8l74PvOfQ1DyZmzCg9tqpgWfA1DfbAPWI/3ALDRlwhfqoqKCm3ZskVZWVnOaYGBgUpLS9PGjRvrtI7Tp0/r7NmzatmyZY1jysvLVV5e7vy5uLj44osGfiAhqpleGN6NP7B83KxZszR27FiNGTNGkpSbm6uVK1dq/vz5bg/0vPbaay4///nPf9ayZcuUl5dX44EesqjxiQxjXwcAf2LrywILCwtVWVmpuLg4l+lxcXFyOBx1Wsdjjz2mhIQEpaWl1TgmOztbkZGRzkdiYuIl1Q38UGRYsK6MbaGUK6J1ZWwL/tjyMecO9JyfI9440EMWAQDg22zdXF2q6dOna/HixVqxYoVCQ0NrHJeVlaWioiLn48iRIw1YJQC7a6gDPWQRAAC+zdbNVUxMjIKCglRQUOAyvaCgQPHx8bUu+8wzz2j69On68MMP1aVLl1rHhoSEKCIiwuUBAJ5S1wM9ZBGAuuDmOoB92bq5Cg4OVvfu3ZWXl+ecVlVVpby8PPXq1avG5Z5++mk98cQTWrVqlXr06NEQpQJoxBrqQA8AXAh3UQbszdbNlSRlZmZq3rx5euWVV7R7926NGzdOZWVlzg+Vjxw50uWGFzNmzNCkSZM0f/58JSUlyeFwyOFwqLS01KpNAODjONADwC7Ov7lOx44dlZubq7CwMM2fP9/t+Ndee02//vWvlZKSog4dOujPf/6zM7/cKS8vV3FxscsDQN3ZvrkaNmyYnnnmGU2ePFkpKSnatm2bVq1a5fzsw+HDh3Xs2DHn+BdffFEVFRW688471apVK+fjmWeesWoTADQCHOgBYLWGuLkON9YBLo2tb8V+zvjx4zV+/Hi38/Lz811+PnTokPcLAuB3hg0bphMnTmjy5MlyOBxKSUmpdqAnMPC/x6vOP9BzvilTpujxxx9vyNIBNBK13Vxnz549dVrHhW6uk5WVpczMTOfPxcXFNFhAPfhEcwUAdsCBHgC+7NzNdfLz82u8uU5ISIhCQkIauDKg8aC5AgAA8AGeuLnO3//+d26uA3iR7T9zBQAAAG6uA/gCzlwBAAD4iMzMTI0aNUo9evRQz549lZOTU+3mOq1bt1Z2drak72+uM3nyZL3++uvOm+tIUosWLdSiRQvLtgNorGiuAAAAfAQ31wHsjeYKAADAh3BzHcC+aK4AAAAA+J2i0xUqLK1Q8ZmzimjWVDHNgxUZFnxJ66S5AgAAAOBXjp76To8t2671+wqd0/q2j9H0O7ooIarZRa+XuwUCAAAA8BtFpyuqNVaStG5foSYu266i0xUXvW6aKwAAAAB+o7C0olpjdc66fYUqLKW5AgAAAIALKj5zttb5JReYXxuaKwAAAAB+IyK0aa3zwy8wvzY0VwAAAAD8RkyLYPVtH+N2Xt/2MYppcfF3DKS5AgAAAOA3IsOCNf2OLtUarL7tYzTjji6XdDt2bsUOAAAAwK8kRDXTC8O7qbC0QiVnzio8tKliWvA9VwAAAABQb5Fhl95M/RCXBQIAAACAB9BcAQAAAIAH0FwBAAAAgAfQXAEAAACAB9BcAQAAAIAH0FwBAAAAgAdwK3bAzxSdrlBhaYWKz5xVRLOmimnu+duQAgAA+COaK8CPHD31nR5btl3r9xU6p/VtH6Ppd3RRQlQzCysDAADwfVwWCPiJotMV1RorSVq3r1ATl21X0ekKiyoDAABoHDhzBfiJwtKKao3VOev2FaqwtILLAwE0GC5RBtAY0VwBfqL4zNla55dcYD4AeAqXKANorHzissA5c+YoKSlJoaGhSk1N1ebNm2sd/7e//U0dOnRQaGioOnfurPfee6+BKm14RacrdOB4qT4//G8dOFFq6aVddqrFjvVYLSK0aa3zwy8wH2RRbey0v9mpFjuxy+vCJcqXjixyzy6/43ash1oaju3PXC1ZskSZmZnKzc1VamqqcnJylJ6err179yo2Nrba+E8++UTDhw9Xdna2brvtNr3++usaOnSotm7dqk6dOlmwBd5jpyN/dqrFjvXYQUyLYPVtH6N1bi4N7Ns+RjEtuBynNmRRzey0v9mpFjux0+vCJcqXhixyz06/43arh1oaVoAxxlhdRG1SU1N1/fXXa/bs2ZKkqqoqJSYm6sEHH9TEiROrjR82bJjKysr07rvvOqf9z//8j1JSUpSbm1un5ywuLlZkZKSKiooUERHhmQ3xsKLTFRr/xudu/4Hq2z5GLwzv1mD/ONmpFjvWYydHT32nicu2uzRYfdvHaMYdXdTqB6HmC/tBQyKL3LPT/manWuzEbq/L54f/rf/90yc1zn/z172VckW0JN/YBxpaQ2eRL7wHdvsdt1M91OIZ9dkPbH1ZYEVFhbZs2aK0tDTntMDAQKWlpWnjxo1ul9m4caPLeElKT0+vcbwklZeXq7i42OVhd3U58uePtdixHjtJiGqmF4Z3U15mP735697Ky+ynF4Z3q9ZYwRVZVDM77W92qsVO7Pa6cInyxWuILCKHGlc91NLwbN1cFRYWqrKyUnFxcS7T4+Li5HA43C7jcDjqNV6SsrOzFRkZ6XwkJiZeevFeZqebE9ipFsl+9dhNZFiwroxtoZQronVlbAvbHiWyE7KoZnba3+xUi53Y7XU5d4myO1yiXLuGyCJy6NLZqR5qaXi2bq4aSlZWloqKipyPI0eOWF3SBdnpyJ+dapHsVw9QV2TRpbFTLXZit9clMixY0+/oUq3BOneJMgd8rEUOXTo71UMtDc/WN7SIiYlRUFCQCgoKXKYXFBQoPj7e7TLx8fH1Gi9JISEhCgkJufSCG5Cdbk5gp1rsWA98H1lUMzvtb3aqxU7s+Lqcu0S5sLRCJWfOKjy0qWJa8D1XF9IQWUQONa56qKXh2frMVXBwsLp37668vDzntKqqKuXl5alXr15ul+nVq5fLeElavXp1jeN9lZ2O/NmpFjvWA99HFtXMTvubnWqxE7u+LlyiXH9kkXt2+x23Uz3UYgFjc4sXLzYhISFm4cKF5p///Kd54IEHTFRUlHE4HMYYY0aMGGEmTpzoHP/xxx+bJk2amGeeecbs3r3bTJkyxTRt2tTs2LGjzs9ZVFRkJJmioiKPb4+nnSorN/sLSsznX39r9heUmFNl5dRi03p8jS/tBw2BLKqdnfY3O9ViJ774uvjSPtBQGjqLfOk9sNvvuJ3qoZZLU5/9wNaXBUrf30L0xIkTmjx5shwOh1JSUrRq1SrnhzMPHz6swMD/noDr3bu3Xn/9df3+97/X7373O7Vv315vvvlmo/ouh/NFhtnnMgo71SLZrx74NrKodnba3+xUi53wujQOZFHN7PY7bqd6qKXh2P57rqxQVFSkqKgoHTlyxLbf6QB4W3FxsRITE3Xq1ClFRkZaXY5fIovg78gh65FDQP2yyPZnrqxQUlIiST5x+1HA20pKSvijxiJkEfA9csg65BDwX3XJIs5cuVFVVaWjR48qPDxcAQEBbsec62A5kuOK16VmvvbaGGNUUlKihIQEl0tM0HAulEW+9jvVkHht3PO114Ucsh5/E10aXhv3fO11qU8WcebKjcDAQLVp06ZOYyMiInzil6Kh8brUzJdeG44UW6uuWeRLv1MNjdfGPV96Xcgha/E3kWfw2rjnS69LXbOIw0AAAAAA4AE0VwAAAADgATRXFykkJERTpkzxuW8x9zZel5rx2sDT+J2qGa+Ne7wu8AZ+r2rGa+NeY35duKEFAAAAAHgAZ64AAAAAwANorgAAAADAA2iuAAAAAMADaK4AAAAAwANori7CnDlzlJSUpNDQUKWmpmrz5s1Wl2S57OxsXX/99QoPD1dsbKyGDh2qvXv3Wl2W7UyfPl0BAQF6+OGHrS4FjQBZVB1ZVDdkETyJLHJFDtVNY80hmqt6WrJkiTIzMzVlyhRt3bpVXbt2VXp6uo4fP251aZZau3atMjIy9Omnn2r16tU6e/asBg0apLKyMqtLs43PPvtMc+fOVZcuXawuBY0AWeQeWXRhZBE8iSyqjhy6sEadQwb10rNnT5ORkeH8ubKy0iQkJJjs7GwLq7Kf48ePG0lm7dq1VpdiCyUlJaZ9+/Zm9erVpl+/fmbChAlWlwQfRxbVDVnkiiyCp5FFF0YOuWrsOcSZq3qoqKjQli1blJaW5pwWGBiotLQ0bdy40cLK7KeoqEiS1LJlS4srsYeMjAzdeuutLr87wMUii+qOLHJFFsGTyKK6IYdcNfYcamJ1Ab6ksLBQlZWViouLc5keFxenPXv2WFSV/VRVVenhhx9Wnz591KlTJ6vLsdzixYu1detWffbZZ1aXgkaCLKobssgVWQRPI4sujBxy5Q85RHMFj8vIyNDOnTu1YcMGq0ux3JEjRzRhwgStXr1aoaGhVpcD+BWy6L/IIsAa5NB/+UsO0VzVQ0xMjIKCglRQUOAyvaCgQPHx8RZVZS/jx4/Xu+++q3Xr1qlNmzZWl2O5LVu26Pjx47ruuuuc0yorK7Vu3TrNnj1b5eXlCgoKsrBC+CKy6MLIIldkEbyBLKodOeTKX3KIz1zVQ3BwsLp37668vDzntKqqKuXl5alXr14WVmY9Y4zGjx+vFStW6KOPPlJycrLVJdnCwIEDtWPHDm3bts356NGjh+655x5t27atUYQIGh5ZVDOyyD2yCN5AFrlHDrnnLznEmat6yszM1KhRo9SjRw/17NlTOTk5Kisr05gxY6wuzVIZGRl6/fXX9dZbbyk8PFwOh0OSFBkZqWbNmllcnXXCw8OrXWPdvHlzXXbZZVx7jUtCFrlHFrlHFsFbyKLqyCH3/CWHaK7qadiwYTpx4oQmT54sh8OhlJQUrVq1qtqHOf3Niy++KEnq37+/y/QFCxZo9OjRDV8Q0MiRRe6RRUDDIouqI4f8W4AxxlhdBAAAAAD4Oj5zBQAAAAAeQHMFAAAAAB5AcwUAAAAAHkBzBQAAAAAeQHMFAAAAAB5AcwUAAAAAHkBzBQAAAAAeQHMFAAAAAB5AcwUAAAAAHkBzBa8aPXq0hg4d6jJt6dKlCg0N1bPPPmtNUQD8DlkEwGrkkH9oYnUB8C9//vOflZGRodzcXI0ZM8bqcgD4KbIIgNXIocaJM1doME8//bQefPBBLV682Bkib731lq677jqFhoaqXbt2mjp1qv7zn/9Ikn7xi1/otttuc1nH2bNnFRsbq5dfflnS90d8OnfurGbNmumyyy5TWlqaysrKGnbDAPgUsgiA1cihRswAXjRq1CgzZMgQ89vf/ta0aNHC/P3vf3fOW7dunYmIiDALFy40Bw4cMB9++KFJSkoyjz/+uDHGmI8//tgEBQWZo0ePOpdZvny5ad68uSkpKTFHjx41TZo0MbNmzTIHDx4027dvN3PmzDElJSUNvp0A7I0sAmA1csg/0FzBq0aNGmWCg4ONJJOXl+cyb+DAgWbatGku0/7yl7+YVq1aOX/u2LGjmTFjhvPn22+/3YwePdoYY8yWLVuMJHPo0CEvbgGAxoAsAmA1csg/BBhjjJVnztC4jR49Wrt27VJhYaHatGmj999/Xy1atJAkXX755SotLVVQUJBzfGVlpc6cOaOysjKFhYXpueee00svvaTdu3eroKBAbdq00UcffaQbb7xRlZWVSk9P1+bNm5Wenq5BgwbpzjvvVHR0tFWbC8CmyCIAViOH/APNFbxq9OjROnXqlJ5//nkNGDBACQkJev/99xUeHq5mzZpp6tSp+ulPf1ptuXbt2ikwMFAnT55UQkKC8vPz9cknn2ju3Ln68ssvneOMMfrkk0/04YcfasWKFXI4HNq0aZOSk5MbcjMB2BxZBMBq5JB/4IYWaBBt27bV2rVr5XA4dPPNN6ukpETXXXed9u7dq6uuuqraIzDw+1/Nyy67TEOHDtWCBQu0cOHCanfTCQgIUJ8+fTR16lR9/vnnCg4O1ooVK6zYRAA+gCwCYDVyqHHjVuxoMImJicrPz9eAAQOUnp6uxx57THfeeaeuuOIK3XnnnQoMDNQXX3yhnTt36sknn3Qud//99+u2225TZWWlRo0a5Zy+adMm5eXladCgQYqNjdWmTZt04sQJXXvttVZsHgAfQRYBsBo51HjRXKFBtWnTxhkm06dP19KlS/X0009rxowZatq0qTp06KD777/fZZm0tDS1atVKP/rRj5SQkOCcHhERoXXr1iknJ0fFxcVq27atnn32Wd1yyy0NvVkAfAxZBMBq5FDjxGeuYHulpaVq3bq1FixY4PZaZABoCGQRAKuRQ/bHmSvYVlVVlQoLC/Xss88qKipKgwcPtrokAH6ILAJgNXLId9BcwbYOHz6s5ORktWnTRgsXLlSTJvy6Amh4ZBEAq5FDvoPLAgEAAADAA7gVOwAAAAB4AM0VAAAAAHgAzRUAAAAAeADNFQAAAAB4AM0VAAAAAHgAzRUAAAAAeADNFQAAAAB4AM0VAAAAAHjA/wffAvkpgxGfLAAAAABJRU5ErkJggg=="/>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=600b8f79-8434-47e5-a01c-088a587af36b"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Attention-Output">Attention Output<a class="anchor-link" href="#Attention-Output">¶</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=d61da97b-8bd5-43ab-b7d5-37f6fd98947f"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have the attention weights, we can apply them to the values. This will give us a weighted sum of contextual information. However, the answer is still in "value space". Before we combine them with the token embeddings, we'll project them back to "model space".</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=6a8d1c74-fc5e-4ea7-a263-be6eccb208a3">
<div class="input">
<div class="prompt input_prompt">In [26]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Compute weighted combination of values</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">w</span> <span class="o">@</span> <span class="n">v</span>
<span class="n">a</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[26]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([6, 768])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=21b505b4-b94c-408d-8e9e-5b5a6d0980cb">
<div class="input">
<div class="prompt input_prompt">In [27]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Configure output projection</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>

<span class="c1"># Load pre-trained state</span>
<span class="n">outputs</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
    <span class="s2">"weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.transformer.layer.0.attention.out_lin.weight"</span><span class="p">],</span>
    <span class="s2">"bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.transformer.layer.0.attention.out_lin.bias"</span><span class="p">],</span>
<span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[27]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>&lt;All keys matched successfully&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=65c9b39f-6b40-40a4-bbb8-273821bc21aa">
<div class="input">
<div class="prompt input_prompt">In [28]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Project attention embeddings back to model space</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">a</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[28]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([6, 768])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=3075e1fd-8477-4d2f-857e-250abe9eba04"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Multi-Head-Attention">Multi-Head Attention<a class="anchor-link" href="#Multi-Head-Attention">¶</a></h3><p>At this point, we've walked through the core SDPA algorithm step-by-step. However, we're not quite done. Vaswani et al. realized there are more than one set of relationships involved in transferring context across tokens. A single application of SDPA would effectively average these together. The solution is to apply SDPA multiple times on separate query, key, and value embeddings. Each of these is referred to as an "attention head". Each head is isolated, leaving it free to learn distinct relational structures.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=0b7eb210-6157-4f7d-bb16-80054f99d4e0">
<div class="input">
<div class="prompt input_prompt">In [29]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">split_heads</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">combine_heads</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">d_head</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=566f79ae-4445-4a92-a7d0-5b15bb1327f2">
<div class="input">
<div class="prompt input_prompt">In [30]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Render query, key, value dimensions before we split</span>
<span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[30]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([6, 768]), torch.Size([6, 768]), torch.Size([6, 768]))</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=efa0cabc-9903-4e1f-a188-8fe9ef2c3647">
<div class="input">
<div class="prompt input_prompt">In [31]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Split queries, keys, values into separate heads</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

<span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[31]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([12, 6, 64]), torch.Size([12, 6, 64]), torch.Size([12, 6, 64]))</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=8e0a8eab-37ea-42ea-b466-d237b499d87c"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that the queries, keys, and values have been split into 12 heads, leaving each of the original 768-element query, key, and value embeddings is now 64 elements long.</p>
<p>Next, let's recompute the attention embeddings.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=1539a494-99dc-477f-bfb0-1dfbafeafce2">
<div class="input">
<div class="prompt input_prompt">In [32]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Compute attention for all heads in parallel</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_head</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">v</span>

<span class="n">a</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[32]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([12, 6, 64])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=f7cc6ac3-16b4-4ebb-8b21-b650d3b63d4f"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>While the attention code is the same, you can see the attention values are still split into heads. Next, we'll recombine them before applying the final output projection.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=5df05cde-1886-49b1-b2df-9c24b689391e">
<div class="input">
<div class="prompt input_prompt">In [33]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Recombine heads</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">combine_heads</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">a</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[33]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([6, 768])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=be91d403-0580-4519-aaf1-d432ef0f8e2d">
<div class="input">
<div class="prompt input_prompt">In [34]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Project attention embeddings back to model space</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">a</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[34]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([6, 768])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=e5d8a6f8-deb0-4e68-bffa-149e3ca3c2c5"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Add-and-Normalize">Add and Normalize<a class="anchor-link" href="#Add-and-Normalize">¶</a></h3><p>Before we get to the FNN, we'll combine the attention embeddings with input embeddings the same way we combined the value and position embeddings.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=fd4453d3-0ee2-42e1-8b26-f0a8d8e8298d">
<div class="input">
<div class="prompt input_prompt">In [35]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Configure attention normalization</span>
<span class="n">normalize_attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span>

<span class="c1"># Load pre-trained state</span>
<span class="n">normalize_attention</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
    <span class="s2">"weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.transformer.layer.0.sa_layer_norm.weight"</span><span class="p">],</span> 
    <span class="s2">"bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.transformer.layer.0.sa_layer_norm.bias"</span><span class="p">],</span>
<span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[35]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>&lt;All keys matched successfully&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=5dfbb320-be66-4456-81c2-92d63c7129a8">
<div class="input">
<div class="prompt input_prompt">In [36]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Combine attention with input embeddings</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">normalize_attention</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">a</span><span class="p">)</span>

<span class="n">y</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[36]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([6, 768])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=79a8da5b-78fa-4528-9a74-f0cedc33c49b"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="FNN">FNN<a class="anchor-link" href="#FNN">¶</a></h3><p>The FNN block is a straightforward fully connected multi-layer perceptron.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=f2661b91-b813-44af-83e1-7870e4c1fef8">
<div class="input">
<div class="prompt input_prompt">In [37]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Configure FNN</span>
<span class="n">fnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_fnn</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_fnn</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Load pre-trained state</span>
<span class="n">fnn</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
    <span class="s2">"0.weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.transformer.layer.0.ffn.lin1.weight"</span><span class="p">],</span> 
    <span class="s2">"0.bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.transformer.layer.0.ffn.lin1.bias"</span><span class="p">],</span>
    <span class="s2">"2.weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.transformer.layer.0.ffn.lin2.weight"</span><span class="p">],</span> 
    <span class="s2">"2.bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.transformer.layer.0.ffn.lin2.bias"</span><span class="p">],</span>
<span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[37]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>&lt;All keys matched successfully&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=f90b3b4f-98ff-4683-9302-ed08bab7440e">
<div class="input">
<div class="prompt input_prompt">In [38]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Transform attention outputs</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">fnn</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="n">f</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[38]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([6, 768])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=4252c07b-de67-4220-ac2d-415790ba906b"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Add-and-Normalize">Add and Normalize<a class="anchor-link" href="#Add-and-Normalize">¶</a></h3><p>Next, we combine the FNN outputs with the attention outputs.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=7d4b356b-a9e3-4081-bf36-dbb2a6a1c300">
<div class="input">
<div class="prompt input_prompt">In [39]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Configure attention normalization</span>
<span class="n">normalize_fnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span>

<span class="c1"># Load pre-trained state</span>
<span class="n">normalize_fnn</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
    <span class="s2">"weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.transformer.layer.0.output_layer_norm.weight"</span><span class="p">],</span> 
    <span class="s2">"bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"distilbert.transformer.layer.0.output_layer_norm.bias"</span><span class="p">],</span>
<span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[39]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>&lt;All keys matched successfully&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=45b46002-94be-4fc0-b9cb-6caa9127a422">
<div class="input">
<div class="prompt input_prompt">In [40]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">normalize_fnn</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">f</span><span class="p">)</span>
<span class="n">z</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[40]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([6, 768])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=b0917d31-aa8c-495e-b6eb-cfaaa845bfd2"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Full-Stack">Full Stack<a class="anchor-link" href="#Full-Stack">¶</a></h3><p>Quick recap. Given input embeddings $X$, we added attention embeddings to get $Y$, and added transformed embeddings to get $Z$. Next, we combine all of these elements together and repeat for each layer in the stack. While you would normally create a stack of torch modules, we run the layers in a loop instead to make it easier to see what's happening.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=c8de2a32-25e9-43a4-8f57-57dd0a55bc02">
<div class="input">
<div class="prompt input_prompt">In [41]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
    
    <span class="c1"># Configure query, key, value, and output projections</span>
    <span class="n">queries</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>

    <span class="c1"># Load pre-trained state</span>
    <span class="n">queries</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
        <span class="s2">"weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">"distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.attention.q_lin.weight"</span><span class="p">],</span>
        <span class="s2">"bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">"distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.attention.q_lin.bias"</span><span class="p">],</span>
    <span class="p">})</span>
    <span class="n">keys</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
        <span class="s2">"weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">"distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.attention.k_lin.weight"</span><span class="p">],</span>
        <span class="s2">"bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">"distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.attention.k_lin.bias"</span><span class="p">],</span>
    <span class="p">})</span>
    <span class="n">values</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
        <span class="s2">"weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">"distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.attention.v_lin.weight"</span><span class="p">],</span>
        <span class="s2">"bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">"distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.attention.v_lin.bias"</span><span class="p">],</span>
    <span class="p">})</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
        <span class="s2">"weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">"distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.attention.out_lin.weight"</span><span class="p">],</span>
        <span class="s2">"bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">"distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.attention.out_lin.bias"</span><span class="p">],</span>
    <span class="p">})</span>
    
    <span class="c1"># Project x to query, key, and value spaces</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">queries</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">keys</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">values</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="c1"># Split q, k, v into separate heads</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

    <span class="c1"># Compute attention for all heads in parallel</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_head</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">v</span>
    
    <span class="c1"># Recombine heads</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">combine_heads</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    
    <span class="c1"># Project attention embeddings back to model space</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">a</span>


<span class="k">def</span> <span class="nf">normalize_attention</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
    
    <span class="c1"># Configure attention normalization</span>
    <span class="n">xform</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span>
    
    <span class="c1"># Load pre-trained state</span>
    <span class="n">xform</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
        <span class="s2">"weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">"distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.sa_layer_norm.weight"</span><span class="p">],</span> 
        <span class="s2">"bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">"distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.sa_layer_norm.bias"</span><span class="p">],</span>
    <span class="p">})</span>

    <span class="k">return</span> <span class="n">xform</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">fnn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
    
    <span class="c1"># Configure FNN</span>
    <span class="n">xform</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_fnn</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_fnn</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">),</span>
    <span class="p">)</span>
    
    <span class="c1"># Load pre-trained state</span>
    <span class="n">xform</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
        <span class="s2">"0.weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">"distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.ffn.lin1.weight"</span><span class="p">],</span> 
        <span class="s2">"0.bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">"distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.ffn.lin1.bias"</span><span class="p">],</span>
        <span class="s2">"2.weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">"distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.ffn.lin2.weight"</span><span class="p">],</span> 
        <span class="s2">"2.bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">"distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.ffn.lin2.bias"</span><span class="p">],</span>
    <span class="p">})</span>
    
    <span class="c1"># Transform attention outputs</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">xform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">f</span>


<span class="k">def</span> <span class="nf">normalize_fnn</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
    
    <span class="c1"># Configure attention normalization</span>
    <span class="n">xform</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span>
    
    <span class="c1"># Load pre-trained state</span>
    <span class="n">xform</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
        <span class="s2">"weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">"distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.output_layer_norm.weight"</span><span class="p">],</span> 
        <span class="s2">"bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">"distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">.output_layer_norm.bias"</span><span class="p">],</span>
    <span class="p">})</span>
    
    <span class="k">return</span> <span class="n">xform</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=4dbc9b44-2fb2-4d2e-98ee-4778f243b37a">
<div class="input">
<div class="prompt input_prompt">In [42]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Initialize loop</span>
<span class="n">z_i</span> <span class="o">=</span> <span class="n">x</span>

<span class="c1"># Apply layer logic in a loop</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">n_layers</span><span class="p">):</span>
    
    <span class="c1"># Use previous layer's outputs as inputs</span>
    <span class="n">x_i</span> <span class="o">=</span> <span class="n">z_i</span>

    <span class="c1"># Attention</span>
    <span class="n">y_i</span> <span class="o">=</span> <span class="n">normalize_attention</span><span class="p">(</span><span class="n">x_i</span> <span class="o">+</span> <span class="n">attention</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">),</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>

    <span class="c1"># Transform</span>
    <span class="n">z_i</span> <span class="o">=</span> <span class="n">normalize_fnn</span><span class="p">(</span><span class="n">y_i</span> <span class="o">+</span> <span class="n">fnn</span><span class="p">(</span><span class="n">y_i</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">),</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>

<span class="c1"># Save outputs from last layer</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">z_i</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=4e46166a-a427-41fb-958d-0deb84d46682">
<div class="input">
<div class="prompt input_prompt">In [43]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">z</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[43]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 3.6173e-01, -1.3168e-01,  3.5340e-02,  ...,  4.4015e-01,
          1.0666e+00, -1.9293e-01],
        [ 7.3341e-01,  4.9823e-02, -1.7590e-02,  ...,  5.0063e-01,
          1.1480e+00, -1.2997e-01],
        [ 1.1230e+00,  2.7603e-01,  3.2096e-01,  ...,  1.8820e-01,
          1.0586e+00, -1.2496e-01],
        [ 4.8728e-01,  1.4863e-02,  4.2930e-01,  ...,  4.8993e-01,
          7.9435e-01,  1.2331e-01],
        [ 1.0595e-03, -1.4508e-01,  2.8892e-01,  ...,  5.5342e-01,
          7.9370e-01, -9.0899e-02],
        [ 1.1021e+00,  8.6115e-02,  5.7461e-01,  ...,  6.8800e-01,
          5.6345e-01, -6.6278e-01]], grad_fn=&lt;NativeLayerNormBackward0&gt;)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=f9f577c2-82dc-4187-856e-2d64f34630fa">
<div class="input">
<div class="prompt input_prompt">In [44]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Sanity check</span>
<span class="n">hiddens</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">distilbert</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=1f318a24-9dc4-4ba3-a369-3dcc231c4a3f"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Head">Head<a class="anchor-link" href="#Head">¶</a></h2><p>As the final stage in the Transformer pipeline, the Head stage maps the contextualized embeddings to task-specific predictions. In our case, the Head stage is responsible for turning the contextualized embeddings into a binary classifier that predicts whether the original text contains positive or negative sentiments. This sounds like a straightforward output layer until you realize that instead of one set of features, we have a sequence of features. And the length of the sequence is arbitrary. How do you connect an arbitrary length sequence of feature vectors to an output layer?</p>
<p>The trick is hiding in our contextualized embeddings. Each input embedding represents a single token in isolation. But the output embeddings have been infused with information from all of the tokens. This is why the common practice is to simply take the first output embedding and drop the rest. The first embedding represents the start of sequence marker <code>[CLS]</code>. Since the <code>[CLS]</code> marker token is added to every sequence, the first input embedding is always the same. In contrast, the first output embedding uniquely represents one specific sequence.</p>
<p>If we let the first output embedding represent the entire sequence, then we have a single feature vector that's easy to connect to any task-specific output layer we need.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=0cf5b65b-28b7-41be-9d48-a98f56d35e5c">
<div class="input">
<div class="prompt input_prompt">In [45]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Use [CLS] embedding to represent the entire sequence</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">z</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">features</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[45]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([768])</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=babf146e-10c0-4a57-9ee2-6499e18ec6f4">
<div class="input">
<div class="prompt input_prompt">In [46]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Configure classifier</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">n_labels</span><span class="p">),</span>
<span class="p">)</span>
    
<span class="c1"># Load pre-trained state</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
    <span class="s2">"0.weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"pre_classifier.weight"</span><span class="p">],</span> 
    <span class="s2">"0.bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"pre_classifier.bias"</span><span class="p">],</span>
    <span class="s2">"2.weight"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"classifier.weight"</span><span class="p">],</span> 
    <span class="s2">"2.bias"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">"classifier.bias"</span><span class="p">],</span>
<span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[46]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>&lt;All keys matched successfully&gt;</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=875ada3a-42e6-42b2-9c6f-7b611a5252c6">
<div class="input">
<div class="prompt input_prompt">In [47]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Classify features</span>
<span class="n">prediction1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">classifier</span><span class="p">(</span><span class="n">features</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">prediction1</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[47]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>0.9998118281364441</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=3209240f-d3c7-4445-a9f7-242043dc8830">
<div class="input">
<div class="prompt input_prompt">In [48]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Verify custom results match off-the-shelf ones</span>
<span class="n">prediction2</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span><span class="s2">"I love ice cream"</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s2">"score"</span><span class="p">]</span>
<span class="n">prediction2</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[48]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>0.9998118281364441</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=a3f405e1-9b83-45b7-b318-36560da32f7b">
<div class="input">
<div class="prompt input_prompt">In [49]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">assert</span> <span class="n">prediction1</span> <span class="o">==</span> <span class="n">approx</span><span class="p">(</span><span class="n">prediction2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=1266662a-c54e-438e-95ef-cb13eeaf463b"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Discussion">Discussion<a class="anchor-link" href="#Discussion">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=350dd6d4-b1af-47ef-a833-614f5e86fa5f">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=fce44040-c06c-4bc2-afe3-7edccf5943db"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References">References<a class="anchor-link" href="#References">¶</a></h2><p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv, May 24, 2019. <a href="https://doi.org/10.48550/arXiv.1810.04805">https://doi.org/10.48550/arXiv.1810.04805</a>.</p>
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” arXiv, August 1, 2023. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>.</p>
<p>Xu, Peng, Xiatian Zhu, and David A. Clifton. “Multimodal Learning with Transformers: A Survey.” arXiv, May 9, 2023. <a href="https://doi.org/10.48550/arXiv.2206.06488">https://doi.org/10.48550/arXiv.2206.06488</a>.</p>
</div>
</div>
</div>

  </div><a class="u-url" href="/2024/09/04/transformer-from-scratch.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Pattern Recognition</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Pattern Recognition</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/stickshift"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">stickshift</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Software architect, AI researcher, entrepreneur, and startup veteran with a passion for creativity and good coffee.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
