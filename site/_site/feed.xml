<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-04T19:55:08-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Pattern Recognition</title><subtitle>Software architect, AI researcher, entrepreneur, and startup veteran with a passion for creativity and good coffee.</subtitle><entry><title type="html">Hello World</title><link href="http://localhost:4000/2024/09/04/hello-world.html" rel="alternate" type="text/html" title="Hello World" /><published>2024-09-04T00:00:00-04:00</published><updated>2024-09-04T00:00:00-04:00</updated><id>http://localhost:4000/2024/09/04/hello-world</id><content type="html" xml:base="http://localhost:4000/2024/09/04/hello-world.html"><![CDATA[<p>Hello world $QK^T$!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[Hello world $QK^T$!]]></summary></entry><entry><title type="html">Writing a Transformer One Cell at a Time</title><link href="http://localhost:4000/2024/09/04/transformers.html" rel="alternate" type="text/html" title="Writing a Transformer One Cell at a Time" /><published>2024-09-04T00:00:00-04:00</published><updated>2024-09-04T00:00:00-04:00</updated><id>http://localhost:4000/2024/09/04/transformers</id><content type="html" xml:base="http://localhost:4000/2024/09/04/transformers.html"><![CDATA[<p>There is something special about Transformers. Since first introduced in 2017, the Transformer architecture has redefined the entire NLP category and is quickly spreading to other types of machine learning. Transformers, and the language embeddings at their core, seem to have hit on some underlying fundamental substrate that transcends boundaries, making them far more generalizable than previous approaches. Anyone who wants to contribute to the future of AI should clearly be studying Transformers inside and out.</p>

<p>While there are a million papers, blogs, and tutorials written on Transformers, I still find it challenging to map the abstract ideas from the research literature into concrete, actionable steps I can experiment with. My engineer’s brain wants to “see the code” behind high level concepts like embeddings, residuals, and multi-head self-attention. While it’s easy to find open source Transformer implementations, they are often overloaded with configuration settings, conditionalized to support every variation ever imagined, to the point that the main ideas are completely obscured.</p>

<p>The goal of this post is to give you a stronger sense for how Transformers work under the hood. We’ll take a single inference—from raw data to final prediction—and break it down into tiny steps, illustrating the main ideas from the Transformer literature with minimal, straightforward, working Python code. You may be surprised by how few steps are required!</p>

<h2 id="setup">Setup</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">warnings</span>

<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">pytest</span> <span class="kn">import</span> <span class="n">approx</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.nn.functional</span> <span class="kn">import</span> <span class="n">relu</span><span class="p">,</span> <span class="n">softmax</span>
<span class="kn">import</span> <span class="n">transformers</span>

<span class="kn">from</span> <span class="n">stickshift.models</span> <span class="kn">import</span> <span class="n">distilbert</span>
</code></pre></div></div>

<h2 id="text-classification-with-distilbert">Text Classification with DistilBERT</h2>

<p>If you’ve worked with Transformers at all, I’m sure you’re familiar with Hugging Face’s collection of Python libraries as well as their endless repository of models and datasets. Throughout the post, we’ll be working with Hugging Face’s default text classification model: DistilBERT. DistilBERT is a smaller, faster, lighter-weight version of the original BERT model that’s easier to experiment with. We’ll use the pre-trained model parameters from Hugging Face, but we’ll implement the model’s logic step-by-step in Jupyter using a slightly modified version of the actual DistilBERT PyTorch implementation from Hugging Face’s <code class="language-plaintext highlighter-rouge">transformers</code> library.</p>

<p>Before we get into the implementation, let’s start by running the entire process end-to-end using Hugging Face’s high level <code class="language-plaintext highlighter-rouge">pipeline</code> API. The following cells create a complete text classification pipeline and then apply it to the sentence “I love ice cream”. As you might expect, the model classifies the sentence as overwhelmingly positive. There is a lot happening in very few lines of code here. Over the rest of the post, we’ll break this prediction down and recreate it one step at a time.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create off-the-shelf text classification transformer
</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">transformers</span><span class="p">.</span><span class="nf">pipeline</span><span class="p">(</span><span class="sh">"</span><span class="s">text-classification</span><span class="sh">"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in production is not recommended.
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">transformer</span><span class="p">(</span><span class="sh">"</span><span class="s">I love ice cream</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[{'label': 'POSITIVE', 'score': 0.9998118281364441}]
</code></pre></div></div>

<p>The code samples throughout this post recreate portions of Hugging Face’s DistilBERT implementation. They’ve been lightly refactored to improve clarity and help illustrate the core ideas of the Transformer architecture. The following cell loads a custom version of the model configuration as well as the pre-trained model parameters. We’ll reference these as we go.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load model config and pre-trained parameters
</span><span class="n">config</span> <span class="o">=</span> <span class="n">distilbert</span><span class="p">.</span><span class="nf">config</span><span class="p">(</span><span class="n">transformer</span><span class="p">.</span><span class="n">model</span><span class="p">)</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="transformer-pipeline">Transformer Pipeline</h2>

<p>The following diagram depicts a Transformer as a multi-stage pipeline. The Context stage at the center of the pipeline is where most of the magic happens. The stages before and after Context provide the extra machinery required to convert raw data into input embeddings and output embeddings into task-specific outputs. While we’ll focus on text data, it’s worth noting that the same stages can be applied to all data modalities including audio and images (Xu et al. 2023).</p>

<p><img src="transformer-pipeline.svg" alt="Transformer Pipeline" /></p>

<h2 id="tokenize">Tokenize</h2>

<p>The Tokenize stage is responsible for breaking raw data into a sequence of “tokens”. While the word “token” is often associated with text processing, the Transformer literature extends this to other data modalities as well. Examples include patches of an image or segments of an audio recording. In fact, tokenization is seen as a core strength of the Transformer architecture because it allows Transformers to process different types of data using a single, universal approach (Xu et al. 2023).</p>

<p>While tokenization is a general concept, the specific algorithms used are modality-specific. In this case, our transformer uses an algorithm known as “word-piece” (Devlin et al. 2019) to split raw text into a sequence of tokens. Next, special tokens are injected to mark the beginning and end of the sequence. Each token is then converted into an integer-encoded categorical value using a fixed token vocabulary, producing the final sequence of “input_ids” that are passed to the next stage.</p>

<p>Since our primary interest is in the Transformer layers that come later, we’ll use Hugging Face’s off-the-shelf tokenizer implementation here.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Extract tokenizer from transformer
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">.</span><span class="n">tokenizer</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Tokenize sentence
</span><span class="n">batch</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">I love ice cream</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>

<span class="n">batch</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'input_ids': tensor([[ 101, 1045, 2293, 3256, 6949,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}
</code></pre></div></div>

<p>Tokenizing “I love ice cream” generates the token sequence: <code class="language-plaintext highlighter-rouge">[101, 1045, 2293, 3256, 6949, 102]</code>. If we decode the integer-encoded values to see what each one represents, we can see the 4 words are represented by values 1045 to 6949. The values 101 and 102 represent special tokens <code class="language-plaintext highlighter-rouge">[CLS]</code> and <code class="language-plaintext highlighter-rouge">[SEP]</code> that were added to mark the beginning and end of the sequence respectively.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">input_id</span><span class="p">)</span> <span class="k">for</span> <span class="n">input_id</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['[CLS]', 'i', 'love', 'ice', 'cream', '[SEP]']
</code></pre></div></div>

<h2 id="embeddings">Embeddings</h2>

<p>The second stage in the Transformer pipeline converts each of the integer-encoded categorical values into an “embedding”. Embeddings (Bengio et al. 2000) are the fundamental data structure of the Transformer architecture. The Transformer layers we’ll look at in the next stage take embeddings as input, <em>transform</em> them, and produce embeddings as output. Embeddings predated Transformers by almost 2 decades and are a fascinating topic in their own right. But we’ll save the embeddings deep dive for another post. For now, all we need to know is embeddings represent each token as a unique point in an n-dimensional vector space. The vector space coordinates are initialized randomly and then learned during training.</p>

<p>Similar to tokenization, the steps required to convert tokens into embeddings depend on the data modality. In text transformers, the Embeddings stage is typically implemented using 2 lookup tables. The first lookup table maps the value of each token to a unique embedding vector. The second lookup table maps the position of each token to a unique embedding vector. The value and position embeddings are then added together to create the initial token embeddings.</p>

<p><img src="embeddings.svg" alt="Embeddings" /></p>

<p>Let’s start with value embeddings. First, we initialize the value embeddings lookup table. Next, we read the values from the tokenizer output. Finally, we pass the token values to the lookup table to get unique embeddings for each value.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize value embeddings lookup table
</span><span class="n">value_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span>
    <span class="n">num_embeddings</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> 
    <span class="n">embedding_dim</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Load pre-trained state
</span><span class="n">value_embeddings</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.embeddings.word_embeddings.weight</span><span class="sh">"</span><span class="p">],</span>
<span class="p">})</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;All keys matched successfully&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate token values
</span><span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">input_ids</span><span class="p">)</span>

<span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">input_id</span><span class="p">)</span> <span class="k">for</span> <span class="n">input_id</span> <span class="ow">in</span> <span class="n">values</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['[CLS]', 'i', 'love', 'ice', 'cream', '[SEP]']
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Map token values to embeddings
</span><span class="n">v</span> <span class="o">=</span> <span class="nf">value_embeddings</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>

<span class="n">v</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([6, 768])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Show sample of value embeddings
</span><span class="n">v</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 3.9925e-02, -1.0171e-02, -2.0390e-02,  ...,  6.1588e-02,
          2.1959e-02,  2.2732e-02],
        [-1.2794e-02,  4.9879e-03, -2.6270e-02,  ..., -7.2300e-05,
          5.3657e-03,  1.1908e-02],
        [ 5.9359e-02, -2.3563e-02, -2.0560e-03,  ..., -1.0420e-02,
          1.4846e-02, -1.2815e-02],
        [-2.4101e-02, -2.4911e-02, -2.2601e-02,  ..., -2.5139e-02,
          1.1392e-02,  3.2655e-02],
        [-8.5466e-02, -5.9276e-02, -5.6659e-02,  ..., -1.7192e-02,
         -8.6179e-02, -4.5105e-02],
        [-2.1060e-02, -6.4941e-03, -1.0682e-02,  ..., -2.3401e-02,
          6.1463e-03, -6.4845e-03]], grad_fn=&lt;EmbeddingBackward0&gt;)
</code></pre></div></div>

<p>Next, we’ll follow a similar set of steps for the position embeddings. We’ll start by initializing the position embeddings lookup table. Next, we’ll calculate the positions from the tokenizer output. Finally, we pass the token positions to the lookup table to get unique embeddings for each position.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Configure position embeddings lookup table
</span><span class="n">position_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span>
    <span class="n">num_embeddings</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">max_sequence_length</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Load pre-trained state
</span><span class="n">position_embeddings</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.embeddings.position_embeddings.weight</span><span class="sh">"</span><span class="p">],</span>
<span class="p">})</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;All keys matched successfully&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate token positions
</span><span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">values</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="n">positions</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0, 1, 2, 3, 4, 5])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Map token positions to embeddings
</span><span class="n">p</span> <span class="o">=</span> <span class="nf">position_embeddings</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span>

<span class="n">p</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([6, 768])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Show sample of position embeddings
</span><span class="n">p</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 1.8007e-02, -2.3798e-02, -3.5982e-02,  ...,  4.5726e-04,
          5.1363e-05,  1.5002e-02],
        [ 7.8592e-03,  4.8144e-03, -1.6093e-02,  ...,  2.9312e-02,
          2.7634e-02, -8.5431e-03],
        [-1.1663e-02, -3.1590e-03, -9.4000e-03,  ...,  1.4870e-02,
          2.1609e-02, -7.4069e-03],
        [-4.0848e-03, -1.1123e-02, -2.1704e-02,  ...,  1.8962e-02,
          4.6763e-03, -1.0220e-03],
        [-8.2666e-03, -4.1641e-03, -7.5136e-03,  ...,  1.9757e-02,
         -2.2192e-03,  3.8681e-03],
        [ 4.6293e-04, -1.8499e-02, -1.9709e-02,  ...,  5.4042e-03,
          1.8076e-02,  2.9490e-03]], grad_fn=&lt;EmbeddingBackward0&gt;)
</code></pre></div></div>

<p>Now that we have value and position embeddings, we add and normalize them to get the final “position-encoded token embeddings”.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Configure embeddings normalization
</span><span class="n">normalize_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span>
    <span class="n">normalized_shape</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> 
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Load pre-trained state
</span><span class="n">normalize_embeddings</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.embeddings.LayerNorm.weight</span><span class="sh">"</span><span class="p">],</span> 
    <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.embeddings.LayerNorm.bias</span><span class="sh">"</span><span class="p">],</span>
<span class="p">})</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;All keys matched successfully&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Add and normalize value and position embeddings
</span><span class="n">x</span> <span class="o">=</span> <span class="nf">normalize_embeddings</span><span class="p">(</span><span class="n">v</span> <span class="o">+</span> <span class="n">p</span><span class="p">)</span>

<span class="n">x</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([6, 768])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Show sample of token embeddings
</span><span class="n">x</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 0.3549, -0.1386, -0.2253,  ...,  0.1536,  0.0748,  0.1310],
        [ 0.2282,  0.5511, -0.5092,  ...,  0.6421,  0.9541,  0.3192],
        [ 1.4511, -0.0794,  0.2168,  ...,  0.2851,  1.0723, -0.0919],
        [-0.0564, -0.1761, -0.2870,  ...,  0.1442,  0.6767,  1.0396],
        [-1.1349, -0.5135, -0.4714,  ...,  0.3874, -1.0348, -0.2812],
        [-0.2980, -0.3332, -0.3742,  ..., -0.3392,  0.3764, -0.1298]],
       grad_fn=&lt;NativeLayerNormBackward0&gt;)
</code></pre></div></div>

<p>Congrats! You’ve converted the raw text “I love ice cream” into embeddings that encode both the token values and positions.</p>

<h2 id="context">Context</h2>

<p>In the previous stage, we mapped the token values and positions to embeddings. But these embeddings represent the tokens in <em>isolation</em>. The Context stage is responsible for infusing each embedding with contextual signals drawn from the entire sequence. At a conceptual level, this should be intuitive. The meaning of the word “ice” changes when you add “cream” after it.</p>

<center><img src="contextualized-embeddings.svg" width="632" /></center>

<p>The Context stage works by passing the token embeddings through multiple layers of attention and feedforward blocks. The attention blocks focus on relationships between tokens, augmenting each embedding with information drawn from the surrounding embeddings. The feedforward blocks focus on individual tokens, transforming the contextual clues added by attention with the non-linear transformation magic neural networks are famous for.</p>

<p>The following diagram illustrates the stack of Transformer layers in the Context stage. The contents of each layer are identical. By arranging the layers in a stack, the model builds context in small increments similar to the hierarchical features in a CNN. The main differences between popular Transformer models such as BERT and GPT come down to how these layers are configured.</p>

<center><img src="transformer-layers.svg" width="80%" /></center>

<p>As illustrated above, given input embeddings $X$, we can define the output embeddings $Z$ as:</p>

\[\begin{align}
Y &amp;= Normalize(X + Attention(X)) \\
Z &amp;= Normalize(Y + FNN(Y))
\end{align}\]

<h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3>

<p>The Attention block is the signature component of the Transformer architecture. It’s also one of the most complicated and likely the least familiar when you’re first learning about Transformers. We’ll walk through the core attention algorithm described in the original “All You Need is Attention” paper by Vaswani et al. one step at a time. At the end of the Context section, we’ll put all the pieces together.</p>

<p>Vaswani et al. described their attention algorithm as Scaled Dot-Product Attention (SDPA) and defined the standard attention equation everyone cites:</p>

\[\begin{equation}
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_K}})V
\end{equation}\]

<h3 id="queries-keys-values">Queries, Keys, Values</h3>

<p>The $Q$, $K$, and $V$ terms in the SDPA equation are “query”, “key”, and “value” matrices respectively. Each row in $Q$, $K$, and $V$ represents a token embedding that has been projected to distinct representation subspaces. Query embeddings represent selection criteria for the surrounding tokens that would add context to the current token definition. Key embeddings represent characteristics that satisfy the selection criteria. Value embeddings represent the contextual information one token transfers to another. Together, queries, keys, and values allow the attention mechanism to refine the representation of each token based on the surrounding tokens.</p>

<p>Given an $n \times d_{model}$ matrix of input embeddings $X$, we can expand on the SDPA equation by defining linear projections $Queries$, $Keys$, and $Values$:</p>

\[\begin{align}
SDPA(Q, K, V) &amp;= softmax(\frac{QK^T}{\sqrt{d_K}})V \\
\text{where } Q &amp;= Queries(X) \\
K &amp;= Keys(X) \\
V &amp;= Values(X)
\end{align}\]

<p>Enough LaTeX, let’s see what this looks like in Python.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Configure query, key, value projections
</span><span class="n">queries</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span>
    <span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span>
    <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">keys</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span>
    <span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> 
    <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span>
    <span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> 
    <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Load pre-trained state
</span><span class="n">queries</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.0.attention.q_lin.weight</span><span class="sh">"</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.0.attention.q_lin.bias</span><span class="sh">"</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">keys</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.0.attention.k_lin.weight</span><span class="sh">"</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.0.attention.k_lin.bias</span><span class="sh">"</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">values</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.0.attention.v_lin.weight</span><span class="sh">"</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.0.attention.k_lin.bias</span><span class="sh">"</span><span class="p">],</span>
<span class="p">})</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;All keys matched successfully&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Project token embeddings to query, key, and value spaces
</span><span class="n">q</span> <span class="o">=</span> <span class="nf">queries</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="nf">keys</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="nf">values</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(torch.Size([6, 768]), torch.Size([6, 768]), torch.Size([6, 768]))
</code></pre></div></div>

<p>We can see the projections generated unique query, key, and value embeddings for each of the 6 tokens <code class="language-plaintext highlighter-rouge">['[CLS]', 'i', 'love', 'ice', 'cream', '[SEP]']</code>.</p>

<h3 id="attention-weights">Attention Weights</h3>

<p>Now that we have $Q$, $K$, and $V$, we can delve into the SDPA equation itself. For each input embedding, SDPA calculates a weighted sum of the value projections for all the tokens in the sequence. We already saw the value projections are represented by <code class="language-plaintext highlighter-rouge">V</code>. The weights are represented by the softmax term:</p>

\[\begin{align}
softmax(\frac{QK^T}{\sqrt{d_K}})
\end{align}\]

<p>I wouldn’t hold it against you if it’s not immediately obvious what we get here. To see what’s happening, let’s break this down even further.</p>

<p>First, the $QK^T$ term calculates a $d_Q \times d_K$ matrix of the dot products of each query embedding with every key embedding. To see why, imagine we have 2 token embeddings of length 3.</p>

\[\begin{align}
QK^T
&amp;=
\begin{bmatrix}
q_{00} &amp; q_{01} &amp; q_{02} \\
q_{10} &amp; q_{11} &amp; q_{12}
\end{bmatrix}
\begin{bmatrix}
k_{00} &amp; k_{10} \\
k_{01} &amp; k_{11} \\
k_{02} &amp; k_{12}
\end{bmatrix}
=
\begin{bmatrix}
w_{00} &amp; w_{01} \\
w_{10} &amp; w_{11}
\end{bmatrix} \\
\text{where } w_{ij} &amp;= row(Q, i) \cdot row(K, j)
\end{align}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate similarity between Q and K
</span><span class="n">w</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">w</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([6, 6])
</code></pre></div></div>

<p>Second, the $1/\sqrt{d_K}$ term scales the dot products down to avoid pushing the softmax function into regions with very small gradients.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span> <span class="o">/=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">k</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="n">w</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([6, 6])
</code></pre></div></div>

<p>Finally, the softmax function normalizes the weights across the keys.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Normalize weights across keys
</span><span class="n">w</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">w</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[9.9964e-01, 8.3108e-06, 5.9084e-06, 1.2478e-05, 1.7158e-05, 3.1488e-04],
        [9.6971e-01, 3.5093e-04, 3.8457e-03, 1.4637e-04, 2.0632e-04, 2.5743e-02],
        [9.8888e-01, 1.4836e-03, 3.4143e-04, 3.4613e-04, 1.1235e-03, 7.8293e-03],
        [7.4676e-01, 3.8455e-05, 1.0549e-03, 8.3005e-05, 2.2695e-01, 2.5122e-02],
        [7.5151e-01, 9.0717e-06, 2.4871e-04, 2.3733e-01, 1.7790e-05, 1.0880e-02],
        [8.6867e-01, 4.3978e-04, 4.6904e-05, 7.9754e-05, 1.2606e-03, 1.2950e-01]],
       grad_fn=&lt;SoftmaxBackward0&gt;)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot weights for each query
</span><span class="n">_</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">gridspec_kw</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">wspace</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="sh">"</span><span class="s">hspace</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">})</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="o">//</span><span class="mi">3</span><span class="p">][</span><span class="n">i</span><span class="o">%</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">])),</span> <span class="n">y</span><span class="o">=</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">detach</span><span class="p">(),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Query </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Keys</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Weight</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="928d66b5ce2483cf910cb4e3d861fc17.png" alt="png" /></p>

<h3 id="attention-output">Attention Output</h3>

<p>Now that we have the attention weights, we can apply them to the values. This will give us a weighted sum of contextual information. However, the answer is still in “value space”. Before we combine them with the token embeddings, we’ll project them back to “model space”.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compute weighted combination of values
</span><span class="n">a</span> <span class="o">=</span> <span class="n">w</span> <span class="o">@</span> <span class="n">v</span>
<span class="n">a</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([6, 768])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Configure output projection
</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>

<span class="c1"># Load pre-trained state
</span><span class="n">outputs</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.0.attention.out_lin.weight</span><span class="sh">"</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.0.attention.out_lin.bias</span><span class="sh">"</span><span class="p">],</span>
<span class="p">})</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;All keys matched successfully&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Project attention embeddings back to model space
</span><span class="n">a</span> <span class="o">=</span> <span class="nf">outputs</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">a</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([6, 768])
</code></pre></div></div>

<h3 id="multi-head-attention">Multi-Head Attention</h3>

<p>At this point, we’ve walked through the core SDPA algorithm step-by-step. However, we’re not quite done. Vaswani et al. realized there are more than one set of relationships involved in transferring context across tokens. A single application of SDPA would effectively water these down by averaging these together. The solution is to apply SDPA multiple times on separate query, key, and value embeddings. Each of these is referred to as an “attention head”. Each head is isolated, leaving it free to learn distinct relational structures.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">split_heads</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">d_head</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">combine_heads</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nf">int</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">d_head</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Render query, key, value dimensions before we split
</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(torch.Size([6, 768]), torch.Size([6, 768]), torch.Size([6, 768]))
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Split queries, keys, values into separate heads
</span><span class="n">q</span> <span class="o">=</span> <span class="nf">split_heads</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="nf">split_heads</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="nf">split_heads</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

<span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(torch.Size([12, 6, 64]), torch.Size([12, 6, 64]), torch.Size([12, 6, 64]))
</code></pre></div></div>

<p>We can see that the queries, keys, and values have been split into 12 heads, leaving each of the original 768-element query, key, and value embeddings is now 64 elements long.</p>

<p>Next, let’s recompute the attention embeddings.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compute attention for all heads in parallel
</span><span class="n">a</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">d_head</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">v</span>

<span class="n">a</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([12, 6, 64])
</code></pre></div></div>

<p>While the attention code is the same, you can see the attention values are still split into heads. Next, we’ll recombine them before applying the final output projection.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Recombine heads
</span><span class="n">a</span> <span class="o">=</span> <span class="nf">combine_heads</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">a</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([6, 768])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Project attention embeddings back to model space
</span><span class="n">a</span> <span class="o">=</span> <span class="nf">outputs</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">a</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([6, 768])
</code></pre></div></div>

<h3 id="add-and-normalize">Add and Normalize</h3>

<p>Before we get to the FNN, we’ll combine the attention embeddings with input embeddings the same way we combined the value and position embeddings.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Configure attention normalization
</span><span class="n">normalize_attention</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span>

<span class="c1"># Load pre-trained state
</span><span class="n">normalize_attention</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.0.sa_layer_norm.weight</span><span class="sh">"</span><span class="p">],</span> 
    <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.0.sa_layer_norm.bias</span><span class="sh">"</span><span class="p">],</span>
<span class="p">})</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;All keys matched successfully&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Combine attention with input embeddings
</span><span class="n">y</span> <span class="o">=</span> <span class="nf">normalize_attention</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">a</span><span class="p">)</span>

<span class="n">y</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([6, 768])
</code></pre></div></div>

<h3 id="fnn">FNN</h3>

<p>The FNN block is a straightforward fully connected multi-layer perceptron.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Configure FNN
</span><span class="n">fnn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_fnn</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_fnn</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Load pre-trained state
</span><span class="n">fnn</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">0.weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.0.ffn.lin1.weight</span><span class="sh">"</span><span class="p">],</span> 
    <span class="sh">"</span><span class="s">0.bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.0.ffn.lin1.bias</span><span class="sh">"</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">2.weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.0.ffn.lin2.weight</span><span class="sh">"</span><span class="p">],</span> 
    <span class="sh">"</span><span class="s">2.bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.0.ffn.lin2.bias</span><span class="sh">"</span><span class="p">],</span>
<span class="p">})</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;All keys matched successfully&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Transform attention outputs
</span><span class="n">f</span> <span class="o">=</span> <span class="nf">fnn</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="n">f</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([6, 768])
</code></pre></div></div>

<h3 id="add-and-normalize-1">Add and Normalize</h3>

<p>Next, we combine the FNN outputs with the attention outputs.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Configure attention normalization
</span><span class="n">normalize_fnn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span>

<span class="c1"># Load pre-trained state
</span><span class="n">normalize_fnn</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.0.output_layer_norm.weight</span><span class="sh">"</span><span class="p">],</span> 
    <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.0.output_layer_norm.bias</span><span class="sh">"</span><span class="p">],</span>
<span class="p">})</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;All keys matched successfully&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">z</span> <span class="o">=</span> <span class="nf">normalize_fnn</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">f</span><span class="p">)</span>
<span class="n">z</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([6, 768])
</code></pre></div></div>

<h3 id="full-stack">Full Stack</h3>

<p>Quick recap. Given input embeddings $X$, we added attention embeddings to get $Y$, and added transformed embeddings to get $Z$. Next, we combine all of these elements together and repeat for each layer in the stack. While you would normally create a stack of torch modules, we run the layers in a loop instead to make it easier to see what’s happening.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
    
    <span class="c1"># Configure query, key, value, and output projections
</span>    <span class="n">queries</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>

    <span class="c1"># Load pre-trained state
</span>    <span class="n">queries</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
        <span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">.attention.q_lin.weight</span><span class="sh">"</span><span class="p">],</span>
        <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">.attention.q_lin.bias</span><span class="sh">"</span><span class="p">],</span>
    <span class="p">})</span>
    <span class="n">keys</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
        <span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">.attention.k_lin.weight</span><span class="sh">"</span><span class="p">],</span>
        <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">.attention.k_lin.bias</span><span class="sh">"</span><span class="p">],</span>
    <span class="p">})</span>
    <span class="n">values</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
        <span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">.attention.v_lin.weight</span><span class="sh">"</span><span class="p">],</span>
        <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">.attention.v_lin.bias</span><span class="sh">"</span><span class="p">],</span>
    <span class="p">})</span>
    <span class="n">outputs</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
        <span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">.attention.out_lin.weight</span><span class="sh">"</span><span class="p">],</span>
        <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">.attention.out_lin.bias</span><span class="sh">"</span><span class="p">],</span>
    <span class="p">})</span>
    
    <span class="c1"># Project x to query, key, and value spaces
</span>    <span class="n">q</span> <span class="o">=</span> <span class="nf">queries</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="nf">keys</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="nf">values</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="c1"># Split q, k, v into separate heads
</span>    <span class="n">q</span> <span class="o">=</span> <span class="nf">split_heads</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="nf">split_heads</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="nf">split_heads</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

    <span class="c1"># Compute attention for all heads in parallel
</span>    <span class="n">a</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">d_head</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">v</span>
    
    <span class="c1"># Recombine heads
</span>    <span class="n">a</span> <span class="o">=</span> <span class="nf">combine_heads</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    
    <span class="c1"># Project attention embeddings back to model space
</span>    <span class="n">a</span> <span class="o">=</span> <span class="nf">outputs</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">a</span>


<span class="k">def</span> <span class="nf">normalize_attention</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
    
    <span class="c1"># Configure attention normalization
</span>    <span class="n">xform</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span>
    
    <span class="c1"># Load pre-trained state
</span>    <span class="n">xform</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
        <span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">.sa_layer_norm.weight</span><span class="sh">"</span><span class="p">],</span> 
        <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">.sa_layer_norm.bias</span><span class="sh">"</span><span class="p">],</span>
    <span class="p">})</span>

    <span class="k">return</span> <span class="nf">xform</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">fnn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
    
    <span class="c1"># Configure FNN
</span>    <span class="n">xform</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_fnn</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_fnn</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">),</span>
    <span class="p">)</span>
    
    <span class="c1"># Load pre-trained state
</span>    <span class="n">xform</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
        <span class="sh">"</span><span class="s">0.weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">.ffn.lin1.weight</span><span class="sh">"</span><span class="p">],</span> 
        <span class="sh">"</span><span class="s">0.bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">.ffn.lin1.bias</span><span class="sh">"</span><span class="p">],</span>
        <span class="sh">"</span><span class="s">2.weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">.ffn.lin2.weight</span><span class="sh">"</span><span class="p">],</span> 
        <span class="sh">"</span><span class="s">2.bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">.ffn.lin2.bias</span><span class="sh">"</span><span class="p">],</span>
    <span class="p">})</span>
    
    <span class="c1"># Transform attention outputs
</span>    <span class="n">f</span> <span class="o">=</span> <span class="nf">xform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">f</span>


<span class="k">def</span> <span class="nf">normalize_fnn</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
    
    <span class="c1"># Configure attention normalization
</span>    <span class="n">xform</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span>
    
    <span class="c1"># Load pre-trained state
</span>    <span class="n">xform</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
        <span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">.output_layer_norm.weight</span><span class="sh">"</span><span class="p">],</span> 
        <span class="sh">"</span><span class="s">bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">distilbert.transformer.layer.</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s">.output_layer_norm.bias</span><span class="sh">"</span><span class="p">],</span>
    <span class="p">})</span>
    
    <span class="k">return</span> <span class="nf">xform</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initialize loop
</span><span class="n">z_i</span> <span class="o">=</span> <span class="n">x</span>

<span class="c1"># Apply layer logic in a loop
</span><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_layers</span><span class="p">):</span>
    
    <span class="c1"># Use previous layer's outputs as inputs
</span>    <span class="n">x_i</span> <span class="o">=</span> <span class="n">z_i</span>

    <span class="c1"># Attention
</span>    <span class="n">y_i</span> <span class="o">=</span> <span class="nf">normalize_attention</span><span class="p">(</span><span class="n">x_i</span> <span class="o">+</span> <span class="nf">attention</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">),</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>

    <span class="c1"># FNN
</span>    <span class="n">z_i</span> <span class="o">=</span> <span class="nf">normalize_fnn</span><span class="p">(</span><span class="n">y_i</span> <span class="o">+</span> <span class="nf">fnn</span><span class="p">(</span><span class="n">y_i</span><span class="p">,</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">),</span> <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>

<span class="c1"># Save outputs from last layer
</span><span class="n">z</span> <span class="o">=</span> <span class="n">z_i</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">z</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 3.6173e-01, -1.3168e-01,  3.5340e-02,  ...,  4.4015e-01,
          1.0666e+00, -1.9293e-01],
        [ 7.3341e-01,  4.9823e-02, -1.7590e-02,  ...,  5.0063e-01,
          1.1480e+00, -1.2997e-01],
        [ 1.1230e+00,  2.7603e-01,  3.2096e-01,  ...,  1.8820e-01,
          1.0586e+00, -1.2496e-01],
        [ 4.8728e-01,  1.4863e-02,  4.2930e-01,  ...,  4.8993e-01,
          7.9435e-01,  1.2331e-01],
        [ 1.0595e-03, -1.4508e-01,  2.8892e-01,  ...,  5.5342e-01,
          7.9370e-01, -9.0899e-02],
        [ 1.1021e+00,  8.6115e-02,  5.7461e-01,  ...,  6.8800e-01,
          5.6345e-01, -6.6278e-01]], grad_fn=&lt;NativeLayerNormBackward0&gt;)
</code></pre></div></div>

<h2 id="head">Head</h2>

<p>As the final stage in the Transformer pipeline, the Head stage maps the contextualized embeddings to task-specific predictions. In our case, the Head stage is responsible for turning the contextualized embeddings into a binary classifier that predicts whether the original text contains positive or negative sentiments. This sounds like a straightforward output layer until you realize that instead of one set of features, we have a sequence of features. And the length of the sequence is arbitrary. How do you connect an arbitrary length sequence of feature vectors to an output layer?</p>

<p>The trick is hiding in our contextualized embeddings. Each input embedding represents a single token in isolation. But the output embeddings have been infused with information from all of the tokens. This is why the common practice is to simply take the first output embedding and drop the rest. The first embedding represents the start of sequence marker <code class="language-plaintext highlighter-rouge">[CLS]</code>. Since the <code class="language-plaintext highlighter-rouge">[CLS]</code> marker token is added to every sequence, the first input embedding is always the same. In contrast, the first output embedding uniquely represents one specific sequence.</p>

<p>If we let the first output embedding represent the entire sequence, then we have a single feature vector that’s easy to connect to any task-specific output layer we need.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Use [CLS] embedding to represent the entire sequence
</span><span class="n">features</span> <span class="o">=</span> <span class="n">z</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">features</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([768])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Configure classifier
</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">n_labels</span><span class="p">),</span>
<span class="p">)</span>
    
<span class="c1"># Load pre-trained state
</span><span class="n">classifier</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">0.weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">pre_classifier.weight</span><span class="sh">"</span><span class="p">],</span> 
    <span class="sh">"</span><span class="s">0.bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">pre_classifier.bias</span><span class="sh">"</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">2.weight</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">classifier.weight</span><span class="sh">"</span><span class="p">],</span> 
    <span class="sh">"</span><span class="s">2.bias</span><span class="sh">"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">[</span><span class="sh">"</span><span class="s">classifier.bias</span><span class="sh">"</span><span class="p">],</span>
<span class="p">})</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;All keys matched successfully&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Classify features
</span><span class="n">prediction1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="nf">classifier</span><span class="p">(</span><span class="n">features</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span>
<span class="n">prediction1</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.9998118281364441
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Verify custom results match off-the-shelf ones
</span><span class="n">prediction2</span> <span class="o">=</span> <span class="nf">transformer</span><span class="p">(</span><span class="sh">"</span><span class="s">I love ice cream</span><span class="sh">"</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">score</span><span class="sh">"</span><span class="p">]</span>
<span class="n">prediction2</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.9998118281364441
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">assert</span> <span class="n">prediction1</span> <span class="o">==</span> <span class="nf">approx</span><span class="p">(</span><span class="n">prediction2</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="discussion">Discussion</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h2 id="references">References</h2>

<p>Bengio, Yoshua, Réjean Ducharme, and Pascal Vincent. “A Neural Probabilistic Language Model.” In Advances in Neural Information Processing Systems, Vol. 13. MIT Press, 2000. <a href="https://proceedings.neurips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html">https://proceedings.neurips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html</a>.</p>

<p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv, May 24, 2019. <a href="https://doi.org/10.48550/arXiv.1810.04805">https://doi.org/10.48550/arXiv.1810.04805</a>.</p>

<p>“Transformer Explainer: LLM Transformer Model Visually Explained.” Accessed August 29, 2024. <a href="https://poloclub.github.io/transformer-explainer/">https://poloclub.github.io/transformer-explainer/</a>.</p>

<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” arXiv, August 1, 2023. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a></p>

<p>Xu, Peng, Xiatian Zhu, and David A. Clifton. “Multimodal Learning with Transformers: A Survey.” arXiv, May 9, 2023. <a href="https://doi.org/10.48550/arXiv.2206.06488">https://doi.org/10.48550/arXiv.2206.06488</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[There is something special about Transformers. Since first introduced in 2017, the Transformer architecture has redefined the entire NLP category and is quickly spreading to other types of machine learning. Transformers, and the language embeddings at their core, seem to have hit on some underlying fundamental substrate that transcends boundaries, making them far more generalizable than previous approaches. Anyone who wants to contribute to the future of AI should clearly be studying Transformers inside and out.]]></summary></entry></feed>