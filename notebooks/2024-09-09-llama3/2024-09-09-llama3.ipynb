{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90453ea1-d7da-480b-be6b-86baae89d3b6",
   "metadata": {},
   "source": [
    "# Transformer Teardown: Llama 3\n",
    "\n",
    "> Use what we learned about BERT as a baseline to explore SOTA Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa710c-a778-434e-9b58-b78675530e3a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f939cad5-82e6-49dd-9871-52bda963abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas import Series\n",
    "from pytest import approx\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu, softmax\n",
    "import transformers\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, repeat_kv\n",
    "\n",
    "from stickshift import default_arg, take\n",
    "from stickshift.models import llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c7fa65-b60e-4d6b-9e38-9a840e574743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure gpu\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22788340-4c34-4daf-98fa-9320516fe291",
   "metadata": {},
   "source": [
    "# Text Generation with Llama 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3ef8424-2fa0-4b65-8a29-666cab09b76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668ab5fd0be04bcdb07ce018cb926c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create off-the-shelf text generation transformer\n",
    "transformer = transformers.pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989806c0-ec4d-4bf5-9279-eed474652392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is the capital of Massachusetts? Boston.\\nWhat is the capital of Massachusetts?\\nA. Boston'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer(\"What is the capital of Massachusetts?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1bfa9c-ed51-4ea0-a02c-2d3d8a79beb8",
   "metadata": {},
   "source": [
    "# Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e622ec6a-4f36-4061-87f9-b79e4cc8a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model config and pre-trained parameters\n",
    "config = llama.config(transformer.model)\n",
    "parameters = transformer.model.state_dict()\n",
    "llama_model = transformer.model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1410ef2f-5a30-4d39-9027-24c55df01e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.embed_tokens.weight',\n",
       " 'model.layers.0.self_attn.q_proj.weight',\n",
       " 'model.layers.0.self_attn.k_proj.weight',\n",
       " 'model.layers.0.self_attn.v_proj.weight',\n",
       " 'model.layers.0.self_attn.o_proj.weight',\n",
       " 'model.layers.0.mlp.gate_proj.weight',\n",
       " 'model.layers.0.mlp.up_proj.weight',\n",
       " 'model.layers.0.mlp.down_proj.weight',\n",
       " 'model.layers.0.input_layernorm.weight',\n",
       " 'model.layers.0.post_attention_layernorm.weight',\n",
       " 'model.layers.1.self_attn.q_proj.weight',\n",
       " 'model.layers.1.self_attn.k_proj.weight',\n",
       " 'model.layers.1.self_attn.v_proj.weight',\n",
       " 'model.layers.1.self_attn.o_proj.weight',\n",
       " 'model.layers.1.mlp.gate_proj.weight',\n",
       " 'model.layers.1.mlp.up_proj.weight',\n",
       " 'model.layers.1.mlp.down_proj.weight',\n",
       " 'model.layers.1.input_layernorm.weight',\n",
       " 'model.layers.1.post_attention_layernorm.weight',\n",
       " 'model.layers.2.self_attn.q_proj.weight',\n",
       " 'model.layers.2.self_attn.k_proj.weight',\n",
       " 'model.layers.2.self_attn.v_proj.weight',\n",
       " 'model.layers.2.self_attn.o_proj.weight',\n",
       " 'model.layers.2.mlp.gate_proj.weight',\n",
       " 'model.layers.2.mlp.up_proj.weight',\n",
       " 'model.layers.2.mlp.down_proj.weight',\n",
       " 'model.layers.2.input_layernorm.weight',\n",
       " 'model.layers.2.post_attention_layernorm.weight',\n",
       " 'model.layers.3.self_attn.q_proj.weight',\n",
       " 'model.layers.3.self_attn.k_proj.weight',\n",
       " 'model.layers.3.self_attn.v_proj.weight',\n",
       " 'model.layers.3.self_attn.o_proj.weight',\n",
       " 'model.layers.3.mlp.gate_proj.weight',\n",
       " 'model.layers.3.mlp.up_proj.weight',\n",
       " 'model.layers.3.mlp.down_proj.weight',\n",
       " 'model.layers.3.input_layernorm.weight',\n",
       " 'model.layers.3.post_attention_layernorm.weight',\n",
       " 'model.layers.4.self_attn.q_proj.weight',\n",
       " 'model.layers.4.self_attn.k_proj.weight',\n",
       " 'model.layers.4.self_attn.v_proj.weight',\n",
       " 'model.layers.4.self_attn.o_proj.weight',\n",
       " 'model.layers.4.mlp.gate_proj.weight',\n",
       " 'model.layers.4.mlp.up_proj.weight',\n",
       " 'model.layers.4.mlp.down_proj.weight',\n",
       " 'model.layers.4.input_layernorm.weight',\n",
       " 'model.layers.4.post_attention_layernorm.weight',\n",
       " 'model.layers.5.self_attn.q_proj.weight',\n",
       " 'model.layers.5.self_attn.k_proj.weight',\n",
       " 'model.layers.5.self_attn.v_proj.weight',\n",
       " 'model.layers.5.self_attn.o_proj.weight',\n",
       " 'model.layers.5.mlp.gate_proj.weight',\n",
       " 'model.layers.5.mlp.up_proj.weight',\n",
       " 'model.layers.5.mlp.down_proj.weight',\n",
       " 'model.layers.5.input_layernorm.weight',\n",
       " 'model.layers.5.post_attention_layernorm.weight',\n",
       " 'model.layers.6.self_attn.q_proj.weight',\n",
       " 'model.layers.6.self_attn.k_proj.weight',\n",
       " 'model.layers.6.self_attn.v_proj.weight',\n",
       " 'model.layers.6.self_attn.o_proj.weight',\n",
       " 'model.layers.6.mlp.gate_proj.weight',\n",
       " 'model.layers.6.mlp.up_proj.weight',\n",
       " 'model.layers.6.mlp.down_proj.weight',\n",
       " 'model.layers.6.input_layernorm.weight',\n",
       " 'model.layers.6.post_attention_layernorm.weight',\n",
       " 'model.layers.7.self_attn.q_proj.weight',\n",
       " 'model.layers.7.self_attn.k_proj.weight',\n",
       " 'model.layers.7.self_attn.v_proj.weight',\n",
       " 'model.layers.7.self_attn.o_proj.weight',\n",
       " 'model.layers.7.mlp.gate_proj.weight',\n",
       " 'model.layers.7.mlp.up_proj.weight',\n",
       " 'model.layers.7.mlp.down_proj.weight',\n",
       " 'model.layers.7.input_layernorm.weight',\n",
       " 'model.layers.7.post_attention_layernorm.weight',\n",
       " 'model.layers.8.self_attn.q_proj.weight',\n",
       " 'model.layers.8.self_attn.k_proj.weight',\n",
       " 'model.layers.8.self_attn.v_proj.weight',\n",
       " 'model.layers.8.self_attn.o_proj.weight',\n",
       " 'model.layers.8.mlp.gate_proj.weight',\n",
       " 'model.layers.8.mlp.up_proj.weight',\n",
       " 'model.layers.8.mlp.down_proj.weight',\n",
       " 'model.layers.8.input_layernorm.weight',\n",
       " 'model.layers.8.post_attention_layernorm.weight',\n",
       " 'model.layers.9.self_attn.q_proj.weight',\n",
       " 'model.layers.9.self_attn.k_proj.weight',\n",
       " 'model.layers.9.self_attn.v_proj.weight',\n",
       " 'model.layers.9.self_attn.o_proj.weight',\n",
       " 'model.layers.9.mlp.gate_proj.weight',\n",
       " 'model.layers.9.mlp.up_proj.weight',\n",
       " 'model.layers.9.mlp.down_proj.weight',\n",
       " 'model.layers.9.input_layernorm.weight',\n",
       " 'model.layers.9.post_attention_layernorm.weight',\n",
       " 'model.layers.10.self_attn.q_proj.weight',\n",
       " 'model.layers.10.self_attn.k_proj.weight',\n",
       " 'model.layers.10.self_attn.v_proj.weight',\n",
       " 'model.layers.10.self_attn.o_proj.weight',\n",
       " 'model.layers.10.mlp.gate_proj.weight',\n",
       " 'model.layers.10.mlp.up_proj.weight',\n",
       " 'model.layers.10.mlp.down_proj.weight',\n",
       " 'model.layers.10.input_layernorm.weight',\n",
       " 'model.layers.10.post_attention_layernorm.weight',\n",
       " 'model.layers.11.self_attn.q_proj.weight',\n",
       " 'model.layers.11.self_attn.k_proj.weight',\n",
       " 'model.layers.11.self_attn.v_proj.weight',\n",
       " 'model.layers.11.self_attn.o_proj.weight',\n",
       " 'model.layers.11.mlp.gate_proj.weight',\n",
       " 'model.layers.11.mlp.up_proj.weight',\n",
       " 'model.layers.11.mlp.down_proj.weight',\n",
       " 'model.layers.11.input_layernorm.weight',\n",
       " 'model.layers.11.post_attention_layernorm.weight',\n",
       " 'model.layers.12.self_attn.q_proj.weight',\n",
       " 'model.layers.12.self_attn.k_proj.weight',\n",
       " 'model.layers.12.self_attn.v_proj.weight',\n",
       " 'model.layers.12.self_attn.o_proj.weight',\n",
       " 'model.layers.12.mlp.gate_proj.weight',\n",
       " 'model.layers.12.mlp.up_proj.weight',\n",
       " 'model.layers.12.mlp.down_proj.weight',\n",
       " 'model.layers.12.input_layernorm.weight',\n",
       " 'model.layers.12.post_attention_layernorm.weight',\n",
       " 'model.layers.13.self_attn.q_proj.weight',\n",
       " 'model.layers.13.self_attn.k_proj.weight',\n",
       " 'model.layers.13.self_attn.v_proj.weight',\n",
       " 'model.layers.13.self_attn.o_proj.weight',\n",
       " 'model.layers.13.mlp.gate_proj.weight',\n",
       " 'model.layers.13.mlp.up_proj.weight',\n",
       " 'model.layers.13.mlp.down_proj.weight',\n",
       " 'model.layers.13.input_layernorm.weight',\n",
       " 'model.layers.13.post_attention_layernorm.weight',\n",
       " 'model.layers.14.self_attn.q_proj.weight',\n",
       " 'model.layers.14.self_attn.k_proj.weight',\n",
       " 'model.layers.14.self_attn.v_proj.weight',\n",
       " 'model.layers.14.self_attn.o_proj.weight',\n",
       " 'model.layers.14.mlp.gate_proj.weight',\n",
       " 'model.layers.14.mlp.up_proj.weight',\n",
       " 'model.layers.14.mlp.down_proj.weight',\n",
       " 'model.layers.14.input_layernorm.weight',\n",
       " 'model.layers.14.post_attention_layernorm.weight',\n",
       " 'model.layers.15.self_attn.q_proj.weight',\n",
       " 'model.layers.15.self_attn.k_proj.weight',\n",
       " 'model.layers.15.self_attn.v_proj.weight',\n",
       " 'model.layers.15.self_attn.o_proj.weight',\n",
       " 'model.layers.15.mlp.gate_proj.weight',\n",
       " 'model.layers.15.mlp.up_proj.weight',\n",
       " 'model.layers.15.mlp.down_proj.weight',\n",
       " 'model.layers.15.input_layernorm.weight',\n",
       " 'model.layers.15.post_attention_layernorm.weight',\n",
       " 'model.layers.16.self_attn.q_proj.weight',\n",
       " 'model.layers.16.self_attn.k_proj.weight',\n",
       " 'model.layers.16.self_attn.v_proj.weight',\n",
       " 'model.layers.16.self_attn.o_proj.weight',\n",
       " 'model.layers.16.mlp.gate_proj.weight',\n",
       " 'model.layers.16.mlp.up_proj.weight',\n",
       " 'model.layers.16.mlp.down_proj.weight',\n",
       " 'model.layers.16.input_layernorm.weight',\n",
       " 'model.layers.16.post_attention_layernorm.weight',\n",
       " 'model.layers.17.self_attn.q_proj.weight',\n",
       " 'model.layers.17.self_attn.k_proj.weight',\n",
       " 'model.layers.17.self_attn.v_proj.weight',\n",
       " 'model.layers.17.self_attn.o_proj.weight',\n",
       " 'model.layers.17.mlp.gate_proj.weight',\n",
       " 'model.layers.17.mlp.up_proj.weight',\n",
       " 'model.layers.17.mlp.down_proj.weight',\n",
       " 'model.layers.17.input_layernorm.weight',\n",
       " 'model.layers.17.post_attention_layernorm.weight',\n",
       " 'model.layers.18.self_attn.q_proj.weight',\n",
       " 'model.layers.18.self_attn.k_proj.weight',\n",
       " 'model.layers.18.self_attn.v_proj.weight',\n",
       " 'model.layers.18.self_attn.o_proj.weight',\n",
       " 'model.layers.18.mlp.gate_proj.weight',\n",
       " 'model.layers.18.mlp.up_proj.weight',\n",
       " 'model.layers.18.mlp.down_proj.weight',\n",
       " 'model.layers.18.input_layernorm.weight',\n",
       " 'model.layers.18.post_attention_layernorm.weight',\n",
       " 'model.layers.19.self_attn.q_proj.weight',\n",
       " 'model.layers.19.self_attn.k_proj.weight',\n",
       " 'model.layers.19.self_attn.v_proj.weight',\n",
       " 'model.layers.19.self_attn.o_proj.weight',\n",
       " 'model.layers.19.mlp.gate_proj.weight',\n",
       " 'model.layers.19.mlp.up_proj.weight',\n",
       " 'model.layers.19.mlp.down_proj.weight',\n",
       " 'model.layers.19.input_layernorm.weight',\n",
       " 'model.layers.19.post_attention_layernorm.weight',\n",
       " 'model.layers.20.self_attn.q_proj.weight',\n",
       " 'model.layers.20.self_attn.k_proj.weight',\n",
       " 'model.layers.20.self_attn.v_proj.weight',\n",
       " 'model.layers.20.self_attn.o_proj.weight',\n",
       " 'model.layers.20.mlp.gate_proj.weight',\n",
       " 'model.layers.20.mlp.up_proj.weight',\n",
       " 'model.layers.20.mlp.down_proj.weight',\n",
       " 'model.layers.20.input_layernorm.weight',\n",
       " 'model.layers.20.post_attention_layernorm.weight',\n",
       " 'model.layers.21.self_attn.q_proj.weight',\n",
       " 'model.layers.21.self_attn.k_proj.weight',\n",
       " 'model.layers.21.self_attn.v_proj.weight',\n",
       " 'model.layers.21.self_attn.o_proj.weight',\n",
       " 'model.layers.21.mlp.gate_proj.weight',\n",
       " 'model.layers.21.mlp.up_proj.weight',\n",
       " 'model.layers.21.mlp.down_proj.weight',\n",
       " 'model.layers.21.input_layernorm.weight',\n",
       " 'model.layers.21.post_attention_layernorm.weight',\n",
       " 'model.layers.22.self_attn.q_proj.weight',\n",
       " 'model.layers.22.self_attn.k_proj.weight',\n",
       " 'model.layers.22.self_attn.v_proj.weight',\n",
       " 'model.layers.22.self_attn.o_proj.weight',\n",
       " 'model.layers.22.mlp.gate_proj.weight',\n",
       " 'model.layers.22.mlp.up_proj.weight',\n",
       " 'model.layers.22.mlp.down_proj.weight',\n",
       " 'model.layers.22.input_layernorm.weight',\n",
       " 'model.layers.22.post_attention_layernorm.weight',\n",
       " 'model.layers.23.self_attn.q_proj.weight',\n",
       " 'model.layers.23.self_attn.k_proj.weight',\n",
       " 'model.layers.23.self_attn.v_proj.weight',\n",
       " 'model.layers.23.self_attn.o_proj.weight',\n",
       " 'model.layers.23.mlp.gate_proj.weight',\n",
       " 'model.layers.23.mlp.up_proj.weight',\n",
       " 'model.layers.23.mlp.down_proj.weight',\n",
       " 'model.layers.23.input_layernorm.weight',\n",
       " 'model.layers.23.post_attention_layernorm.weight',\n",
       " 'model.layers.24.self_attn.q_proj.weight',\n",
       " 'model.layers.24.self_attn.k_proj.weight',\n",
       " 'model.layers.24.self_attn.v_proj.weight',\n",
       " 'model.layers.24.self_attn.o_proj.weight',\n",
       " 'model.layers.24.mlp.gate_proj.weight',\n",
       " 'model.layers.24.mlp.up_proj.weight',\n",
       " 'model.layers.24.mlp.down_proj.weight',\n",
       " 'model.layers.24.input_layernorm.weight',\n",
       " 'model.layers.24.post_attention_layernorm.weight',\n",
       " 'model.layers.25.self_attn.q_proj.weight',\n",
       " 'model.layers.25.self_attn.k_proj.weight',\n",
       " 'model.layers.25.self_attn.v_proj.weight',\n",
       " 'model.layers.25.self_attn.o_proj.weight',\n",
       " 'model.layers.25.mlp.gate_proj.weight',\n",
       " 'model.layers.25.mlp.up_proj.weight',\n",
       " 'model.layers.25.mlp.down_proj.weight',\n",
       " 'model.layers.25.input_layernorm.weight',\n",
       " 'model.layers.25.post_attention_layernorm.weight',\n",
       " 'model.layers.26.self_attn.q_proj.weight',\n",
       " 'model.layers.26.self_attn.k_proj.weight',\n",
       " 'model.layers.26.self_attn.v_proj.weight',\n",
       " 'model.layers.26.self_attn.o_proj.weight',\n",
       " 'model.layers.26.mlp.gate_proj.weight',\n",
       " 'model.layers.26.mlp.up_proj.weight',\n",
       " 'model.layers.26.mlp.down_proj.weight',\n",
       " 'model.layers.26.input_layernorm.weight',\n",
       " 'model.layers.26.post_attention_layernorm.weight',\n",
       " 'model.layers.27.self_attn.q_proj.weight',\n",
       " 'model.layers.27.self_attn.k_proj.weight',\n",
       " 'model.layers.27.self_attn.v_proj.weight',\n",
       " 'model.layers.27.self_attn.o_proj.weight',\n",
       " 'model.layers.27.mlp.gate_proj.weight',\n",
       " 'model.layers.27.mlp.up_proj.weight',\n",
       " 'model.layers.27.mlp.down_proj.weight',\n",
       " 'model.layers.27.input_layernorm.weight',\n",
       " 'model.layers.27.post_attention_layernorm.weight',\n",
       " 'model.layers.28.self_attn.q_proj.weight',\n",
       " 'model.layers.28.self_attn.k_proj.weight',\n",
       " 'model.layers.28.self_attn.v_proj.weight',\n",
       " 'model.layers.28.self_attn.o_proj.weight',\n",
       " 'model.layers.28.mlp.gate_proj.weight',\n",
       " 'model.layers.28.mlp.up_proj.weight',\n",
       " 'model.layers.28.mlp.down_proj.weight',\n",
       " 'model.layers.28.input_layernorm.weight',\n",
       " 'model.layers.28.post_attention_layernorm.weight',\n",
       " 'model.layers.29.self_attn.q_proj.weight',\n",
       " 'model.layers.29.self_attn.k_proj.weight',\n",
       " 'model.layers.29.self_attn.v_proj.weight',\n",
       " 'model.layers.29.self_attn.o_proj.weight',\n",
       " 'model.layers.29.mlp.gate_proj.weight',\n",
       " 'model.layers.29.mlp.up_proj.weight',\n",
       " 'model.layers.29.mlp.down_proj.weight',\n",
       " 'model.layers.29.input_layernorm.weight',\n",
       " 'model.layers.29.post_attention_layernorm.weight',\n",
       " 'model.layers.30.self_attn.q_proj.weight',\n",
       " 'model.layers.30.self_attn.k_proj.weight',\n",
       " 'model.layers.30.self_attn.v_proj.weight',\n",
       " 'model.layers.30.self_attn.o_proj.weight',\n",
       " 'model.layers.30.mlp.gate_proj.weight',\n",
       " 'model.layers.30.mlp.up_proj.weight',\n",
       " 'model.layers.30.mlp.down_proj.weight',\n",
       " 'model.layers.30.input_layernorm.weight',\n",
       " 'model.layers.30.post_attention_layernorm.weight',\n",
       " 'model.layers.31.self_attn.q_proj.weight',\n",
       " 'model.layers.31.self_attn.k_proj.weight',\n",
       " 'model.layers.31.self_attn.v_proj.weight',\n",
       " 'model.layers.31.self_attn.o_proj.weight',\n",
       " 'model.layers.31.mlp.gate_proj.weight',\n",
       " 'model.layers.31.mlp.up_proj.weight',\n",
       " 'model.layers.31.mlp.down_proj.weight',\n",
       " 'model.layers.31.input_layernorm.weight',\n",
       " 'model.layers.31.post_attention_layernorm.weight',\n",
       " 'model.norm.weight',\n",
       " 'lm_head.weight']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28a4ea80-b639-4b2a-b054-7791a2f86a0a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_state(*args, layer=None):\n",
    "    # Defaults\n",
    "    layer = default_arg(layer, lambda: 0)\n",
    "\n",
    "    for module, key in take(2, args):\n",
    "        match key:\n",
    "            case \"value_embeddings\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[\"model.embed_tokens.weight\"],\n",
    "                })\n",
    "            case \"normalize_inputs\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.input_layernorm.weight\"],\n",
    "                })\n",
    "            case \"queries\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.self_attn.q_proj.weight\"],\n",
    "                })\n",
    "            case \"keys\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.self_attn.k_proj.weight\"],\n",
    "                })\n",
    "            case \"values\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.self_attn.v_proj.weight\"],\n",
    "                })                \n",
    "            case \"attention_outputs\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.self_attn.o_proj.weight\"],\n",
    "                })                \n",
    "            case \"normalize_attention\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.post_attention_layernorm.weight\"],\n",
    "                })\n",
    "            case \"gate\":\n",
    "                module.load_state_dict({\n",
    "                    \"0.weight\": parameters[f\"model.layers.{layer}.mlp.gate_proj.weight\"],\n",
    "                })\n",
    "            case \"up\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.mlp.up_proj.weight\"],\n",
    "                })\n",
    "            case \"down\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.mlp.down_proj.weight\"],\n",
    "                })\n",
    "            case \"normalize_context\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[\"model.norm.weight\"],\n",
    "                })\n",
    "            case \"classifier\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[\"lm_head.weight\"],\n",
    "                })\n",
    "            case _:\n",
    "                raise ValueError(f\"Unexpected key {key}\")\n",
    "\n",
    "\n",
    "def load_pretrained_state(layer):    \n",
    "    # Load pre-trained state\n",
    "    load_state(\n",
    "        normalize_inputs, \"normalize_inputs\", \n",
    "        queries, \"queries\", \n",
    "        keys, \"keys\", \n",
    "        values, \"values\", \n",
    "        attention_outputs, \"attention_outputs\", \n",
    "        normalize_attention, \"normalize_attention\",\n",
    "        gate, \"gate\",\n",
    "        up, \"up\",\n",
    "        down, \"down\",\n",
    "        layer=layer,\n",
    "    )                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46a3e664-d824-4808-a11e-b43ca1e64860",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compare_embeddings(t, llama_t):\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Move both tensors to cpu\n",
    "        t = t.to(\"cpu\")\n",
    "        llama_t = llama_t.to(\"cpu\")\n",
    "    \n",
    "        # Squeeze llama\n",
    "        llama_t = llama_t.squeeze()\n",
    "        assert t.shape == llama_t.shape\n",
    "\n",
    "        # Reshape both to be 1 long list of embeddings\n",
    "        t = t.reshape(-1, t.shape[-1])\n",
    "        llama_t = llama_t.reshape(-1, llama_t.shape[-1])\n",
    "        assert t.shape == llama_t.shape\n",
    "\n",
    "        # Compare each embedding\n",
    "        for i in range(t.shape[0]):\n",
    "            e1 = t[i]\n",
    "            e2 = llama_t[i]\n",
    "            score = torch.dot(e1, e2) / torch.norm(e2)**2\n",
    "            error = 1.0 - score\n",
    "            errors.append(error.abs().item())\n",
    "\n",
    "    return Series(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02808016-46bc-4e9b-8f9b-9d5fe8625f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06b5efa5-1d3c-4a82-b375-01c7e8465b0e",
   "metadata": {},
   "source": [
    "# Transformer Pipeline\n",
    "\n",
    "<img src=\"transformer-pipeline.svg\" class=\"stickshift-figure\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46391170-5ea5-4ae0-bbbc-5021069047d1",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdb4a979-a5c1-4870-9e1c-7a6c03c0f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokenizer from transformer\n",
    "tokenizer = transformer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f31494d-aed3-437b-95f0-cd797895a810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,   3923,    374,    279,   6864,    315,  22108,     30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize sentence\n",
    "batch = tokenizer(\"What is the capital of Massachusetts?\", return_tensors=\"pt\")\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a62fc903-13ba-40f8-9b9f-f3e415eb38ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>',\n",
       " 'What',\n",
       " ' is',\n",
       " ' the',\n",
       " ' capital',\n",
       " ' of',\n",
       " ' Massachusetts',\n",
       " '?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(input_id) for input_id in batch.input_ids[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54665637-5d10-46fb-b737-fe9e44a33c30",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adddae5d-d99b-4b23-b2ce-dc1b988c431f",
   "metadata": {},
   "source": [
    "## Value Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ed4e88a-cb49-439a-b105-41ff6e1461bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize value embeddings lookup table\n",
    "value_embeddings = nn.Embedding(\n",
    "    num_embeddings=config.vocab_size, \n",
    "    embedding_dim=config.d_model,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(value_embeddings, \"value_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3b00309-d217-4fe0-8fa5-62cdbfce862d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>',\n",
       " 'What',\n",
       " ' is',\n",
       " ' the',\n",
       " ' capital',\n",
       " ' of',\n",
       " ' Massachusetts',\n",
       " '?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate token values\n",
    "values = torch.squeeze(batch.input_ids)\n",
    "\n",
    "[tokenizer.decode(input_id) for input_id in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aff780b-edad-44cd-9193-635fdf204884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(values)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9968baf4-6336-489f-833d-caed3921e958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map token values to embeddings\n",
    "v = value_embeddings(values)\n",
    "\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8ffca58-799f-4d6e-82d2-3ce696f400be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
       "          6.3419e-05,  1.1902e-03],\n",
       "        [ 2.0752e-02, -1.2894e-03,  2.8229e-03,  ...,  2.1973e-02,\n",
       "          3.1128e-03,  1.0681e-02],\n",
       "        [-2.6093e-03,  7.7057e-04,  2.6131e-04,  ...,  1.1902e-02,\n",
       "          4.6387e-03,  9.1553e-03],\n",
       "        ...,\n",
       "        [ 1.2817e-03,  9.1171e-04,  2.0905e-03,  ...,  1.6251e-03,\n",
       "          4.0894e-03, -4.0283e-03],\n",
       "        [ 1.2146e-02,  1.1597e-02,  1.7822e-02,  ...,  1.9684e-03,\n",
       "         -1.4771e-02, -2.5940e-03],\n",
       "        [-4.8523e-03, -1.8005e-03,  7.2937e-03,  ...,  2.3956e-03,\n",
       "         -1.3657e-03, -5.4932e-03]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show sample of value embeddings\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a07a486b-ddc1-4f9e-97ff-c87c7ab44d32",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_v = llama_model.embed_tokens(batch.input_ids.to(device))\n",
    "\n",
    "assert compare_embeddings(v, llama_v).max() < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1df41-2357-40e1-b9b5-14fdae963244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dd50b25-dcaf-434b-bf77-5bab5e8c586b",
   "metadata": {},
   "source": [
    "## Position Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbfca98-6f83-477a-81d4-1049384507db",
   "metadata": {},
   "source": [
    "Llama 3 uses rotary position encoding (RoPE) algorithm. Instead of baking absolute positions into the input embeddings, RoPE rotates the query and key embeddings according to the tokens' positions in the sequence.\n",
    "\n",
    "The match between token embedding $m$ and $n$ is calculated as\n",
    "\n",
    "$$\n",
    "q_{m}^T k_{n} = (R_{\\Theta,m}^d W_{q} x_{m})^T (R_{\\Theta,n}^d W_{k} x_{n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef359dd-2257-4a3c-a83c-1073bfa2af09",
   "metadata": {},
   "source": [
    "which can be rewritten in pseudo code as\n",
    "\n",
    "```python\n",
    "weights[m][n] = transpose(rotate(query(x[m]), m)) * rotate(query(x[n]), n)\n",
    "```\n",
    "\n",
    "However, if we pack all the embeddings, queries, and keys into matrices, then we can still use the familiar SDPA equation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "SDPA &= softmax(\\frac{QK^T}{\\sqrt{d_K}})V \\\\\n",
    "\\text{where } Q &= R_{\\Theta}^d W_Q X \\\\\n",
    "              K &= R_{\\Theta}^d W_K X \\\\\n",
    "              V &= W_V X\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that while the query, key, and value projections are still layer-specific, the rotation matrix $R_{\\Theta}^d$ is shared across all layers.\n",
    "\n",
    "The rotation matrix is defined by a series of 2D rotations for each pair of values in the embedding vectors.\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{\\Theta,m}^d = \n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) & -sin(m \\theta_0) & 0 & 0 & \\dots & 0 & 0 \\\\\n",
    "sin(m \\theta_0) & cos(m \\theta_0) & 0 & 0 & \\dots & 0 & 0 \\\\\n",
    "0 & 0 & cos(m \\theta_1) & -sin(m \\theta_1) & \\dots & 0 & 0 \\\\\n",
    "0 & 0 & sin(m \\theta_1) & cos(m \\theta_1) & \\dots & 0 & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & \\dots & 0 & 0 & cos(m \\theta_{d/2-1}) & -sin(m \\theta_{d/2-1}) \\\\\n",
    "0 & 0 & \\dots & 0 & 0 & sin(m \\theta_{d/2-1}) & cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\theta_i = \\frac{1}{\\Theta^{2i/d}}\n",
    "$$\n",
    "\n",
    "which is computed using the more efficient form\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{\\Theta,m}^d \\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 \\\\\n",
    "\\vdots \\\\\n",
    "x_{d/2-2} \\\\\n",
    "x_{d/2-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "-x_2 \\\\\n",
    "x_1 \\\\\n",
    "-x_4 \\\\\n",
    "x_3 \\\\\n",
    "\\vdots \\\\\n",
    "-x_{d/2-1} \\\\\n",
    "x_{d/2-2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For now, we simply compute the $cos$ and $sin$ terms which rely only on base $\\Theta$, head dimension $d$, and the sequence length $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c1f474f-b279-4e43-bd4b-b3c3dbc723e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = config.rope_base\n",
    "d = config.d_head\n",
    "\n",
    "# Compute theta_i = 1 / base^(2i/d) from i = 0 to d/2-1\n",
    "thetas = 1.0 / base**(2 * torch.arange(d // 2) / d)\n",
    "\n",
    "thetas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4a35279-47cc-4396-b7ff-b78d1593c984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute m * theta_i for position m in 0 to n\n",
    "frequencies = torch.stack([m*thetas for m in range(n)])\n",
    "\n",
    "# Duplicate each row\n",
    "frequencies = torch.cat((frequencies, frequencies), dim=-1)\n",
    "\n",
    "frequencies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2fa4b28-dfa0-4da6-ad78-365fa90ea6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 128]), torch.Size([8, 128]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rope_cos = torch.cos(frequencies)\n",
    "rope_sin = torch.sin(frequencies)\n",
    "\n",
    "rope_cos.shape, rope_sin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48f8630a-dfa6-4caa-8260-47854d3e8df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert rope_cos.shape[0] == n and rope_cos.shape[1] == config.d_head\n",
    "assert rope_sin.shape[0] == n and rope_sin.shape[1] == config.d_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a77bb0e-f87e-4bec-b990-c17530e423f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 128]), torch.Size([1, 8, 128]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add extra dimension so we can multiply against multi-head q,k,v\n",
    "rope_cos = rope_cos.unsqueeze(0)\n",
    "rope_sin = rope_sin.unsqueeze(0)\n",
    "\n",
    "rope_cos.shape, rope_sin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9150fe55-d2b8-4296-a856-7319f419f877",
   "metadata": {},
   "source": [
    "## Input Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03cebd07-c023-4193-a991-8efa2abb7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc89be88-4c31-4e95-b438-3a10662fc5f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_x = llama_v\n",
    "llama_position_ids = torch.arange(n).unsqueeze(0).to(device)\n",
    "llama_rope_cos, llama_rope_sin = llama_model.rotary_emb(llama_x, llama_position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6e524-e770-4176-a406-955cc7fad453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "395d3a8e-6476-4830-901b-99b9f81dec96",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "<img src=\"transformer-layers.svg\" class=\"stickshift-figure\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2c84c4a-ea29-430e-b3d2-435c1305217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_layer = llama_model.layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41053e8-ea21-491b-a166-3e312b9e0466",
   "metadata": {},
   "source": [
    "## Normalize Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3703c958-bdf4-41d6-b12b-f293b607453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(config.d_model))\n",
    "        self.eps = config.rms_norm_eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        variance = x.pow(2).mean(-1, keepdim=True)\n",
    "        return self.weight * x * torch.rsqrt(variance + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54270da1-d9fa-4c6c-b5c9-fee4c8fd9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure input normalization\n",
    "normalize_inputs = RMSNorm(config=config)\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(normalize_inputs, \"normalize_inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e56afdd0-616f-4245-a107-0b6df3db7aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual = x\n",
    "hidden_states = normalize_inputs(x)\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a38ecc9-b2cf-4fcb-b12f-27b7ae5624d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5436e-03, -1.1578e-02, -3.0144e-02,  ...,  3.5905e-02,\n",
       "          3.0229e-04,  3.5905e-03],\n",
       "        [ 1.0366e-01, -2.5630e-02,  1.2508e-01,  ...,  1.7744e-01,\n",
       "          1.2730e-02,  2.7645e-02],\n",
       "        [-1.7112e-02,  2.0110e-02,  1.5202e-02,  ...,  1.2618e-01,\n",
       "          2.4905e-02,  3.1110e-02],\n",
       "        ...,\n",
       "        [ 9.2948e-03,  2.6309e-02,  1.3447e-01,  ...,  1.9051e-02,\n",
       "          2.4277e-02, -1.5136e-02],\n",
       "        [ 4.7014e-02,  1.7862e-01,  6.1193e-01,  ...,  1.2317e-02,\n",
       "         -4.6805e-02, -5.2024e-03],\n",
       "        [-3.0458e-02, -4.4975e-02,  4.0612e-01,  ...,  2.4310e-02,\n",
       "         -7.0178e-03, -1.7866e-02]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b8c4ea1-1989-45c1-8db8-1a14fdd6f06f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_residual = llama_x\n",
    "llama_hidden_states = llama_layer.input_layernorm(llama_x)\n",
    "llama_hidden_states.shape\n",
    "\n",
    "errors = compare_embeddings(hidden_states, llama_hidden_states)\n",
    "assert errors.max() < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b48b43-687c-42be-a2c4-98e998484137",
   "metadata": {},
   "source": [
    "## Queries, Keys, Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd06a3bf-8447-4ed6-9507-29bebdc3d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, n_heads):\n",
    "    return x.view(-1, n_heads, config.d_head).transpose(-3, -2)\n",
    "\n",
    "def combine_heads(x):\n",
    "    return x.transpose(-3, -2).contiguous().view(-1, int(config.n_heads * config.d_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6f02f0c-c14c-4fd7-9f6c-7824005eaede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure query, key, value projections\n",
    "queries = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.n_heads * config.d_head,\n",
    "    bias=False,\n",
    ")\n",
    "keys = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    ")\n",
    "values = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    ")\n",
    "attention_outputs = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(queries, \"queries\", keys, \"keys\", values, \"values\", attention_outputs, \"attention_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47023651-7418-468c-ba27-82770cd2d53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 4096]), torch.Size([8, 1024]), torch.Size([8, 1024]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project token embeddings to query, key, and value spaces\n",
    "q = queries(hidden_states)\n",
    "k = keys(hidden_states)\n",
    "v = values(hidden_states)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cf7e59a-c147-493c-869b-b7864b48c788",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_q = llama_layer.self_attn.q_proj(llama_hidden_states)\n",
    "llama_k = llama_layer.self_attn.k_proj(llama_hidden_states)\n",
    "llama_v = llama_layer.self_attn.v_proj(llama_hidden_states)\n",
    "assert compare_embeddings(q, llama_q).max() < 0.001\n",
    "assert compare_embeddings(k, llama_k).max() < 0.001\n",
    "assert compare_embeddings(v, llama_v).max() < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4d4f95a-e20e-4a45-a709-840310a07c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 128]), torch.Size([8, 8, 128]), torch.Size([8, 8, 128]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split attention heads\n",
    "q = split_heads(q, config.n_heads)\n",
    "k = split_heads(k, config.n_kv_heads)\n",
    "v = split_heads(v, config.n_kv_heads)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8628578-cb81-4e9a-8038-6419ea4d4b43",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bsz, q_len, _ = llama_hidden_states.size()\n",
    "llama_q = llama_q.view(bsz, q_len, llama_layer.self_attn.num_heads, llama_layer.self_attn.head_dim).transpose(1, 2)\n",
    "llama_k = llama_k.view(bsz, q_len, llama_layer.self_attn.num_key_value_heads, llama_layer.self_attn.head_dim).transpose(1, 2)\n",
    "llama_v = llama_v.view(bsz, q_len, llama_layer.self_attn.num_key_value_heads, llama_layer.self_attn.head_dim).transpose(1, 2)\n",
    "assert compare_embeddings(q, llama_q).max() < 0.001\n",
    "assert compare_embeddings(k, llama_k).max() < 0.001\n",
    "assert compare_embeddings(v, llama_v).max() < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31a8ee56-8826-431e-a5d1-8be49fff13c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 128]), torch.Size([8, 8, 128]), torch.Size([8, 8, 128]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "# Rotate queries and keys\n",
    "q = (q * rope_cos) + (rotate_half(q) * rope_sin)\n",
    "k = (k * rope_cos) + (rotate_half(k) * rope_sin)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abe84d82-b9f0-4f22-b096-db82d7990327",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_q, llama_k = apply_rotary_pos_emb(llama_q, llama_k, llama_rope_cos, llama_rope_sin)\n",
    "assert compare_embeddings(q, llama_q).max() < 0.001\n",
    "assert compare_embeddings(k, llama_k).max() < 0.001\n",
    "assert compare_embeddings(v, llama_v).max() < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41d0b006-64a6-4cbe-8bda-462a52ad56c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 128]), torch.Size([32, 8, 128]), torch.Size([32, 8, 128]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeat key, value groups\n",
    "k = k.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "v = v.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e094b07-434b-48fe-903b-ab8c9229c2aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_k = repeat_kv(llama_k, llama_layer.self_attn.num_key_value_groups)\n",
    "llama_v = repeat_kv(llama_v, llama_layer.self_attn.num_key_value_groups)\n",
    "assert compare_embeddings(q, llama_q).max() < 0.001\n",
    "assert compare_embeddings(k, llama_k).max() < 0.001\n",
    "assert compare_embeddings(v, llama_v).max() < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3572d84-2687-488c-affc-30a9ee829f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6230eef1-a51f-41c4-bce5-721165760208",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8b53954-0723-4970-841e-2da247f54091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 8])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute masked attention bias\n",
    "mask = torch.ones(n, n, dtype=torch.bool).tril(diagonal=0)\n",
    "bias = torch.zeros(n, n)\n",
    "bias.masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "# Compute attention weights\n",
    "w = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head) + bias, dim=-1)\n",
    "\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f89c556c-0e52-4dcd-974c-f6b26a519a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 128])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute attention for all heads in parallel\n",
    "a = w @ v\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0994b412-6725-44fb-a1b8-6fcd01213665",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_a = torch.nn.functional.scaled_dot_product_attention(\n",
    "    llama_q,\n",
    "    llama_k,\n",
    "    llama_v,\n",
    "    is_causal=True,\n",
    ")\n",
    "\n",
    "assert compare_embeddings(a, llama_a).max() < 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dca90cd9-630c-4874-8d84-cff1cb741f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine attention heads\n",
    "a = combine_heads(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "564ddf17-57b1-4afc-b197-8d78fe9c9e94",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_a = llama_a.transpose(1, 2).contiguous().view(bsz, q_len, -1)\n",
    "assert compare_embeddings(a, llama_a).max() < 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc803f4-18ab-4b14-8e7c-083357786fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0eaa3c0d-c8c0-41cf-8888-e43ebb72f793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project attention embeddings back to model space\n",
    "hidden_states = attention_outputs(a)\n",
    "\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1682f670-c31f-40ef-9ce5-69f048e14d9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_hidden_states = llama_layer.self_attn.o_proj(llama_a)\n",
    "assert compare_embeddings(hidden_states, llama_hidden_states).max() < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d7d19-aec5-4497-992d-062031855b5d",
   "metadata": {},
   "source": [
    "## Add and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56b52382-b605-4f12-8d5d-3e4db101c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine attention with input embeddings\n",
    "hidden_states = residual + hidden_states\n",
    "residual = hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b285e6c3-c5bf-4f7f-9880-18d7d4f28c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention normalization\n",
    "normalize_attention = RMSNorm(config=config)\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(normalize_attention, \"normalize_attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd678bea-4fe5-4426-8078-aef26e038b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize attention\n",
    "hidden_states = normalize_attention(hidden_states)\n",
    "\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f9ac56d-0d17-4791-852b-94828a8893f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Combine attention with input embeddings\n",
    "llama_hidden_states = llama_residual + llama_hidden_states\n",
    "llama_residual = llama_hidden_states\n",
    "\n",
    "llama_hidden_states = llama_layer.post_attention_layernorm(llama_hidden_states)\n",
    "assert compare_embeddings(hidden_states, llama_hidden_states).max() < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5dee5e-62b1-4949-a871-eaab1744e1ec",
   "metadata": {},
   "source": [
    "## FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "142a6c35-553d-4067-91d9-9ce9322bbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FNN layers\n",
    "gate = nn.Sequential(\n",
    "    nn.Linear(\n",
    "        in_features=config.d_model,\n",
    "        out_features=config.d_fnn,\n",
    "        bias=False,\n",
    "    ),\n",
    "    nn.SiLU()\n",
    ")\n",
    "up = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_fnn,\n",
    "    bias=False,\n",
    ")\n",
    "down = nn.Linear(\n",
    "    in_features=config.d_fnn,\n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(gate, \"gate\", up, \"up\", down, \"down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef818bd7-7006-4baa-9e57-ede038cac94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = down(gate(hidden_states) * up(hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad748c6e-6927-4390-875e-2f6b36c25b64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_hidden_states = llama_layer.mlp(llama_hidden_states)\n",
    "assert compare_embeddings(hidden_states, llama_hidden_states).max() < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd0d0fa-0c4d-45ff-a1dd-795ebe0b13dd",
   "metadata": {},
   "source": [
    "## Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "531dcdae-34d8-441a-b939-c39f2640d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = residual + hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d387a461-3477-454e-828a-b62d96737742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a937ecd8-169d-4de4-866f-0254ef8af04f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_hidden_states = llama_residual + llama_hidden_states\n",
    "assert compare_embeddings(hidden_states, llama_hidden_states).max() < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0004020-2f00-48d1-baf9-fddd6afdeb16",
   "metadata": {},
   "source": [
    "## Stacking the Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d9be498-8de7-48d1-b4c7-bd018a7ee0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loop w/ initial input embeddings\n",
    "z_i = x\n",
    "\n",
    "# Apply layer logic in a loop\n",
    "for layer in range(config.n_layers):\n",
    "    \n",
    "    # Use previous layer's outputs as inputs\n",
    "    hidden_states_i = z_i\n",
    "\n",
    "    # Load pre-trained state for layer\n",
    "    load_pretrained_state(layer)\n",
    "\n",
    "    #\n",
    "    # Inputs\n",
    "    #\n",
    "\n",
    "    # Normalize inputs\n",
    "    residual_i = hidden_states_i\n",
    "    hidden_states_i = normalize_inputs(hidden_states_i)\n",
    "\n",
    "    #\n",
    "    # Attention\n",
    "    #\n",
    "    \n",
    "    # Project hidden_states_i to query, key, and value spaces\n",
    "    q_i = queries(hidden_states_i)\n",
    "    k_i = keys(hidden_states_i)\n",
    "    v_i = values(hidden_states_i)\n",
    "    \n",
    "    # Split q, k, v into separate attention heads\n",
    "    q_i = split_heads(q_i, config.n_heads)\n",
    "    k_i = split_heads(k_i, config.n_kv_heads)\n",
    "    v_i = split_heads(v_i, config.n_kv_heads)\n",
    "    \n",
    "    # Rotate queries and keys\n",
    "    q_i = (q_i * rope_cos) + (rotate_half(q_i) * rope_sin)\n",
    "    k_i = (k_i * rope_cos) + (rotate_half(k_i) * rope_sin)\n",
    "\n",
    "    # Expand keys and values for GQA\n",
    "    k_i = k_i.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "    v_i = v_i.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "    \n",
    "    # Compute masked attention bias\n",
    "    mask = torch.ones(n, n, dtype=torch.bool).tril(diagonal=0)\n",
    "    b_i = torch.zeros(n, n)\n",
    "    b_i.masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "    # Compute attention for all heads in parallel\n",
    "    w_i = softmax(\n",
    "        q_i @ k_i.transpose(-2, -1) / np.sqrt(config.d_head) + b_i, \n",
    "        dim=-1,\n",
    "    )\n",
    "    a_i = w_i @ v_i\n",
    "    \n",
    "    # Recombine attention heads\n",
    "    a_i = combine_heads(a_i)\n",
    "    \n",
    "    # Project attention embeddings back to model space\n",
    "    hidden_states_i = attention_outputs(a_i)\n",
    "\n",
    "    # Combine attention with input embeddings\n",
    "    hidden_states_i = residual_i + hidden_states_i\n",
    "    residual_i = hidden_states_i\n",
    "    \n",
    "    # Normalize attention\n",
    "    hidden_states_i = normalize_attention(hidden_states_i)\n",
    "\n",
    "    #\n",
    "    # FNN\n",
    "    #\n",
    "\n",
    "    # Transform\n",
    "    hidden_states_i = down(gate(hidden_states_i) * up(hidden_states_i))\n",
    "\n",
    "    # Combine FNN with attention embeddings\n",
    "    hidden_states_i = residual_i + hidden_states_i\n",
    "    residual_i = hidden_states_i\n",
    "\n",
    "    #\n",
    "    # Outputs\n",
    "    #\n",
    "\n",
    "    z_i = hidden_states_i\n",
    "\n",
    "# Save outputs from last layer\n",
    "z = z_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "028b6b9f-718e-4ba5-87f9-4c46ea903b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure context normalization\n",
    "normalize_context = RMSNorm(config=config)\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(normalize_context, \"normalize_context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d171dd79-08fa-4eca-8d0b-955369c55612",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = normalize_context(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9340421d-efec-4b61-9bc3-c5564a07ba59",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "llama_z = llama_model(input_ids=batch.input_ids.to(device)).last_hidden_state\n",
    "assert compare_embeddings(z, llama_z).max() < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6debbf-6600-4814-86f1-3e268646bfe9",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1e5efbb-9cfe-4ee4-97c6-1990905a0507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure language modeling head\n",
    "classifier = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.vocab_size, \n",
    "    bias=False,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(classifier, \"classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3661fa0e-a0e4-4f87-9fc4-69b141ec7df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use last embedding to represent the entire sequence\n",
    "features = z[-1]\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c552cf64-fa7b-4cc3-8771-2d12238a8793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128256])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = classifier(features)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fe321090-5309-4556-8644-c70146252b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Boston'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_id = logits.argmax()\n",
    "tokenizer.decode(output_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "be07d817-0494-4eb2-87be-24154b32cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert output_id == 10406"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5d64f-f566-48a5-bf87-cc838db03e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
