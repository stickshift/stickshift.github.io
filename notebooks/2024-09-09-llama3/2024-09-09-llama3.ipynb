{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90453ea1-d7da-480b-be6b-86baae89d3b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "source": [
    "# Transformer Teardown: Llama 3\n",
    "\n",
    "> Use what we learned about BERT as a baseline to explore SOTA Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa710c-a778-434e-9b58-b78675530e3a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f939cad5-82e6-49dd-9871-52bda963abdb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas import Series\n",
    "from pytest import approx\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu, softmax\n",
    "import transformers\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, repeat_kv\n",
    "\n",
    "from stickshift import default_arg, take\n",
    "from stickshift.models import llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c7fa65-b60e-4d6b-9e38-9a840e574743",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-publish"
    ]
   },
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure gpu\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22788340-4c34-4daf-98fa-9320516fe291",
   "metadata": {},
   "source": [
    "# Text Generation with Llama 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef8424-2fa0-4b65-8a29-666cab09b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create off-the-shelf text generation transformer\n",
    "transformer = transformers.pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989806c0-ec4d-4bf5-9279-eed474652392",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer(\"What is the capital of Massachusetts?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1bfa9c-ed51-4ea0-a02c-2d3d8a79beb8",
   "metadata": {},
   "source": [
    "# Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622ec6a-4f36-4061-87f9-b79e4cc8a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model config and pre-trained parameters\n",
    "config = llama.config(transformer.model)\n",
    "parameters = transformer.model.state_dict()\n",
    "llama_model = transformer.model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410ef2f-5a30-4d39-9027-24c55df01e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[k for k in parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4ea80-b639-4b2a-b054-7791a2f86a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state(*args, layer=None):\n",
    "    # Defaults\n",
    "    layer = default_arg(layer, lambda: 0)\n",
    "\n",
    "    for module, key in take(2, args):\n",
    "        match key:\n",
    "            case \"value_embeddings\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[\"model.embed_tokens.weight\"],\n",
    "                })\n",
    "            case \"normalize_inputs\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.input_layernorm.weight\"],\n",
    "                })\n",
    "            case \"queries\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.self_attn.q_proj.weight\"],\n",
    "                })\n",
    "            case \"keys\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.self_attn.k_proj.weight\"],\n",
    "                })\n",
    "            case \"values\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.self_attn.v_proj.weight\"],\n",
    "                })                \n",
    "            case \"attention_outputs\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.self_attn.o_proj.weight\"],\n",
    "                })                \n",
    "            case \"normalize_attention\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.post_attention_layernorm.weight\"],\n",
    "                })\n",
    "            case \"gate\":\n",
    "                module.load_state_dict({\n",
    "                    \"0.weight\": parameters[f\"model.layers.{layer}.mlp.gate_proj.weight\"],\n",
    "                })\n",
    "            case \"up\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.mlp.up_proj.weight\"],\n",
    "                })\n",
    "            case \"down\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[f\"model.layers.{layer}.mlp.down_proj.weight\"],\n",
    "                })\n",
    "            case \"normalize_context\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[\"model.norm.weight\"],\n",
    "                })\n",
    "            case \"classifier\":\n",
    "                module.load_state_dict({\n",
    "                    \"weight\": parameters[\"lm_head.weight\"],\n",
    "                })\n",
    "            case _:\n",
    "                raise ValueError(f\"Unexpected key {key}\")\n",
    "\n",
    "\n",
    "def load_pretrained_state(layer):    \n",
    "    # Load pre-trained state\n",
    "    load_state(\n",
    "        normalize_inputs, \"normalize_inputs\", \n",
    "        queries, \"queries\", \n",
    "        keys, \"keys\", \n",
    "        values, \"values\", \n",
    "        attention_outputs, \"attention_outputs\", \n",
    "        normalize_attention, \"normalize_attention\",\n",
    "        gate, \"gate\",\n",
    "        up, \"up\",\n",
    "        down, \"down\",\n",
    "        layer=layer,\n",
    "    )                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a3e664-d824-4808-a11e-b43ca1e64860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embeddings(t, llama_t):\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Move both tensors to cpu\n",
    "        t = t.to(\"cpu\")\n",
    "        llama_t = llama_t.to(\"cpu\")\n",
    "    \n",
    "        # Squeeze llama\n",
    "        llama_t = llama_t.squeeze()\n",
    "        assert t.shape == llama_t.shape\n",
    "\n",
    "        # Reshape both to be 1 long list of embeddings\n",
    "        t = t.reshape(-1, t.shape[-1])\n",
    "        llama_t = llama_t.reshape(-1, llama_t.shape[-1])\n",
    "        assert t.shape == llama_t.shape\n",
    "\n",
    "        # Compare each embedding\n",
    "        for i in range(t.shape[0]):\n",
    "            e1 = t[i]\n",
    "            e2 = llama_t[i]\n",
    "            score = torch.dot(e1, e2) / torch.norm(e2)**2\n",
    "            error = 1.0 - score\n",
    "            errors.append(error.abs().item())\n",
    "\n",
    "    return Series(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02808016-46bc-4e9b-8f9b-9d5fe8625f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06b5efa5-1d3c-4a82-b375-01c7e8465b0e",
   "metadata": {},
   "source": [
    "# Transformer Pipeline\n",
    "\n",
    "<img src=\"transformer-pipeline.svg\" class=\"stickshift-figure\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46391170-5ea5-4ae0-bbbc-5021069047d1",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4a979-a5c1-4870-9e1c-7a6c03c0f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokenizer from transformer\n",
    "tokenizer = transformer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31494d-aed3-437b-95f0-cd797895a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentence\n",
    "batch = tokenizer(\"What is the capital of Massachusetts?\", return_tensors=\"pt\")\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62fc903-13ba-40f8-9b9f-f3e415eb38ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tokenizer.decode(input_id) for input_id in batch.input_ids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c07643-0d49-4b1e-8e86-a0951f7e457a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54665637-5d10-46fb-b737-fe9e44a33c30",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adddae5d-d99b-4b23-b2ce-dc1b988c431f",
   "metadata": {},
   "source": [
    "## Value Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4e88a-cb49-439a-b105-41ff6e1461bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize value embeddings lookup table\n",
    "value_embeddings = nn.Embedding(\n",
    "    num_embeddings=config.vocab_size, \n",
    "    embedding_dim=config.d_model,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(value_embeddings, \"value_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b00309-d217-4fe0-8fa5-62cdbfce862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate token values\n",
    "values = torch.squeeze(batch.input_ids)\n",
    "\n",
    "[tokenizer.decode(input_id) for input_id in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff780b-edad-44cd-9193-635fdf204884",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(values)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968baf4-6336-489f-833d-caed3921e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map token values to embeddings\n",
    "v = value_embeddings(values)\n",
    "\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ffca58-799f-4d6e-82d2-3ce696f400be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample of value embeddings\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a486b-ddc1-4f9e-97ff-c87c7ab44d32",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_v = llama_model.embed_tokens(batch.input_ids.to(device))\n",
    "\n",
    "assert compare_embeddings(v, llama_v).max() < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1df41-2357-40e1-b9b5-14fdae963244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dd50b25-dcaf-434b-bf77-5bab5e8c586b",
   "metadata": {},
   "source": [
    "## Position Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbfca98-6f83-477a-81d4-1049384507db",
   "metadata": {},
   "source": [
    "Llama 3 uses rotary position encoding (RoPE) algorithm. Instead of baking absolute positions into the input embeddings, RoPE rotates the query and key embeddings according to the tokens' positions in the sequence.\n",
    "\n",
    "The match between token embedding $m$ and $n$ is calculated as\n",
    "\n",
    "$$\n",
    "q_{m}^T k_{n} = (R_{\\Theta,m}^d W_{q} x_{m})^T (R_{\\Theta,n}^d W_{k} x_{n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef359dd-2257-4a3c-a83c-1073bfa2af09",
   "metadata": {},
   "source": [
    "which can be rewritten in pseudo code as\n",
    "\n",
    "```python\n",
    "weights[m][n] = transpose(rotate(query(x[m]), m)) * rotate(query(x[n]), n)\n",
    "```\n",
    "\n",
    "However, if we pack all the embeddings, queries, and keys into matrices, then we can still use the familiar SDPA equation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "SDPA &= softmax(\\frac{QK^T}{\\sqrt{d_K}})V \\\\\n",
    "\\text{where } Q &= R_{\\Theta}^d W_Q X \\\\\n",
    "              K &= R_{\\Theta}^d W_K X \\\\\n",
    "              V &= W_V X\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that while the query, key, and value projections are still layer-specific, the rotation matrix $R_{\\Theta}^d$ is shared across all layers.\n",
    "\n",
    "The rotation matrix is defined by a series of 2D rotations for each pair of values in the embedding vectors.\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{\\Theta,m}^d = \n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) & -sin(m \\theta_0) & 0 & 0 & \\dots & 0 & 0 \\\\\n",
    "sin(m \\theta_0) & cos(m \\theta_0) & 0 & 0 & \\dots & 0 & 0 \\\\\n",
    "0 & 0 & cos(m \\theta_1) & -sin(m \\theta_1) & \\dots & 0 & 0 \\\\\n",
    "0 & 0 & sin(m \\theta_1) & cos(m \\theta_1) & \\dots & 0 & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & \\dots & 0 & 0 & cos(m \\theta_{d/2-1}) & -sin(m \\theta_{d/2-1}) \\\\\n",
    "0 & 0 & \\dots & 0 & 0 & sin(m \\theta_{d/2-1}) & cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\theta_i = \\frac{1}{\\Theta^{2i/d}}\n",
    "$$\n",
    "\n",
    "which is computed using the more efficient form\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{\\Theta,m}^d \\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 \\\\\n",
    "\\vdots \\\\\n",
    "x_{d/2-2} \\\\\n",
    "x_{d/2-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "-x_2 \\\\\n",
    "x_1 \\\\\n",
    "-x_4 \\\\\n",
    "x_3 \\\\\n",
    "\\vdots \\\\\n",
    "-x_{d/2-1} \\\\\n",
    "x_{d/2-2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For now, we simply compute the $cos$ and $sin$ terms which rely only on base $\\Theta$, head dimension $d$, and the sequence length $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f474f-b279-4e43-bd4b-b3c3dbc723e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = config.rope_base\n",
    "d = config.d_head\n",
    "\n",
    "# Compute theta_i = 1 / base^(2i/d) from i = 0 to d/2-1\n",
    "thetas = 1.0 / base**(2 * torch.arange(d // 2) / d)\n",
    "\n",
    "thetas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a35279-47cc-4396-b7ff-b78d1593c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute m * theta_i for position m in 0 to n\n",
    "frequencies = torch.stack([m*thetas for m in range(n)])\n",
    "\n",
    "# Duplicate each row\n",
    "frequencies = torch.cat((frequencies, frequencies), dim=-1)\n",
    "\n",
    "frequencies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa4b28-dfa0-4da6-ad78-365fa90ea6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rope_cos = torch.cos(frequencies)\n",
    "rope_sin = torch.sin(frequencies)\n",
    "\n",
    "rope_cos.shape, rope_sin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8630a-dfa6-4caa-8260-47854d3e8df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert rope_cos.shape[0] == n and rope_cos.shape[1] == config.d_head\n",
    "assert rope_sin.shape[0] == n and rope_sin.shape[1] == config.d_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77bb0e-f87e-4bec-b990-c17530e423f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add extra dimension so we can multiply against multi-head q,k,v\n",
    "rope_cos = rope_cos.unsqueeze(0)\n",
    "rope_sin = rope_sin.unsqueeze(0)\n",
    "\n",
    "rope_cos.shape, rope_sin.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9150fe55-d2b8-4296-a856-7319f419f877",
   "metadata": {},
   "source": [
    "## Input Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cebd07-c023-4193-a991-8efa2abb7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc89be88-4c31-4e95-b438-3a10662fc5f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_x = llama_v\n",
    "llama_position_ids = torch.arange(n).unsqueeze(0).to(device)\n",
    "llama_rope_cos, llama_rope_sin = llama_model.rotary_emb(llama_x, llama_position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6e524-e770-4176-a406-955cc7fad453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "395d3a8e-6476-4830-901b-99b9f81dec96",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "<img src=\"transformer-layers.svg\" class=\"stickshift-figure\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c84c4a-ea29-430e-b3d2-435c1305217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_layer = llama_model.layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41053e8-ea21-491b-a166-3e312b9e0466",
   "metadata": {},
   "source": [
    "## Normalize Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3703c958-bdf4-41d6-b12b-f293b607453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(config.d_model))\n",
    "        self.eps = config.rms_norm_eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        variance = x.pow(2).mean(-1, keepdim=True)\n",
    "        return self.weight * x * torch.rsqrt(variance + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54270da1-d9fa-4c6c-b5c9-fee4c8fd9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure input normalization\n",
    "normalize_inputs = RMSNorm(config=config)\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(normalize_inputs, \"normalize_inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56afdd0-616f-4245-a107-0b6df3db7aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = x\n",
    "hidden_states = normalize_inputs(x)\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38ecc9-b2cf-4fcb-b12f-27b7ae5624d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8c4ea1-1989-45c1-8db8-1a14fdd6f06f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_residual = llama_x\n",
    "llama_hidden_states = llama_layer.input_layernorm(llama_x)\n",
    "llama_hidden_states.shape\n",
    "\n",
    "errors = compare_embeddings(hidden_states, llama_hidden_states)\n",
    "assert errors.max() < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b48b43-687c-42be-a2c4-98e998484137",
   "metadata": {},
   "source": [
    "## Queries, Keys, Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06a3bf-8447-4ed6-9507-29bebdc3d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, n_heads):\n",
    "    return x.view(-1, n_heads, config.d_head).transpose(-3, -2)\n",
    "\n",
    "def combine_heads(x):\n",
    "    return x.transpose(-3, -2).contiguous().view(-1, int(config.n_heads * config.d_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f02f0c-c14c-4fd7-9f6c-7824005eaede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure query, key, value projections\n",
    "queries = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.n_heads * config.d_head,\n",
    "    bias=False,\n",
    ")\n",
    "keys = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    ")\n",
    "values = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    ")\n",
    "attention_outputs = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(queries, \"queries\", keys, \"keys\", values, \"values\", attention_outputs, \"attention_outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47023651-7418-468c-ba27-82770cd2d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project token embeddings to query, key, and value spaces\n",
    "q = queries(hidden_states)\n",
    "k = keys(hidden_states)\n",
    "v = values(hidden_states)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7e59a-c147-493c-869b-b7864b48c788",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_q = llama_layer.self_attn.q_proj(llama_hidden_states)\n",
    "llama_k = llama_layer.self_attn.k_proj(llama_hidden_states)\n",
    "llama_v = llama_layer.self_attn.v_proj(llama_hidden_states)\n",
    "assert compare_embeddings(q, llama_q).max() < 0.001\n",
    "assert compare_embeddings(k, llama_k).max() < 0.001\n",
    "assert compare_embeddings(v, llama_v).max() < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d4f95a-e20e-4a45-a709-840310a07c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split attention heads\n",
    "q = split_heads(q, config.n_heads)\n",
    "k = split_heads(k, config.n_kv_heads)\n",
    "v = split_heads(v, config.n_kv_heads)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8628578-cb81-4e9a-8038-6419ea4d4b43",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bsz, q_len, _ = llama_hidden_states.size()\n",
    "llama_q = llama_q.view(bsz, q_len, llama_layer.self_attn.num_heads, llama_layer.self_attn.head_dim).transpose(1, 2)\n",
    "llama_k = llama_k.view(bsz, q_len, llama_layer.self_attn.num_key_value_heads, llama_layer.self_attn.head_dim).transpose(1, 2)\n",
    "llama_v = llama_v.view(bsz, q_len, llama_layer.self_attn.num_key_value_heads, llama_layer.self_attn.head_dim).transpose(1, 2)\n",
    "assert compare_embeddings(q, llama_q).max() < 0.001\n",
    "assert compare_embeddings(k, llama_k).max() < 0.001\n",
    "assert compare_embeddings(v, llama_v).max() < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a8ee56-8826-431e-a5d1-8be49fff13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "# Rotate queries and keys\n",
    "q = (q * rope_cos) + (rotate_half(q) * rope_sin)\n",
    "k = (k * rope_cos) + (rotate_half(k) * rope_sin)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe84d82-b9f0-4f22-b096-db82d7990327",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_q, llama_k = apply_rotary_pos_emb(llama_q, llama_k, llama_rope_cos, llama_rope_sin)\n",
    "assert compare_embeddings(q, llama_q).max() < 0.001\n",
    "assert compare_embeddings(k, llama_k).max() < 0.001\n",
    "assert compare_embeddings(v, llama_v).max() < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d0b006-64a6-4cbe-8bda-462a52ad56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat key, value groups\n",
    "k = k.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "v = v.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e094b07-434b-48fe-903b-ab8c9229c2aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_k = repeat_kv(llama_k, llama_layer.self_attn.num_key_value_groups)\n",
    "llama_v = repeat_kv(llama_v, llama_layer.self_attn.num_key_value_groups)\n",
    "assert compare_embeddings(q, llama_q).max() < 0.001\n",
    "assert compare_embeddings(k, llama_k).max() < 0.001\n",
    "assert compare_embeddings(v, llama_v).max() < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3572d84-2687-488c-affc-30a9ee829f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6230eef1-a51f-41c4-bce5-721165760208",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b53954-0723-4970-841e-2da247f54091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute masked attention bias\n",
    "mask = torch.ones(n, n, dtype=torch.bool).tril(diagonal=0)\n",
    "bias = torch.zeros(n, n)\n",
    "bias.masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "# Compute attention weights\n",
    "w = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head) + bias, dim=-1)\n",
    "\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c556c-0e52-4dcd-974c-f6b26a519a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attention for all heads in parallel\n",
    "a = w @ v\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994b412-6725-44fb-a1b8-6fcd01213665",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_a = torch.nn.functional.scaled_dot_product_attention(\n",
    "    llama_q,\n",
    "    llama_k,\n",
    "    llama_v,\n",
    "    is_causal=True,\n",
    ")\n",
    "\n",
    "assert compare_embeddings(a, llama_a).max() < 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca90cd9-630c-4874-8d84-cff1cb741f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine attention heads\n",
    "a = combine_heads(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ddf17-57b1-4afc-b197-8d78fe9c9e94",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_a = llama_a.transpose(1, 2).contiguous().view(bsz, q_len, -1)\n",
    "assert compare_embeddings(a, llama_a).max() < 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc803f4-18ab-4b14-8e7c-083357786fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa3c0d-c8c0-41cf-8888-e43ebb72f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project attention embeddings back to model space\n",
    "hidden_states = attention_outputs(a)\n",
    "\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1682f670-c31f-40ef-9ce5-69f048e14d9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_hidden_states = llama_layer.self_attn.o_proj(llama_a)\n",
    "assert compare_embeddings(hidden_states, llama_hidden_states).max() < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d7d19-aec5-4497-992d-062031855b5d",
   "metadata": {},
   "source": [
    "## Add and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b52382-b605-4f12-8d5d-3e4db101c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine attention with input embeddings\n",
    "hidden_states = residual + hidden_states\n",
    "residual = hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b285e6c3-c5bf-4f7f-9880-18d7d4f28c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention normalization\n",
    "normalize_attention = RMSNorm(config=config)\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(normalize_attention, \"normalize_attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd678bea-4fe5-4426-8078-aef26e038b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize attention\n",
    "hidden_states = normalize_attention(hidden_states)\n",
    "\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ac56d-0d17-4791-852b-94828a8893f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Combine attention with input embeddings\n",
    "llama_hidden_states = llama_residual + llama_hidden_states\n",
    "llama_residual = llama_hidden_states\n",
    "\n",
    "llama_hidden_states = llama_layer.post_attention_layernorm(llama_hidden_states)\n",
    "assert compare_embeddings(hidden_states, llama_hidden_states).max() < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5dee5e-62b1-4949-a871-eaab1744e1ec",
   "metadata": {},
   "source": [
    "## FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a6c35-553d-4067-91d9-9ce9322bbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FNN layers\n",
    "gate = nn.Sequential(\n",
    "    nn.Linear(\n",
    "        in_features=config.d_model,\n",
    "        out_features=config.d_fnn,\n",
    "        bias=False,\n",
    "    ),\n",
    "    nn.SiLU()\n",
    ")\n",
    "up = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_fnn,\n",
    "    bias=False,\n",
    ")\n",
    "down = nn.Linear(\n",
    "    in_features=config.d_fnn,\n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(gate, \"gate\", up, \"up\", down, \"down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef818bd7-7006-4baa-9e57-ede038cac94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = down(gate(hidden_states) * up(hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad748c6e-6927-4390-875e-2f6b36c25b64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_hidden_states = llama_layer.mlp(llama_hidden_states)\n",
    "assert compare_embeddings(hidden_states, llama_hidden_states).max() < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd0d0fa-0c4d-45ff-a1dd-795ebe0b13dd",
   "metadata": {},
   "source": [
    "## Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531dcdae-34d8-441a-b939-c39f2640d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = residual + hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d387a461-3477-454e-828a-b62d96737742",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a937ecd8-169d-4de4-866f-0254ef8af04f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_hidden_states = llama_residual + llama_hidden_states\n",
    "assert compare_embeddings(hidden_states, llama_hidden_states).max() < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0004020-2f00-48d1-baf9-fddd6afdeb16",
   "metadata": {},
   "source": [
    "## Stacking the Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9be498-8de7-48d1-b4c7-bd018a7ee0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loop w/ initial input embeddings\n",
    "z_i = x\n",
    "\n",
    "# Apply layer logic in a loop\n",
    "for layer in range(config.n_layers):\n",
    "    \n",
    "    # Use previous layer's outputs as inputs\n",
    "    hidden_states_i = z_i\n",
    "\n",
    "    # Load pre-trained state for layer\n",
    "    load_pretrained_state(layer)\n",
    "\n",
    "    #\n",
    "    # Inputs\n",
    "    #\n",
    "\n",
    "    # Normalize inputs\n",
    "    residual_i = hidden_states_i\n",
    "    hidden_states_i = normalize_inputs(hidden_states_i)\n",
    "\n",
    "    #\n",
    "    # Attention\n",
    "    #\n",
    "    \n",
    "    # Project hidden_states_i to query, key, and value spaces\n",
    "    q_i = queries(hidden_states_i)\n",
    "    k_i = keys(hidden_states_i)\n",
    "    v_i = values(hidden_states_i)\n",
    "    \n",
    "    # Split q, k, v into separate attention heads\n",
    "    q_i = split_heads(q_i, config.n_heads)\n",
    "    k_i = split_heads(k_i, config.n_kv_heads)\n",
    "    v_i = split_heads(v_i, config.n_kv_heads)\n",
    "    \n",
    "    # Rotate queries and keys\n",
    "    q_i = (q_i * rope_cos) + (rotate_half(q_i) * rope_sin)\n",
    "    k_i = (k_i * rope_cos) + (rotate_half(k_i) * rope_sin)\n",
    "\n",
    "    # Expand keys and values for GQA\n",
    "    k_i = k_i.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "    v_i = v_i.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "    \n",
    "    # Compute masked attention bias\n",
    "    mask = torch.ones(n, n, dtype=torch.bool).tril(diagonal=0)\n",
    "    b_i = torch.zeros(n, n)\n",
    "    b_i.masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "    # Compute attention for all heads in parallel\n",
    "    w_i = softmax(\n",
    "        q_i @ k_i.transpose(-2, -1) / np.sqrt(config.d_head) + b_i, \n",
    "        dim=-1,\n",
    "    )\n",
    "    a_i = w_i @ v_i\n",
    "    \n",
    "    # Recombine attention heads\n",
    "    a_i = combine_heads(a_i)\n",
    "    \n",
    "    # Project attention embeddings back to model space\n",
    "    hidden_states_i = attention_outputs(a_i)\n",
    "\n",
    "    # Combine attention with input embeddings\n",
    "    hidden_states_i = residual_i + hidden_states_i\n",
    "    residual_i = hidden_states_i\n",
    "    \n",
    "    # Normalize attention\n",
    "    hidden_states_i = normalize_attention(hidden_states_i)\n",
    "\n",
    "    #\n",
    "    # FNN\n",
    "    #\n",
    "\n",
    "    # Transform\n",
    "    hidden_states_i = down(gate(hidden_states_i) * up(hidden_states_i))\n",
    "\n",
    "    # Combine FNN with attention embeddings\n",
    "    hidden_states_i = residual_i + hidden_states_i\n",
    "    residual_i = hidden_states_i\n",
    "\n",
    "    #\n",
    "    # Outputs\n",
    "    #\n",
    "\n",
    "    z_i = hidden_states_i\n",
    "\n",
    "# Save outputs from last layer\n",
    "z = z_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b6b9f-718e-4ba5-87f9-4c46ea903b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure context normalization\n",
    "normalize_context = RMSNorm(config=config)\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(normalize_context, \"normalize_context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171dd79-08fa-4eca-8d0b-955369c55612",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = normalize_context(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340421d-efec-4b61-9bc3-c5564a07ba59",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "llama_z = llama_model(input_ids=batch.input_ids.to(device)).last_hidden_state\n",
    "assert compare_embeddings(z, llama_z).max() < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6debbf-6600-4814-86f1-3e268646bfe9",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e5efbb-9cfe-4ee4-97c6-1990905a0507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure language modeling head\n",
    "classifier = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.vocab_size, \n",
    "    bias=False,\n",
    ")\n",
    "\n",
    "# Load pre-trained state\n",
    "load_state(classifier, \"classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661fa0e-a0e4-4f87-9fc4-69b141ec7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use last embedding to represent the entire sequence\n",
    "features = z[-1]\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552cf64-fa7b-4cc3-8771-2d12238a8793",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = classifier(features)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe321090-5309-4556-8644-c70146252b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_id = logits.argmax()\n",
    "tokenizer.decode(output_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be07d817-0494-4eb2-87be-24154b32cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert output_id == 10406"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5d64f-f566-48a5-bf87-cc838db03e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
