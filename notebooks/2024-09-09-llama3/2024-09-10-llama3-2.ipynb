{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c2806b-4390-4662-9432-c8de5a99a8cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Transformer Teardown: Llama 3\n",
    "\n",
    "> Single token prediction: What is capital of Massachusetts? Boston."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33316f09-93d9-449d-8108-266df48375ff",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94af2881-34c3-4c9f-b1ab-c949882329d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas import Series\n",
    "from pytest import approx\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu, silu, softmax\n",
    "\n",
    "from llama_models.llama3.api.tokenizer import Tokenizer\n",
    "from llama_models.llama3.reference_impl.model import RMSNorm\n",
    "\n",
    "import stickshift as ss\n",
    "from stickshift.torch import device as torch_device\n",
    "from stickshift import default_arg, take\n",
    "from stickshift.models import llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0d45ae0-b306-4ce1-9abc-0837c5268514",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure gpu\n",
    "device = torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7462c5b-0a67-4993-94bd-5c9b6c012983",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".stickshift-figure {\n",
       "    display: block;\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".stickshift-figure {\n",
    "    display: block;\n",
    "    margin-left: auto !important;\n",
    "    margin-right: auto !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb274b6-bccc-4f0c-96bd-8938e81ac9bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Text Generation with Llama 3\n",
    "\n",
    "Our goal is to build a Llama 3 text generation pipeline using the pre-trained weights and Meta's reference implementation as a guide. By the time we're done, our model should be able to correctly answer the question \"What is the capital of Massachusetts?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a166e-84a1-4f07-9a8c-9d1e5b3e9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the capital of Massachusetts?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c9723-5d57-4590-a2bf-26f480ae7986",
   "metadata": {},
   "source": [
    "# Load Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c55121-ac4c-481d-bb06-05b6c7d041ff",
   "metadata": {},
   "source": [
    "We'll start by loading the hyperparameters and pre-trained weights published by Meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdbece4-bdb3-470e-9e29-3f287480a04f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model config\n",
    "config = llama.config(\"Meta-Llama3.1-8B-Instruct\")\n",
    "\n",
    "# Load model parameters\n",
    "checkpoint = torch.load(config.checkpoint_path / \"consolidated.00.pth\", weights_only=True, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8382f302-b2eb-4c33-ae4e-9effad73e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efce665-a4f8-4ffb-a8b3-b6b4f8c60bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_state(layer):    \n",
    "    # Load pre-trained state\n",
    "    llama.load_state(\n",
    "        normalize_attention, \"normalize_attention\", \n",
    "        normalize_ffn, \"normalize_ffn\", \n",
    "        w_q, \"w_q\", \n",
    "        w_k, \"w_k\", \n",
    "        w_v, \"w_v\", \n",
    "        attention_outputs, \"attention_outputs\",\n",
    "        ffn_gates, \"ffn_gates\",\n",
    "        ffn_inputs, \"ffn_inputs\",\n",
    "        ffn_outputs, \"ffn_outputs\",\n",
    "        checkpoint=checkpoint,\n",
    "        layer=layer,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5935777d-85fa-48a2-8f8a-af753ae5fabc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [k for k in checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360c8e1b-e56a-4a9d-97e7-364a97a6b8ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Transformer Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9765f511-7eec-4545-b687-9ef8946317c4",
   "metadata": {},
   "source": [
    "<img src=\"transformer-pipeline.svg\" class=\"stickshift-figure\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cda4ec-55fe-415c-8145-9e4c935a3d57",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Tokenize\n",
    "\n",
    "The Tokenize stage transforms raw data into a sequence of \"tokens\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd8f7a9-6a6c-4616-b525-6666e5a9d8c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(str(config.checkpoint_path / \"tokenizer.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef25c70a-f606-4fa1-91bc-6afbae28809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.encode(question, bos=True, eos=False)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f399c583-59e1-4f6a-9f9b-d8d1f8cb6894",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d8f9e-3461-4275-a9e7-d0ae311de0e2",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "The Embeddings stage converts tokens into \"embeddings\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cffc68c-6f61-44f5-98ae-4c7c220c71b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings lookup table\n",
    "embeddings = nn.Embedding(\n",
    "    num_embeddings=config.vocab_size, \n",
    "    embedding_dim=config.d_model,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(embeddings, \"embeddings\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849045b-6d83-446b-8d6e-21ba816feff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load token_ids into a tensor\n",
    "token_values = torch.tensor(token_ids, device=device)\n",
    "\n",
    "token_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1262236-0212-4dac-b8b1-348f28dc9a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record sequence length\n",
    "n = len(token_values)\n",
    "\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c5481-bdef-48a0-b6aa-44c7c42d66f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map token values to embeddings\n",
    "x = embeddings(token_values)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ab890-3d46-486d-9ecd-93b45f90c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab343a-5835-445e-904f-1e27dbfce67a",
   "metadata": {},
   "source": [
    "# Context Layers\n",
    "\n",
    "The Context Layers in a Transformer are responsible for infusing each token embedding with contextual signals drawn from the rest of the sequence. The mechanism works by passing the token embeddings through multiple layers of attention and feedforward blocks. The attention blocks focus on relationships between tokens, augmenting each embedding with contextual information drawn from the surrounding embeddings. The feedforward blocks capitalize on the extra context, transforming each augmented embedding using a fully connected neural network. This pattern of attention and transformation is repeated over and over again, gradually converting representations of individual words into representations of abstract semantic concepts over a series of small increments.\n",
    "\n",
    "<img src=\"transformer-layers.svg\" class=\"stickshift-figure\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa13e7f7-ad24-433e-a4c7-d2549a6ff754",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca5ed83-1d59-4642-9ed8-d517c31a83eb",
   "metadata": {},
   "source": [
    "The attention block augments each token representation with additional context drawn from the surrounding tokens.\n",
    "\n",
    "<img src=\"attention.svg\" class=\"stickshift-figure\" width=\"500\">\n",
    "\n",
    "Attention starts with the token embeddings stacked in an $n \\times d_{model}$ matrix $\\mathbf{X}$. For each embedding $\\Set{\\mathbf{x}_i | \\mathbf{x}_i \\in \\mathbf{X}}$, we'll generate a new attention embedding $\\mathbf{a}_i$ using a weighted combination of all embeddings in $\\mathbf{X}$. The equation below emphasizes the fact that the weight for each embedding $\\mathbf{x}_j$ is a calculated as a function $f_{w}$ of the embedding values $\\mathbf{x}_i$, $\\mathbf{x}_j$ and their positions $i$, $j$.\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\Set{\\mathbf{a}_i | \\mathbf{a}_i = \\sum_{\\mathbf{x}_j \\in \\mathbf{X}} f_{w}(\\mathbf{x}_i, \\mathbf{x}_j, i, j) \\mathbf{x}_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3fd0fb-794f-402b-87c9-380d8481fa56",
   "metadata": {},
   "source": [
    "The attention function used by LLama 3 can be rewritten as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{A} &= softmax\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}} + \\mathbf{M}\\right)\\mathbf{V} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which expands to\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{A} &= softmax\\left(\\frac{(\\mathbf{R}_{\\Theta}^d\\mathbf{W}_Q\\mathbf{X})(\\mathbf{R}_{\\Theta}^d\\mathbf{W}_K\\mathbf{X})^T}{\\sqrt{d}} + \\mathbf{M}\\right)\\mathbf{W}_V\\mathbf{X} \\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf0a052-e7f5-4f9e-bc13-4bb1daa9f4f8",
   "metadata": {},
   "source": [
    "### Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47779515-e504-4305-a720-7ce88414c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention normalization\n",
    "normalize_attention = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(normalize_attention, \"normalize_attention\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd76a2-2f04-4112-bfae-712ffd569f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize attention inputs\n",
    "residual = x\n",
    "x = normalize_attention(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3ea58-8949-4ca2-8523-9cad12c96df2",
   "metadata": {},
   "source": [
    "### Project Queries, Keys, Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e46704-eacb-4b41-88ed-7ef1a14476c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure query, key, value projections\n",
    "w_q = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "w_k = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "w_v = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(w_q, \"w_q\", w_k, \"w_k\", w_v, \"w_v\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c642ff4-24c7-4a60-8ef2-07b8d33f5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project embeddings to query, key, value spaces\n",
    "q = w_q(x)\n",
    "k = w_k(x)\n",
    "v = w_v(x)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d8ffa9-17c2-4047-82cd-18eacab5394f",
   "metadata": {},
   "source": [
    "### Split Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb9b443-f315-4f0c-a875-c0d0c362826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, n_heads):\n",
    "    return x.view(-1, n_heads, config.d_head).transpose(-3, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ec2e24-42bb-4a85-a709-19cdb4233406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split attention heads\n",
    "q = split_heads(q, config.n_heads)\n",
    "k = split_heads(k, config.n_kv_heads)\n",
    "v = split_heads(v, config.n_kv_heads)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42594212-a2f7-4d4b-a5b5-e8ab6b14c944",
   "metadata": {},
   "source": [
    "### Encode Positions (RoPE)\n",
    "\n",
    "Given $\\Theta = \\text{base}$, $m = \\text{position}$ and $d = d_{head},$ the RoPE rotation matrix $R_{\\Theta,m}^d$ can be calculated as:\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{\\Theta,m}^d \\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\vdots \\\\\n",
    "x_{d/2-2} \\\\\n",
    "x_{d/2-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "-x_1 \\\\\n",
    "x_0 \\\\\n",
    "-x_3 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "-x_{d/2-1} \\\\\n",
    "x_{d/2-2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}             \n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\theta_i = \\frac{1}{\\Theta^{2i/d}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ec0d7-6a42-4dad-a2c1-0f1ec4676e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rope_cos and rope_sin\n",
    "base = config.rope_theta\n",
    "d = config.d_head\n",
    "\n",
    "# Compute theta_i = 1 / base^(2i/d) from i = 0 to d/2-1\n",
    "thetas = 1.0 / base**(2 * torch.arange(d // 2, device=device) / d)\n",
    "\n",
    "# Compute m * theta_i for position m in 0 to n\n",
    "frequencies = torch.stack([m*thetas for m in range(n)])\n",
    "\n",
    "# Duplicate each row\n",
    "frequencies = torch.cat((frequencies, frequencies), dim=-1)\n",
    "\n",
    "# Apply cos, sin\n",
    "rope_cos = torch.cos(frequencies)\n",
    "rope_sin = torch.sin(frequencies)\n",
    "\n",
    "rope_cos.shape, rope_sin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a022a-f592-435d-b94d-7c43631d7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert rope_cos.shape[0] == n and rope_cos.shape[1] == config.d_head\n",
    "assert rope_sin.shape[0] == n and rope_sin.shape[1] == config.d_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc28db6-0f59-460c-b66c-ed2ea3e0bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode positions by rotating queries and keys\n",
    "q = (q * rope_cos) + (llama.rotate_half(q) * rope_sin)\n",
    "k = (k * rope_cos) + (llama.rotate_half(k) * rope_sin)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8041b6-2ed8-4a65-a5ba-50197825b999",
   "metadata": {},
   "source": [
    "### Expand Key / Value Groups (GQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b29445-bc16-494b-89c9-5929ded41f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand key/value groups\n",
    "k = k.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "v = v.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b874703a-ec5b-4f38-80be-88700df32bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert q.shape == k.shape == v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159ff761-2ab1-4821-bb3f-f91e800192e1",
   "metadata": {},
   "source": [
    "### Calculate Attention\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{A} &= softmax\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}} + \\mathbf{M}\\right)\\mathbf{V} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e988fc-8d2a-46ed-9773-e804cfe81550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute masked attention bias M\n",
    "mask = torch.ones(n, n, dtype=torch.bool, device=device).tril(diagonal=0)\n",
    "m = torch.zeros(n, n, device=device).masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf97d3b6-d4bf-4c13-9be4-19da0ce68ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attention for all heads in parallel\n",
    "a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head) + m, dim=-1) @ v\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f412ca-7ec0-46cb-9247-a7eaca436977",
   "metadata": {},
   "source": [
    "### Recombine Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9548f0d8-6d7a-4e11-8b4e-7f75d272d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_heads(x):\n",
    "    return x.transpose(-3, -2).contiguous().view(-1, int(config.n_heads * config.d_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7888fe2-6ed7-4d81-8976-6dc00e6f3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine attention heads\n",
    "a = combine_heads(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6816855e-1668-455a-97d2-8be5380b0d7c",
   "metadata": {},
   "source": [
    "### Project Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b181dadd-b7a5-48a7-9a43-eb66147cf868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention output projection\n",
    "attention_outputs = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(attention_outputs, \"attention_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d756b7b-74bb-4c8f-8fb9-b9248f92bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project attention embeddings back to model space\n",
    "a = attention_outputs(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6cd7a-a64b-4cc1-9758-a3a82c691efb",
   "metadata": {},
   "source": [
    "### Combine w/ Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183bdc1e-4146-4090-97f5-be88fb8f7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine attention embeddings with residuals\n",
    "x = residual + a\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c6b692-622a-4cce-b072-8b8a369caead",
   "metadata": {},
   "source": [
    "## FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499745f-670d-4033-840d-0c8b488857aa",
   "metadata": {},
   "source": [
    "### Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060079cb-e013-428d-ba9f-5a28a1cc8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FFN normalization\n",
    "normalize_ffn = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained state\n",
    "llama.load_state(normalize_ffn, \"normalize_ffn\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15d4fc-6580-4e05-aa71-7fcecdabdc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize FFN inputs\n",
    "residual = x\n",
    "x = normalize_ffn(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab457a1-b743-40ce-95ea-657c47e69634",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33c9006-5a46-41d7-a573-2b273484c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SwiGLU FFN\n",
    "ffn_gates = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_ffn,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "ffn_inputs = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_ffn,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(ffn_gates, \"ffn_gates\", ffn_inputs, \"ffn_inputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00475212-a7cb-4df0-a3f6-5fac1169c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply FFN\n",
    "f = silu(ffn_gates(x)) * ffn_inputs(x)\n",
    "\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6439772-8b11-4ca5-8722-55936fff7c29",
   "metadata": {},
   "source": [
    "### Project Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef137791-14fe-4d6e-a38e-4479bb5b0a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FFN output projection\n",
    "ffn_outputs = nn.Linear(\n",
    "    in_features=config.d_ffn,\n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(ffn_outputs, \"ffn_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443f14c-437b-4732-bca1-a6c770735188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project FFN embeddings back to model space\n",
    "f = ffn_outputs(f)\n",
    "\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b8ed4-2c98-44b7-ba87-ef711a784480",
   "metadata": {},
   "source": [
    "### Combine w/ Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dfc612-6aea-4c48-af9a-adc8514ccf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine FFN embeddings with residuals\n",
    "x = residual + f\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc7cb2-6783-481b-a90c-cd9900eb3c66",
   "metadata": {},
   "source": [
    "## Stacking the Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646bb905-65a2-434e-b5aa-3399c069601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start over from initial token embeddings\n",
    "x = embeddings(token_values)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed62fcc-eab1-41d3-8a63-eaab1772ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply layer logic in a loop\n",
    "for layer in range(config.n_layers):\n",
    "\n",
    "    # Load pre-trained state for layer\n",
    "    load_pretrained_state(layer)\n",
    "\n",
    "    #\n",
    "    # Attention\n",
    "    #\n",
    "\n",
    "    # Normalize attention inputs\n",
    "    residual = x\n",
    "    x = normalize_attention(x)\n",
    "    \n",
    "    # Project embeddings to query, key, value spaces\n",
    "    q = w_q(x)\n",
    "    k = w_k(x)\n",
    "    v = w_v(x)\n",
    "    \n",
    "    # Split attention heads\n",
    "    q = split_heads(q, config.n_heads)\n",
    "    k = split_heads(k, config.n_kv_heads)\n",
    "    v = split_heads(v, config.n_kv_heads)\n",
    "\n",
    "    # Encode positions by rotating queries and keys\n",
    "    q = (q * rope_cos) + (llama.rotate_half(q) * rope_sin)\n",
    "    k = (k * rope_cos) + (llama.rotate_half(k) * rope_sin)\n",
    "    \n",
    "    # Expand key/value groups\n",
    "    k = k.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "    v = v.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "\n",
    "    # Compute masked attention bias M\n",
    "    mask = torch.ones(n, n, dtype=torch.bool, device=device).tril(diagonal=0)\n",
    "    m = torch.zeros(n, n, device=device).masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "    \n",
    "    # Compute attention for all heads in parallel\n",
    "    a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head) + m, dim=-1) @ v\n",
    "\n",
    "    # Combine attention heads\n",
    "    a = combine_heads(a)\n",
    "    \n",
    "    # Project attention embeddings back to model space\n",
    "    a = attention_outputs(a)\n",
    "    \n",
    "    # Combine attention embeddings with residuals\n",
    "    x = residual + a\n",
    "    \n",
    "    #\n",
    "    # FFN\n",
    "    #\n",
    "\n",
    "    # Normalize FFN inputs\n",
    "    residual = x\n",
    "    x = normalize_ffn(x)\n",
    "\n",
    "    # Apply FFN\n",
    "    f = silu(ffn_gates(x)) * ffn_inputs(x)\n",
    "\n",
    "    # Project FFN embeddings back to model space\n",
    "    f = ffn_outputs(f)\n",
    "    \n",
    "    # Combine FFN embeddings with residuals\n",
    "    x = residual + f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b23955-575c-4a2e-87e1-850de8fad517",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d674f842-ac9b-4802-b6b2-b568dbcb5879",
   "metadata": {},
   "source": [
    "## Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae2c9c-ac17-42c2-9347-b23e9616881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure head normalization\n",
    "normalize_head = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(normalize_head, \"normalize_head\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897aea10-ebdc-4cf1-ad5a-4c7e749bed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize head inputs\n",
    "x = normalize_head(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e845ea4-31de-4c38-91f4-45292ede9b35",
   "metadata": {},
   "source": [
    "## Predict Next Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee30f96b-038b-4725-a926-dc1e0eb41c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure output projection\n",
    "head_outputs = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.vocab_size,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(head_outputs, \"head_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d9464-0220-47e7-ba15-c7ffb03d1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use last embedding to represent the entire sequence\n",
    "features = x[-1]\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3472ba-06fc-4788-936a-9e15b2822f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict next token\n",
    "logits = head_outputs(features)\n",
    "token_id = logits.argmax()\n",
    "\n",
    "tokenizer.decode([token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6220419-b6f8-4222-9706-4bbdaa8ff1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the answer is Boston\n",
    "assert token_id == 10406"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744fc6f-4207-4d69-b880-e0019760d48b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99464618-2541-408b-b4d3-753478d54a15",
   "metadata": {},
   "source": [
    "# RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87686e-1614-4e46-87a1-0b1a7c2e139f",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\mathbf{R}_{\\Theta,m}^d \\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\vdots \\\\\n",
    "x_{d/2-2} \\\\\n",
    "x_{d/2-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "-x_1 \\\\\n",
    "x_0 \\\\\n",
    "-x_3 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "-x_{d/2-1} \\\\\n",
    "x_{d/2-2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Rotation\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "cos(\\theta) & -sin(\\theta) \\\\\n",
    "sin(\\theta) & cos(\\theta) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7848aabd-0107-409c-8f0f-b15956dd3f11",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2019919-73e7-4318-baf7-a7367b553b99",
   "metadata": {},
   "source": [
    "Given a 2D vector\n",
    "\n",
    "$$\n",
    "x = \n",
    "\\begin{bmatrix}\n",
    "4.9 \\\\\n",
    "0.9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and rotation\n",
    "\n",
    "$$\n",
    "R = \n",
    "\\begin{bmatrix}\n",
    "cos(\\theta) & -sin(\\theta) \\\\\n",
    "sin(\\theta) & cos(\\theta) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "let's calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151bc785-b733-4c77-9e8c-ab832395ae84",
   "metadata": {},
   "source": [
    "$$\n",
    "Rx = \n",
    "\\begin{bmatrix}\n",
    "4.9cos(\\theta) - 0.9sin(\\theta) \\\\\n",
    "4.9sin(\\theta) + 0.9cos(\\theta)\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a116047e-2878-4d99-bf33-7a4ceeb9576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "822e157b-503c-4917-91fe-6f454d1d6edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.9000, 0.9000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([4.9, 0.9])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be508b70-a0b3-4b9a-895a-0b9f974e4016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7071, -0.7071],\n",
       "        [ 0.7071,  0.7071]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = math.pi / 4\n",
    "R = torch.tensor([\n",
    "    [math.cos(theta), -math.sin(theta)],\n",
    "    [math.sin(theta), math.cos(theta)],\n",
    "])\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27315546-4e7e-40fa-a0a3-abaa78c6bd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8284, 4.1012])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R@x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bafc7a-ec13-4450-8e5d-7ea464eaae5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59236335-16b4-43b5-86ce-56e490e067ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.0, 5.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGdCAYAAAAvwBgXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAkPklEQVR4nO3deXTU9b3/8ddM9oRkIGEzJqxq6dUqLQharWKlYqv2ggioiEK9VO8vUCg/j0LbK22vv8ZWbrXFjV8XVBahgoBbrVQLaqstFW2VNlhUSEwEQgIzIcskmfneP74QiEBIgJn3LM/HOTnnm2Em8+ZMMc9+t4/HcRxHAAAABrzWAwAAgORFiAAAADOECAAAMEOIAAAAM4QIAAAwQ4gAAAAzhAgAADBDiAAAADOp1gN0JBwOq6qqSrm5ufJ4PNbjAACATnAcR3V1dSosLJTX2/E+j5gOkaqqKhUXF1uPAQAATkBFRYWKioo6fE5Mh0hubq4k9y+Sl5dnPA0AAOiMQCCg4uLitt/jHYnpEDl4OCYvL48QAQAgznTmtApOVgUAAGYIEQAAYIYQAQAAZggRAABghhABAABmCBEAAGCGEAEAAGYIEQAAYIYQAQAAZggRAABghhABAABmCBEAAGCGEAEAAGYIEQAAYIYQAQAAZggRAABghhABAABmCBEAAGCGEAEAAGYIEQAAYIYQAQAAZggRAABghhABAABmCBEAAGCGEAEAAGYIEQAAYIYQAQAAZggRAABghhABAMQ+x5H+9XvrKRABUQuRe++9Vx6PR7Nnz47WWwIAEoHjSC99T1o2XnrlHutpcIpFJUQ2bdqkRYsW6dxzz43G2wEAEslr/yO98aC73WOg7Sw45SIeIvv379fkyZP1i1/8Qj169Ij02wEAEsmmX0mv/Le7PeZH0ucn286DUy7iIVJSUqKrrrpKo0ePjvRbAQASyXurpef/r7v9pTukC0ts50FEpEbyh69YsUKbN2/Wpk2bOvX8YDCoYDDY9n0gEIjUaACAWLbt99LTt0lypOHfkL78PeuJECER2yNSUVGhWbNmadmyZcrMzOzUa0pLS+Xz+dq+iouLIzUeACBWlf9ZWjlFCrdIZ18rfW2B5PFYT4UI8TiO40TiB69du1bjxo1TSkpK22OhUEgej0der1fBYLDdn0lH3yNSXFwsv9+vvLy8SIwJAIglu7ZIi78qNfmlM0ZL1z8ppaZbT4UuCgQC8vl8nfr9HbFDM5dffrnefffddo9NmzZNQ4YM0V133XVEhEhSRkaGMjIyIjUSACCW1X4kLRnnRkjxSGniE0RIEohYiOTm5uqcc85p91hOTo4KCgqOeBwAkOTqdkpLxkr7d0m9z5ZuXCml51hPhSjgzqoAAFuNe909IXu3Sz0GSFOelrK43UOyiOhVM5+2YcOGaL4dACDWNddLyyZKu/8hdesrTVkr5fa1ngpRxB4RAICN1mb36piP/yJl+tw9IfncOTXZECIAgOgLh6Q1t0kfvCylZUuTV0l9zraeCgYIEQBAdDmO9MId0panJW+aNGmJVDzCeioYIUQAANH1yj3SX38tySNdu8i9XwiSFiECAIiePz0ovbbA3b76p9I5423ngTlCBAAQHW8vlV76rrt9+d3uGjJIeoQIACDy/vmc9MxMd/vCGdLFc2znQcwgRAAAkfXRq9KqaZITlobeJF1xD4vYoQ0hAgCInMrN0pM3SKFmacjV0jU/I0LQDiECAIiM6velZddJzfulgZdI438lpUT1ht6IA4QIAODU21fhLmLXUCMVfl66frmUlmk9FWIQIQIAOLX2V7sREqiUep4lTV4tZeRaT4UYRYgAAE6dpoC0bLxUs03yFUtT1kg5BdZTIYYRIgCAU6Ol0T0x9ZO/Sdk93ZV0fUXWUyHGESIAgJMXapVWfUPa8bqUnivdtFrqeYb1VIgDhAgA4OSEw9IzM6StL0ipmdKNK6TCodZTIU4QIgCAE+c47m3b//ak5EmRJjwmDbjYeirEEUIEAHDiXl0gvfmwuz32YekzX7WdB3GHEAEAnJhNv5T+cI+7feW90nnX286DuESIAAC67t1V0vN3uNuX3iVd8J+28yBuESIAgK55/yVpzW2SHGnEN6VR86wnQhwjRAAAnbfjDek3N0vhVulzE6Qrf8widjgphAgAoHN2vistnyS1NkpnXiGNfUTy8msEJ4f/BQEAjq/mA2nJtVLQL/W7UJrwuJSSZj0VEgAhAgDoWOATdxG7+t1Sn89JN6yQ0rOtp0KCIEQAAMfWUCstGSftK5d6DHRv3Z7V3XoqJBBCBABwdMH90vKJUvU/pdzTpJvXSrl9rKdCgiFEAABHag1KK2+SPt4kZfWQpqyRegywngoJiBABALQXDklPf1P68A9SWo40eZXU+7PWUyFBESIAgEMcR3ru29I/1kreNOn6pVLRcOupkMAIEQDAIS//QNr8uOTxSuN/KQ3+svVESHCECADA9cefS6/f725ffb909ljTcZAcCBEAgLR5ibT+v9zt0d+Xhk21nAZJhBABgGT3j2ekZ7/lbn/xW9LF37adB0mFEAGAZPbhBmn1rZITlj4/RfrKD60nQpIhRAAgWX38lvTkjVKoWfrs16VrfsZKuog6QgQAktHuMmnZeKmlXho0yr1CxptiPRWSECECAMlmX7m7fkzjXun0YdKkZVJqhvVUSFKECAAkk/27pSfGSnVVUq8h7l1TM7pZT4UkRogAQLJo8ktLr5VqP5B8/dz1Y7LzradCkiNEACAZtDRKy6+Xdr4r5fRyV9LNK7SeCiBEACDhhVqkp6ZK5X+SMvKkm56WCgZbTwVIIkQAILGFw9K6Eun9F6XUTOnGldJp51pPBbQhRAAgUTmO9OJc6e8rJW+qNPEJqf8XracC2iFEACBRbfyx9JdF7vbYR6SzxtjOAxwFIQIAiejPi6QNpe72V++Tzp1oOw9wDIQIACSav/9G+u2d7vaoedLIb9rOA3SAEAGARPL+76Q1t7vbI26TLr3Ldh7gOAgRAEgUO/4k/eZmyQlJ506SrryXRewQ8wgRAEgEn/xNWj5Jam2Szvqq9O8PSV7+E4/Yx/9KASDe1XwgLR0vBQNS/4ukCYullDTrqYBOIUQAIJ75K91F7Oqrpb7nSjc8KaVlWU8FdBohAgDxqqHWXcTOXy7lD3Zv3Z7ps54K6BJCBADiUbBOWnadVF0m5Ra6i9h162U9FdBlhAgAxJvWoLRislT5lpTVQ5qyRurez3oq4IQQIgAQT8IhafV/SB9tlNJypMmrpd5DrKcCThghAgDxwnGkZ2dJ/3xGSkmXblguFQ2zngo4KYQIAMSL38+X3l4iebzSdb+WBo2yngg4aYQIAMSD1++X/vgzd/uan0ufvcZ2HuAUIUQAINa99Zj0+++721/5b+kLUyynAU4pQgQAYtmWtdJz33a3L/62dNG3TMcBTjVCBABi1QevuFfIOGFp2FTp8vnWEwGnHCECALHo479KK26Swi3Sv42VrvopK+kiIUU0REpLS3X++ecrNzdXvXv31tixY7V169ZIviUAxL/d/3QXsWuplwZ/Wbr2/0veFOupgIiIaIhs3LhRJSUlevPNN7V+/Xq1tLToiiuuUH19fSTfFgDi197t0pJxUtM+qeh8adJSKTXDeiogYjyO4zjRerPq6mr17t1bGzdu1CWXXHLc5wcCAfl8Pvn9fuXl5UVhQgAwVLdL+vUYae9HUu9/k6Y+L2XnW08FdFlXfn9H9RwRv98vScrP5x8WALTTuM89HLP3I6l7f3clXSIESSA1Wm8UDoc1e/ZsXXTRRTrnnHOO+pxgMKhgMNj2fSAQiNZ4AGCnuUFaPkna9a6U09tdxC7vNOupgKiI2h6RkpISvffee1qxYsUxn1NaWiqfz9f2VVxcHK3xAMBGqEV66hap4k0pw+dGSMFg66mAqInKOSIzZszQunXr9Oqrr2rgwIHHfN7R9ogUFxdzjgiAxBQOS2u+Kb37lJSaJd28Vup3gfVUwEnryjkiET004ziOZs6cqTVr1mjDhg0dRogkZWRkKCODs8MBJAHHkX57pxsh3lRp0hIiBEkpoiFSUlKi5cuXa926dcrNzdXOnTslST6fT1lZWZF8awCIbRtKpU2/kOSRxi2SzvyK9USAiYgemvEc4y6Aixcv1tSpU4/7ei7fBZCQ3nxEenGuu33V/0jn/4ftPMApFlOHZgAAh3nnyUMRctn3iBAkPdaaAYBoKXtBWlfibl/wf6RL7rCdB4gBhAgARMP216WnpkpOSDrvRumK/8cidoAIEQCIvKp3pOXXS6Gg9JmvSV9fKHn5zy8gESIAEFl7trm3bm+ukwZ8SbpusZQStZtaAzGPEAGASPF/LC0ZKzXskU4bKl2/XErLtJ4KiCmECABEQn2NtGSc5K+QCs6UblotZXIbAuDTCBEAONWCddKy8dKe96W80931Y3J6Wk8FxCRCBABOpZYm6ckbpKq3pewCacpaqTsLeALHQogAwKkSapVW3yptf01K7yZNXiX1Ost6KiCmESIAcCo4jvTsLKnsOSklQ7rhSen0L1hPBcQ8QgQATpbjSC99T3pnqeRJkSYslgZeYj0VEBcIEQA4Wa//VHrjQXf73x+UhlxlOw8QRwgRADgZf/219PIP3e0xP5KG3mg7DxBnCBEAOFHvrZaem+Nuf+kO6cIS23mAOESIAMCJ2PZ76enbJDnS8G9IX/6e9URAXCJEAKCryv8srZwihVuks6+VvraAlXSBE0SIAEBX7NoiLZ8gtTRIZ4yWxi2SvCnWUwFxixABgM6q/chdP6bJLxWPlCY+IaWmW08FxDVCBAA6o26nu5Lu/l1S77OlG1dK6TnWUwFxjxABgONp3CstuVbau13qMUCa8rSU1cN6KiAhECIA0JHmemn5JGn3FqlbX3cRu9y+1lMBCYMQAYBjaW12r46p+LOU6XP3hOQPtJ4KSCiECAAcTTgkrblN+uBlKS3bXUm3z9nWUwEJhxABgE9zHOmFO6QtT0veNGnSEql4hPVUQEIiRADg0165x11DRh7p2kXu/UIARAQhAgCHe+Mh6bUF7vbVP5XOGW87D5DgCBEAOOid5dLvvuNuX363u4YMgIgiRABAksqel9bNcLcvnCFdPMd2HiBJECIA8NGr0lPTJCckDb1JuuIeFrEDooQQAZDcKjdLT94ghYLSkKula35GhABRRIgASF7V70vLrpOa90sDL5HG/0pKSbWeCkgqhAiA5LSvwl3ErqFGKvy8dP1yKS3Teiog6RAiAJJP/R5pyTgpUCn1PEuavFrKyLWeCkhK7IMEkFyaAtLSa6Waf0m+YncRu5wC66mAuLCvoVk7ahq0o7ZB5TX1bds7aur13MwvqVduRpd/JiECIHm0NEkrbpQ++ZuU3dONEN/p1lMBMSMcdrQz0KQdNQ0qr60/LDrc2Ag0tR7zteW19YQIABxTqFVaNU3a/pqUnivdtFrqeYb1VEDUBVtDqqhtPBQaNQ0qP7BXo2Jvo5pbwx2+vnduhvoXZKtffo76F2Qf2M7WkL55JzQPIQIg8YXD0jMzpa0vSKmZ0o0rpMKh1lMBEeNvbHH3YhyIjYPb5TUN+iTQJMc59mtTvR6d3iNL/fLdyOifn6N+BdkaUJCj4vwsZaef2nQgRAAkNseRXvqu9LflkidFmvCYNOBi66mAk+I4jnbXBQ/s0ag/sEfj0LkbextaOnx9dnrKodAoyGkXHYXdM5WaEr1rWQgRAInt1QXSmw+722Mflj7zVdt5gE5qbg2rcl9j+9A4cO5GeW2Dmlo6PoTSs1v6gcA4LDQOHFLp2S1dnhi5cR8hAiBxbfql9Id73O0r75XOu952HuBT9gdb3dBou/rk0EmiVfsaFe7gEIrXIxV2z2rbq9E//1Bo9CvIVreM+PgVHx9TAkBXvbtKev4Od/vSu6QL/tN2HiQlx3G0Z39zW1xsrzlw2euBK1Fq6ps7fH1mmlf98o88MbR/QY5O756l9NT4vx0YIQIg8fxrvbTmNkmONOKb0qh51hMhgbWGwqra13ToxNADV6Ac3G5oDnX4+h7ZaerXbo/GgT0cBdnqnZsRM4dQIoUQAZBYyt+UVk6Rwq3S5yZIV/6YRexw0hqaW9vO02h3NUptgyr3Nqq1g2MoHo9U6Dt0FUq/AyeFHtzOy0yL4t8k9hAiABLHznelZROl1kbpzCuksY9I3vjfdY3IcxxHtfXNh92869DlrjtqG1RdF+zw9empXhX3yDrqiaHF+VnKSE2J0t8k/hAiABJDzQfSkmuloF/qd6E04XEpJbn/nybaC4UdfeJvPOqJoeU1DaoLHvuuoZKUl5nqhkZBdrsTQ/sXZKtvXqa8Xva8nQhCBED8C3zirqRbv1vq8znphhVSerb1VDDQ1BJSxafuqXFwL8fHexvVHOr4kte+eZntQ+Owcze6Z6dH6W+RXAgRAPGtodZdSXdfudRjoHvr9qzu1lMhgo618Fp5TYN2Bpo6fG1aikdFPQ7evKt9aBTnZyszjUMo0UaIAIhfzfXS8olS9T+l3NOkm9dKuX2sp8JJCocd7aprandi6Paazi28JkndMlKPfmJofrYKu2cphUMoMYUQARCfWoPSypukjzdJWT2kKWukHgOsp0InBVtD+nhvY1tctJ0kWuteiXK8hdd65WYc2KPR/gqU/vnZys+JnbuG4vgIEQDxJxySnv6m9MErUlqONHmV1Puz1lPhUwJNLUdegXLgktcqf2OHC6+leD0qOsrCawf3bJzqhddgh08SQHxxHOn5OdI/1kreNOn6pVLRcOupktLJLryWlZZy2A283PM1BhgtvAY7hAiA+PLyD6W3HpM8Xmn8L6XBX7aeKKG1hMKq3Nt4xImhB8PjeAuvFeSktx0yaXf30IJs9eqW+HcNxfERIgDixx9/Lr3+U3f76vuls8eajpMo6oOt7e6pceh8jXpV7WtSqIO7hh6+8FrbeigHzt3ol5+t3CS/ayiOjxABEB82L5HW/5e7Pfr70rCpltPElU8vvHb4eijltQ3as7/jhdcyUr3tQyMBF16DHUIEQOz7xzPSs99yty+aJV38bdt5YlBrKKxP/E0HVnitb7fwWkVtg+qPs/Ba9+y0dodPDt3UK0e9czO4aygihhABENs+3CCtvlVywtIXbpZG/8B6IjONzaF2ezIOPzH0404svHZa211DD12BcnDbl8UhFNggRADErsq3pBWTpVCz9NmvS1c/kNAr6TqOo70NLe1D47BzN3Yfb+G1FK+K84++8FpRjyzuGoqYRIgAiE3VW6Wl10nN+6VBo9wrZLzx/4v0ZBdey81MbX9fjQOHT1h4DfGKEAEQe/aVS0+MlRprpdOHSZOWSakZ1lN1WlNLSB/vbTjixNAdtQ36uPb4C6/1yctoFxruYRT33I3u2Wlc8oqEQogAiC37d7sRUlcl9Rri3jU1o5v1VEfwN7Rox8E9GYedGFpe6y681tFdQw8uvNZ2I6/D9moU98hWVnr87/kBOosQARA7mvzS0mul2g8kXz93/ZjsfJNRjrbw2qHoaJC/seO7hrLwGtA5hAiA2NDSKC2/Xtr5rpTTy11JN68wom/Z3Bo+7BBK+4XXKmobFDzOwms9u2W0u4HXwRNDBxSw8BrQWYQIAHuhFumpqVL5n6SMPOmmp6WCwafkR9c1tbTbk3H4Tb0+8TeqgyteleL16PS2u4Zmt7upV7/8bOVk8J9Q4GRF/F/RQw89pPvuu087d+7Ueeedp4ULF2rEiBGRflsA8SIcltaVSO+/KKVmSjeulE47t9MvdxxH1XXBQ1egHNizcTA+aus7vmvo0RZeO7geSmH3LKWx8BoQURENkZUrV2rOnDl69NFHNXLkSD3wwAMaM2aMtm7dqt69e0fyrQHEA8eRfjdP+vtKyZsqTXxC6v/FI552rIXXyg/ERmNLx3cNzc9JP+y+Giy8BsQSj+N0dG73yRk5cqTOP/98Pfjgg5KkcDis4uJizZw5U3Pnzj3u6wOBgHw+n/x+v/Ly8iI1JgArG34sbfiRJKnpmkf1UeFVRxw+6ezCa6f5strdwOvwvRwsvAZEV1d+f0dsj0hzc7PeeustzZs3r+0xr9er0aNH64033jjqa4LBoILBQ3cODAQCkRoPgKHm1rD+uPxHuuzD+yRJP/HeqoefypP02jFfk5HqPfI8jQMnihb1yGbhNSBORSxE9uzZo1AopD59+rR7vE+fPiorKzvqa0pLS/WDHyTvOhJAskjb96Eu+WCB5JHubxmvh0OXS2LhNSAZxdQp3/PmzdOcOXPavg8EAiouLjacCEAkeHqeod+d9QMVNm7VGcO/q2d65qh/fo582RxCAZJNxEKkZ8+eSklJ0a5du9o9vmvXLvXt2/eor8nIyFBGRvzcxhnAifva5FmSpKG2YwAwFrGDqunp6Ro2bJhefvnltsfC4bBefvllXXjhhZF6WwAAEEciemhmzpw5uuWWWzR8+HCNGDFCDzzwgOrr6zVt2rRIvi0AAIgTEQ2RSZMmqbq6Wnfffbd27typoUOH6sUXXzziBFYAAJCcInofkZPFfUQAAIg/Xfn9zYX3AADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMBMREJk+/btuvXWWzVw4EBlZWVp8ODBmj9/vpqbmyPxdgAAIE6lRuKHlpWVKRwOa9GiRTrjjDP03nvvafr06aqvr9eCBQsi8ZYAACAOeRzHcaLxRvfdd58eeeQRffjhh51+TSAQkM/nk9/vV15eXgSnAwAAp0pXfn9HZI/I0fj9fuXn53f4nGAwqGAw2PZ9IBCI9FgAAMBQVE5W3bZtmxYuXKjbbrutw+eVlpbK5/O1fRUXF0djPAAAYKRLITJ37lx5PJ4Ov8rKytq9prKyUldeeaUmTJig6dOnd/jz582bJ7/f3/ZVUVHR9b8RAACIG106R6S6ulo1NTUdPmfQoEFKT0+XJFVVVWnUqFG64IIL9Nhjj8nr7doOGM4RAQAg/kTsHJFevXqpV69enXpuZWWlLrvsMg0bNkyLFy/ucoQAAIDEF5GTVSsrKzVq1Cj1799fCxYsUHV1dduf9e3bNxJvCQAA4lBEQmT9+vXatm2btm3bpqKionZ/FqWrhQEAQByIyPGSqVOnynGco34BAAAcxIkbAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMAMIQIAAMwQIgAAwAwhAgAAzBAiAADADCECAADMECIAAMBMxEMkGAxq6NCh8ng8eueddyL9dgAAII5EPETuvPNOFRYWRvptAABAHIpoiPz2t7/VSy+9pAULFkTybQAAQJxKjdQP3rVrl6ZPn661a9cqOzu7U68JBoMKBoNt3wcCgUiNBwAAYkBE9og4jqOpU6fq9ttv1/Dhwzv9utLSUvl8vrav4uLiSIwHAABiRJdCZO7cufJ4PB1+lZWVaeHChaqrq9O8efO6NMy8efPk9/vbvioqKrr0egAAEF88juM4nX1ydXW1ampqOnzOoEGDNHHiRD377LPyeDxtj4dCIaWkpGjy5Ml6/PHHO/V+gUBAPp9Pfr9feXl5nR0TAAAY6srv7y6FSGeVl5e3O7+jqqpKY8aM0apVqzRy5EgVFRV16ucQIgAAxJ+u/P6OyMmq/fr1a/d9t27dJEmDBw/udIQAAIDEx51VAQCAmYhdvnu4AQMGKAJHgAAAQJxjjwgAADBDiAAAADOECAAAMEOIAAAAM4QIAAAwQ4gAAAAzhAgAADBDiAAAADOECAAAMEOIAAAAM4QIAAAwQ4gAAAAzhAgAADBDiAAAADOECAAAMEOIAAAAM4QIAAAwQ4gAAAAzhAgAADBDiAAAADOECAAAMEOIAAAAM4QIAAAwQ4gAAAAzhAgAADBDiAAAADOECAAAMEOIAAAAM4QIAAAwk2o9QEccx5EkBQIB40kAAEBnHfy9ffD3eEdiOkTq6uokScXFxcaTAACArqqrq5PP5+vwOR6nM7liJBwOq6qqSrm5ufJ4PNbjnLBAIKDi4mJVVFQoLy/PepykxmcRO/gsYgefRexIlM/CcRzV1dWpsLBQXm/HZ4HE9B4Rr9eroqIi6zFOmby8vLj+H1Yi4bOIHXwWsYPPInYkwmdxvD0hB3GyKgAAMEOIAAAAM4RIFGRkZGj+/PnKyMiwHiXp8VnEDj6L2MFnETuS8bOI6ZNVAQBAYmOPCAAAMEOIAAAAM4QIAAAwQ4gAAAAzhIiRYDCooUOHyuPx6J133rEeJ+ls375dt956qwYOHKisrCwNHjxY8+fPV3Nzs/VoSeOhhx7SgAEDlJmZqZEjR+ovf/mL9UhJp7S0VOeff75yc3PVu3dvjR07Vlu3brUeC5LuvfdeeTwezZ4923qUiCNEjNx5550qLCy0HiNplZWVKRwOa9GiRdqyZYvuv/9+Pfroo/rOd75jPVpSWLlypebMmaP58+dr8+bNOu+88zRmzBjt3r3berSksnHjRpWUlOjNN9/U+vXr1dLSoiuuuEL19fXWoyW1TZs2adGiRTr33HOtR4kOB1H3wgsvOEOGDHG2bNniSHLefvtt65HgOM5PfvITZ+DAgdZjJIURI0Y4JSUlbd+HQiGnsLDQKS0tNZwKu3fvdiQ5GzdutB4ladXV1Tlnnnmms379eufSSy91Zs2aZT1SxLFHJMp27dql6dOna8mSJcrOzrYeB4fx+/3Kz8+3HiPhNTc366233tLo0aPbHvN6vRo9erTeeOMNw8ng9/sliX8HhkpKSnTVVVe1+/eR6GJ60btE4ziOpk6dqttvv13Dhw/X9u3brUfCAdu2bdPChQu1YMEC61ES3p49exQKhdSnT592j/fp00dlZWVGUyEcDmv27Nm66KKLdM4551iPk5RWrFihzZs3a9OmTdajRBV7RE6BuXPnyuPxdPhVVlamhQsXqq6uTvPmzbMeOWF19rM4XGVlpa688kpNmDBB06dPN5ocsFVSUqL33ntPK1assB4lKVVUVGjWrFlatmyZMjMzrceJKm7xfgpUV1erpqamw+cMGjRIEydO1LPPPiuPx9P2eCgUUkpKiiZPnqzHH3880qMmvM5+Funp6ZKkqqoqjRo1ShdccIEee+wxeb20eaQ1NzcrOztbq1at0tixY9sev+WWW7Rv3z6tW7fObrgkNWPGDK1bt06vvvqqBg4caD1OUlq7dq3GjRunlJSUtsdCoZA8Ho+8Xq+CwWC7P0skhEgUlZeXKxAItH1fVVWlMWPGaNWqVRo5cqSKiooMp0s+lZWVuuyyyzRs2DAtXbo0Yf+Rx6KRI0dqxIgRWrhwoST3sEC/fv00Y8YMzZ0713i65OE4jmbOnKk1a9Zow4YNOvPMM61HSlp1dXXasWNHu8emTZumIUOG6K677krow2WcIxJF/fr1a/d9t27dJEmDBw8mQqKssrJSo0aNUv/+/bVgwQJVV1e3/Vnfvn0NJ0sOc+bM0S233KLhw4drxIgReuCBB1RfX69p06ZZj5ZUSkpKtHz5cq1bt065ubnauXOnJMnn8ykrK8t4uuSSm5t7RGzk5OSooKAgoSNEIkSQpNavX69t27Zp27ZtR0QgOwkjb9KkSaqurtbdd9+tnTt3aujQoXrxxRePOIEVkfXII49IkkaNGtXu8cWLF2vq1KnRHwhJiUMzAADADGfmAQAAM4QIAAAwQ4gAAAAzhAgAADBDiAAAADOECAAAMEOIAAAAM4QIAAAwQ4gAAAAzhAgAADBDiAAAADOECAAAMPO/MN5UOTHQ+mEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x=[0, 4.9], y=[0, 0.9])\n",
    "sns.lineplot(x=[0, 2.83], y=[0, 4.1])\n",
    "plt.xlim([-5,5])\n",
    "plt.ylim([-5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d38dc5b-56a3-42b0-974b-3182b75841c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c8217-102c-40b6-a3f2-b00adb3fbef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c509d0-dced-4802-86c5-a1097db35ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9389ca2c-2d22-41b4-b784-c38c8fe37921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43ef18-bd08-4839-ab90-40a80124d862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81745093-7bd3-43a5-9215-b34712a509c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "740ac026-42b1-4ba3-a6fb-3eb9e1cf8bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.9000, 0.9000],\n",
       "        [4.7000, 1.7000],\n",
       "        [4.3000, 2.5000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [4.9, 0.9],\n",
    "    [4.7, 1.7],\n",
    "    [4.3, 2.5],\n",
    "])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846c1147-720c-4950-a4ec-ab1ad0b2b1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ba958-26fd-42a7-93b3-7cbb69117520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6973e44-6f59-49c5-96b2-7944f3f7fc79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fbe54c-459d-42f6-a33b-192b43a675c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6677af6-e94b-4c6d-9d55-ef7acebc2f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b4019d-0984-43d5-a997-4785d013a20e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de9117-6059-44b8-96a9-af9414af7e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f3a27-7439-472f-9d68-bdfbb32f5920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e78c1-3991-4972-89ff-89150577db77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1666fc38-ef1f-43d8-be27-b934d6d90245",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fada6d9-0e8a-46cc-bbb1-092a2076e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "\n",
    "mask = torch.full((n, n), float(\"-inf\"))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50108e9a-9480-44ae-a15d-1647695aadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.tril(diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c1a64-8a87-45a9-9527-d70ec7397867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0f1ac-818e-4369-98bb-023db7081831",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.tensor([0.2, 0.1, 0.4, 0.3], device=device)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b90feca-5a3c-482a-9656-1643d40dfa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb76f573-b269-4ebe-9308-cd2793033774",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, _ = probs.sort(descending=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcabdb1e-baf5-4d6b-ab85-c5b09001350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_probs = probs.cumsum(dim=-1)\n",
    "cumulative_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ef0c9e-9833-470c-aeed-f085067b1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "mask = cumulative_probs > p\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc3a1b-f581-44d5-99e3-a476da2cd607",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(mask).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a184ca-4c63-4aba-97f1-d342c2b20dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b465a502-b6c0-4288-a137-9e6abfdb90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import stdout\n",
    "from textwrap import dedent\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import stickshift as ss\n",
    "from stickshift.models import llama\n",
    "\n",
    "from llama_models.llama3.api.tokenizer import Tokenizer\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure gpu\n",
    "device = ss.torch.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ea76f9-3cc2-4a97-8bf5-cf59f2f5618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = llama.config(\"Meta-Llama3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28682749-e8a9-4107-b0a2-ecf2b582a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(str(config.checkpoint_path / \"tokenizer.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b623ebc-cf13-46c5-8374-b38d6d59eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
    "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
    "            220,   1627,  10263,    220,   2366,     19,    271, 128009, 128006,\n",
    "            882, 128007,    271,   3923,    374,    279,   6864,    315,  22108,\n",
    "             30, 128009, 128006,  78191, 128007,    271]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b4bb0e-4a68-4ff5-b23d-995c585d8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = [128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
    "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
    "            220,   1627,  10263,    220,   2366,     19,    271, 128009, 128006,\n",
    "            882, 128007,    271,   3923,    374,    279,   6864,    315,  22108,\n",
    "             30, 128009, 128006,  78191, 128007,    271,    791,   6864,    315,\n",
    "          22108,    374,  10406,     13, 128009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c772a85-16bd-40a2-aee8-f93ea103c857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>',\n",
       " '<|start_header_id|>',\n",
       " 'system',\n",
       " '<|end_header_id|>',\n",
       " '\\n\\n',\n",
       " 'Cut',\n",
       " 'ting',\n",
       " ' Knowledge',\n",
       " ' Date',\n",
       " ':',\n",
       " ' December',\n",
       " ' ',\n",
       " '202',\n",
       " '3',\n",
       " '\\n',\n",
       " 'Today',\n",
       " ' Date',\n",
       " ':',\n",
       " ' ',\n",
       " '26',\n",
       " ' Jul',\n",
       " ' ',\n",
       " '202',\n",
       " '4',\n",
       " '\\n\\n',\n",
       " '<|eot_id|>',\n",
       " '<|start_header_id|>',\n",
       " 'user',\n",
       " '<|end_header_id|>',\n",
       " '\\n\\n',\n",
       " 'What',\n",
       " ' is',\n",
       " ' the',\n",
       " ' capital',\n",
       " ' of',\n",
       " ' Massachusetts',\n",
       " '?',\n",
       " '<|eot_id|>',\n",
       " '<|start_header_id|>',\n",
       " 'assistant',\n",
       " '<|end_header_id|>',\n",
       " '\\n\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode([input_id]) for input_id in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c991d1a0-a938-4f30-9a67-7f993317bab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>',\n",
       " '<|start_header_id|>',\n",
       " 'system',\n",
       " '<|end_header_id|>',\n",
       " '\\n\\n',\n",
       " 'Cut',\n",
       " 'ting',\n",
       " ' Knowledge',\n",
       " ' Date',\n",
       " ':',\n",
       " ' December',\n",
       " ' ',\n",
       " '202',\n",
       " '3',\n",
       " '\\n',\n",
       " 'Today',\n",
       " ' Date',\n",
       " ':',\n",
       " ' ',\n",
       " '26',\n",
       " ' Jul',\n",
       " ' ',\n",
       " '202',\n",
       " '4',\n",
       " '\\n\\n',\n",
       " '<|eot_id|>',\n",
       " '<|start_header_id|>',\n",
       " 'user',\n",
       " '<|end_header_id|>',\n",
       " '\\n\\n',\n",
       " 'What',\n",
       " ' is',\n",
       " ' the',\n",
       " ' capital',\n",
       " ' of',\n",
       " ' Massachusetts',\n",
       " '?',\n",
       " '<|eot_id|>',\n",
       " '<|start_header_id|>',\n",
       " 'assistant',\n",
       " '<|end_header_id|>',\n",
       " '\\n\\n',\n",
       " 'The',\n",
       " ' capital',\n",
       " ' of',\n",
       " ' Massachusetts',\n",
       " ' is',\n",
       " ' Boston',\n",
       " '.',\n",
       " '<|eot_id|>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode([tid]) for tid in output_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a910422-ccb4-4cf2-b286-20b183b6dc23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b03cc-9296-4305-bfc2-fe37847ba7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498cd187-8bcd-4e7f-a765-b335432f2478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3b69e-7df8-4da3-ae5d-87099afe75bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0b8be-9504-4e69-b80f-8322266337f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a93da-cb0e-41c4-8f72-23fc0f291e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7322aaa7-c728-4f09-8b5d-ec9e5c4d69f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformer.tokenizer\n",
    "model = transformer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa282dbd-029c-4937-8675-a71adceaa176",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = dedent(\n",
    "    \"\"\"\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    What is the capital of Massachusetts?\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0326a9-10f8-415f-8c18-bbdd44f6872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(prompt)\n",
    "input_ids = torch.tensor(input_ids, device=device)\n",
    "input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac122a3-0832-4437-a749-1710b06a5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e0a6fd-71bc-4181-a0f7-f5bc320ba7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=50)\n",
    "\n",
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81fccf0-797a-40d1-aed2-a7d1f71c0176",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = [output_id.item() for output_id in output_ids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2192c3-8a04-4696-a68f-08429665c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tid in output_ids:\n",
    "    stdout.write(f\"{tid} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813529c9-ea3b-49ab-9c4e-42db48bd32c0",
   "metadata": {},
   "source": [
    "128000 198 27 91 2527 8932 851 91 29 882 27 91 408 8932 851 91 397 3923 374 279 6864 315 22108 5380 27 91 68 354 851 91 397 27 91 2527 8932 851 91 29 78191 27 91 408 8932 851 91 397 \n",
    "Selection deleted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbfdd97-3db3-4be4-8882-6a6e59000555",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tokenizer.decode([output_id]) for output_id in output_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf58f5-122b-44b0-b12f-17b1d7b5bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77152767-b5f7-4329-a975-f1d03626cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48efa69-1064-4d70-ac52-cd35f7b5c999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef900273-a7b7-496a-8f4d-8cc14e92b1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb8339-3382-4bbd-a908-1448d34c127a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8578a932-d11f-4f29-b443-4a8d24d5b3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
