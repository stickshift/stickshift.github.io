{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c2806b-4390-4662-9432-c8de5a99a8cf",
   "metadata": {},
   "source": [
    "# Transformer Teardown: Llama 3\n",
    "\n",
    "> Look mom no hands!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33316f09-93d9-449d-8108-266df48375ff",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94af2881-34c3-4c9f-b1ab-c949882329d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas import Series\n",
    "from pytest import approx\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu, silu, softmax\n",
    "\n",
    "from llama_models.llama3.api.tokenizer import Tokenizer\n",
    "from llama_models.llama3.reference_impl.model import RMSNorm\n",
    "\n",
    "import stickshift as ss\n",
    "from stickshift import default_arg, take\n",
    "from stickshift.models import llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0d45ae0-b306-4ce1-9abc-0837c5268514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure gpu\n",
    "device = ss.torch.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7462c5b-0a67-4993-94bd-5c9b6c012983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".stickshift-figure {\n",
       "    display: block;\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".stickshift-figure {\n",
    "    display: block;\n",
    "    margin-left: auto !important;\n",
    "    margin-right: auto !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb274b6-bccc-4f0c-96bd-8938e81ac9bc",
   "metadata": {},
   "source": [
    "# Text Generation with Llama 3\n",
    "\n",
    "Our goal is to build a Llama 3 text generation pipeline using the pre-trained weights and Meta's reference implementation as a guide. By the time we're done, our model should be able to correctly answer the question \"What is the capital of Massachusetts?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "658a166e-84a1-4f07-9a8c-9d1e5b3e9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the capital of Massachusetts?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c9723-5d57-4590-a2bf-26f480ae7986",
   "metadata": {},
   "source": [
    "# Load Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c55121-ac4c-481d-bb06-05b6c7d041ff",
   "metadata": {},
   "source": [
    "We'll start by loading the hyperparameters and pre-trained weights published by Meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cdbece4-bdb3-470e-9e29-3f287480a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, checkpoint = llama.load_checkpoint(\"Meta-Llama3.1-8B-Instruct\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8382f302-b2eb-4c33-ae4e-9effad73e91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checkpoint_path': PosixPath('/Users/andrewyoung/.llama/checkpoints/Meta-Llama3.1-8B-Instruct'),\n",
       " 'vocab_size': 128256,\n",
       " 'd_model': 4096,\n",
       " 'd_head': 128,\n",
       " 'd_ffn': 14336,\n",
       " 'n_layers': 32,\n",
       " 'n_heads': 32,\n",
       " 'n_kv_heads': 8,\n",
       " 'n_kv_groups': 4,\n",
       " 'rms_norm_eps': 1e-05,\n",
       " 'rope_theta': 500000.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7efce665-a4f8-4ffb-a8b3-b6b4f8c60bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_state(layer):    \n",
    "    # Load pre-trained state\n",
    "    llama.load_state(\n",
    "        normalize_attention, \"normalize_attention\", \n",
    "        normalize_ffn, \"normalize_ffn\", \n",
    "        w_q, \"w_q\", \n",
    "        w_k, \"w_k\", \n",
    "        w_v, \"w_v\", \n",
    "        attention_outputs, \"attention_outputs\",\n",
    "        ffn_gates, \"ffn_gates\",\n",
    "        ffn_inputs, \"ffn_inputs\",\n",
    "        ffn_outputs, \"ffn_outputs\",\n",
    "        checkpoint=checkpoint,\n",
    "        layer=layer,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5935777d-85fa-48a2-8f8a-af753ae5fabc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [k for k in checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360c8e1b-e56a-4a9d-97e7-364a97a6b8ee",
   "metadata": {},
   "source": [
    "# Transformer Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9765f511-7eec-4545-b687-9ef8946317c4",
   "metadata": {},
   "source": [
    "<img src=\"transformer-pipeline.svg\" class=\"stickshift-figure\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cda4ec-55fe-415c-8145-9e4c935a3d57",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "\n",
    "The Tokenize stage transforms raw data into a sequence of \"tokens\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abd8f7a9-6a6c-4616-b525-6666e5a9d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(str(config.checkpoint_path / \"tokenizer.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef25c70a-f606-4fa1-91bc-6afbae28809f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 3923, 374, 279, 6864, 315, 22108, 30]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(question, bos=True, eos=False)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f399c583-59e1-4f6a-9f9b-d8d1f8cb6894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>What is the capital of Massachusetts?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d8f9e-3461-4275-a9e7-d0ae311de0e2",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "The Embeddings stage converts tokens into \"embeddings\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cffc68c-6f61-44f5-98ae-4c7c220c71b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings lookup table\n",
    "embeddings = nn.Embedding(\n",
    "    num_embeddings=config.vocab_size, \n",
    "    embedding_dim=config.d_model,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(embeddings, \"embeddings\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c849045b-6d83-446b-8d6e-21ba816feff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load token_ids into a tensor\n",
    "token_values = torch.tensor(token_ids, device=device)\n",
    "\n",
    "token_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1262236-0212-4dac-b8b1-348f28dc9a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Record sequence length\n",
    "n = len(token_values)\n",
    "\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "088c5481-bdef-48a0-b6aa-44c7c42d66f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map token values to embeddings\n",
    "x = embeddings(token_values)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "221ab890-3d46-486d-9ecd-93b45f90c97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
       "          6.3419e-05,  1.1902e-03],\n",
       "        [ 2.0752e-02, -1.2894e-03,  2.8229e-03,  ...,  2.1973e-02,\n",
       "          3.1128e-03,  1.0681e-02],\n",
       "        [-2.6093e-03,  7.7057e-04,  2.6131e-04,  ...,  1.1902e-02,\n",
       "          4.6387e-03,  9.1553e-03],\n",
       "        ...,\n",
       "        [ 1.2817e-03,  9.1171e-04,  2.0905e-03,  ...,  1.6251e-03,\n",
       "          4.0894e-03, -4.0283e-03],\n",
       "        [ 1.2146e-02,  1.1597e-02,  1.7822e-02,  ...,  1.9684e-03,\n",
       "         -1.4771e-02, -2.5940e-03],\n",
       "        [-4.8523e-03, -1.8005e-03,  7.2937e-03,  ...,  2.3956e-03,\n",
       "         -1.3657e-03, -5.4932e-03]], device='mps:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show sample\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab343a-5835-445e-904f-1e27dbfce67a",
   "metadata": {},
   "source": [
    "# Context Layers\n",
    "\n",
    "The Context Layers in a Transformer are responsible for infusing each token embedding with contextual signals drawn from the rest of the sequence. The mechanism works by passing the token embeddings through multiple layers of attention and feedforward blocks. The attention blocks focus on relationships between tokens, augmenting each embedding with contextual information drawn from the surrounding embeddings. The feedforward blocks capitalize on the extra context, transforming each augmented embedding using a fully connected neural network. This pattern of attention and transformation is repeated over and over again, gradually converting representations of individual words into representations of abstract semantic concepts over a series of small increments.\n",
    "\n",
    "<img src=\"transformer-layers.svg\" class=\"stickshift-figure\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa13e7f7-ad24-433e-a4c7-d2549a6ff754",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca5ed83-1d59-4642-9ed8-d517c31a83eb",
   "metadata": {},
   "source": [
    "The attention block augments each token representation with additional context drawn from the surrounding tokens.\n",
    "\n",
    "<img src=\"attention.svg\" class=\"stickshift-figure\" width=\"500\">\n",
    "\n",
    "Attention starts with the token embeddings stacked in an $n \\times d_{model}$ matrix $\\mathbf{X}$. For each embedding $\\Set{\\mathbf{x}_i | \\mathbf{x}_i \\in \\mathbf{X}}$, we'll generate a new attention embedding $\\mathbf{a}_i$ using a weighted combination of all embeddings in $\\mathbf{X}$. The equation below emphasizes the fact that the weight for each embedding $\\mathbf{x}_j$ is a calculated as a function $f_{w}$ of the embedding values $\\mathbf{x}_i$, $\\mathbf{x}_j$ and their positions $i$, $j$.\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\Set{\\mathbf{a}_i | \\mathbf{a}_i = \\sum_{\\mathbf{x}_j \\in \\mathbf{X}} f_{w}(\\mathbf{x}_i, \\mathbf{x}_j, i, j) \\mathbf{x}_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3fd0fb-794f-402b-87c9-380d8481fa56",
   "metadata": {},
   "source": [
    "The attention function used by LLama 3 can be rewritten as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{A} &= softmax\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}} + \\mathbf{M}\\right)\\mathbf{V} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which expands to\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{A} &= softmax\\left(\\frac{(\\mathbf{R}_{\\Theta}^d\\mathbf{W}_Q\\mathbf{X})(\\mathbf{R}_{\\Theta}^d\\mathbf{W}_K\\mathbf{X})^T}{\\sqrt{d}} + \\mathbf{M}\\right)\\mathbf{W}_V\\mathbf{X} \\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf0a052-e7f5-4f9e-bc13-4bb1daa9f4f8",
   "metadata": {},
   "source": [
    "### Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47779515-e504-4305-a720-7ce88414c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention normalization\n",
    "normalize_attention = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(normalize_attention, \"normalize_attention\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5bd76a2-2f04-4112-bfae-712ffd569f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize attention inputs\n",
    "residual = x\n",
    "x = normalize_attention(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3ea58-8949-4ca2-8523-9cad12c96df2",
   "metadata": {},
   "source": [
    "### Project Queries, Keys, Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1e46704-eacb-4b41-88ed-7ef1a14476c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure query, key, value projections\n",
    "w_q = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "w_k = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "w_v = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.n_kv_heads * config.d_head,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(w_q, \"w_q\", w_k, \"w_k\", w_v, \"w_v\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c642ff4-24c7-4a60-8ef2-07b8d33f5da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 4096]), torch.Size([8, 1024]), torch.Size([8, 1024]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project embeddings to query, key, value spaces\n",
    "q = w_q(x)\n",
    "k = w_k(x)\n",
    "v = w_v(x)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d8ffa9-17c2-4047-82cd-18eacab5394f",
   "metadata": {},
   "source": [
    "### Split Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cb9b443-f315-4f0c-a875-c0d0c362826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, n_heads):\n",
    "    return x.view(-1, n_heads, config.d_head).transpose(-3, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47ec2e24-42bb-4a85-a709-19cdb4233406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 128]), torch.Size([8, 8, 128]), torch.Size([8, 8, 128]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split attention heads\n",
    "q = split_heads(q, config.n_heads)\n",
    "k = split_heads(k, config.n_kv_heads)\n",
    "v = split_heads(v, config.n_kv_heads)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42594212-a2f7-4d4b-a5b5-e8ab6b14c944",
   "metadata": {},
   "source": [
    "### Encode Positions (RoPE)\n",
    "\n",
    "Given $\\Theta = \\text{base}$, $m = \\text{position}$ and $d = d_{head},$ the RoPE rotation matrix $R_{\\Theta,m}^d$ can be calculated as:\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{\\Theta,m}^d \\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\vdots \\\\\n",
    "x_{d/2-2} \\\\\n",
    "x_{d/2-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_0) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "cos(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "cos(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "-x_1 \\\\\n",
    "x_0 \\\\\n",
    "-x_3 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "-x_{d/2-1} \\\\\n",
    "x_{d/2-2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_0) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "sin(m \\theta_1) \\\\\n",
    "\\vdots \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "sin(m \\theta_{d/2-1}) \\\\\n",
    "\\end{bmatrix}             \n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\theta_i = \\frac{1}{\\Theta^{2i/d}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb1ec0d7-6a42-4dad-a2c1-0f1ec4676e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 128]), torch.Size([8, 128]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute rope_cos and rope_sin\n",
    "base = config.rope_theta\n",
    "d = config.d_head\n",
    "\n",
    "# Compute theta_i = 1 / base^(2i/d) from i = 0 to d/2-1\n",
    "thetas = 1.0 / base**(2 * torch.arange(d // 2, device=device) / d)\n",
    "\n",
    "# Compute m * theta_i for position m in 0 to n\n",
    "frequencies = torch.stack([m*thetas for m in range(n)])\n",
    "\n",
    "# Duplicate each row\n",
    "frequencies = torch.cat((frequencies, frequencies), dim=-1)\n",
    "\n",
    "# Apply cos, sin\n",
    "rope_cos = torch.cos(frequencies)\n",
    "rope_sin = torch.sin(frequencies)\n",
    "\n",
    "rope_cos.shape, rope_sin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c64a022a-f592-435d-b94d-7c43631d7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert rope_cos.shape[0] == n and rope_cos.shape[1] == config.d_head\n",
    "assert rope_sin.shape[0] == n and rope_sin.shape[1] == config.d_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dc28db6-0f59-460c-b66c-ed2ea3e0bbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 128]), torch.Size([8, 8, 128]), torch.Size([8, 8, 128]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode positions by rotating queries and keys\n",
    "q = (q * rope_cos) + (llama.rotate_half(q) * rope_sin)\n",
    "k = (k * rope_cos) + (llama.rotate_half(k) * rope_sin)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8041b6-2ed8-4a65-a5ba-50197825b999",
   "metadata": {},
   "source": [
    "### Expand Key / Value Groups (GQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3b29445-bc16-494b-89c9-5929ded41f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 128]), torch.Size([32, 8, 128]), torch.Size([32, 8, 128]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand key/value groups\n",
    "k = k.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "v = v.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b874703a-ec5b-4f38-80be-88700df32bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert q.shape == k.shape == v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159ff761-2ab1-4821-bb3f-f91e800192e1",
   "metadata": {},
   "source": [
    "### Calculate Attention\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{A} &= softmax\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d}} + \\mathbf{M}\\right)\\mathbf{V} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3e988fc-8d2a-46ed-9773-e804cfe81550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]], device='mps:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute masked attention bias M\n",
    "mask = torch.ones(n, n, dtype=torch.bool, device=device).tril(diagonal=0)\n",
    "m = torch.zeros(n, n, device=device).masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf97d3b6-d4bf-4c13-9be4-19da0ce68ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute attention for all heads in parallel\n",
    "a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head) + m, dim=-1) @ v\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f412ca-7ec0-46cb-9247-a7eaca436977",
   "metadata": {},
   "source": [
    "### Recombine Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9548f0d8-6d7a-4e11-8b4e-7f75d272d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_heads(x):\n",
    "    return x.transpose(-3, -2).contiguous().view(-1, int(config.n_heads * config.d_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7888fe2-6ed7-4d81-8976-6dc00e6f3e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine attention heads\n",
    "a = combine_heads(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6816855e-1668-455a-97d2-8be5380b0d7c",
   "metadata": {},
   "source": [
    "### Project Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b181dadd-b7a5-48a7-9a43-eb66147cf868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure attention output projection\n",
    "attention_outputs = nn.Linear(\n",
    "    in_features=config.d_model, \n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(attention_outputs, \"attention_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d756b7b-74bb-4c8f-8fb9-b9248f92bd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project attention embeddings back to model space\n",
    "a = attention_outputs(a)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6cd7a-a64b-4cc1-9758-a3a82c691efb",
   "metadata": {},
   "source": [
    "### Combine w/ Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "183bdc1e-4146-4090-97f5-be88fb8f7795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine attention embeddings with residuals\n",
    "x = residual + a\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c6b692-622a-4cce-b072-8b8a369caead",
   "metadata": {},
   "source": [
    "## FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499745f-670d-4033-840d-0c8b488857aa",
   "metadata": {},
   "source": [
    "### Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "060079cb-e013-428d-ba9f-5a28a1cc8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FFN normalization\n",
    "normalize_ffn = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained state\n",
    "llama.load_state(normalize_ffn, \"normalize_ffn\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f15d4fc-6580-4e05-aa71-7fcecdabdc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize FFN inputs\n",
    "residual = x\n",
    "x = normalize_ffn(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab457a1-b743-40ce-95ea-657c47e69634",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b33c9006-5a46-41d7-a573-2b273484c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SwiGLU FFN\n",
    "ffn_gates = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_ffn,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "ffn_inputs = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.d_ffn,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "ffn_outputs = nn.Linear(\n",
    "    in_features=config.d_ffn,\n",
    "    out_features=config.d_model,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(ffn_gates, \"ffn_gates\", ffn_inputs, \"ffn_inputs\", ffn_outputs, \"ffn_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00475212-a7cb-4df0-a3f6-5fac1169c23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply FFN\n",
    "f = ffn_outputs(silu(ffn_gates(x)) * ffn_inputs(x))\n",
    "\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b8ed4-2c98-44b7-ba87-ef711a784480",
   "metadata": {},
   "source": [
    "### Combine w/ Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18dfc612-6aea-4c48-af9a-adc8514ccf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine FFN embeddings with residuals\n",
    "x = residual + f\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc7cb2-6783-481b-a90c-cd9900eb3c66",
   "metadata": {},
   "source": [
    "## Stacking the Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "646bb905-65a2-434e-b5aa-3399c069601b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start over from initial token embeddings\n",
    "x = embeddings(token_values)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ed62fcc-eab1-41d3-8a63-eaab1772ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply layer logic in a loop\n",
    "for layer in range(config.n_layers):\n",
    "\n",
    "    # Load pre-trained state for layer\n",
    "    load_pretrained_state(layer)\n",
    "\n",
    "    #\n",
    "    # Attention\n",
    "    #\n",
    "\n",
    "    # Normalize attention inputs\n",
    "    residual = x\n",
    "    x = normalize_attention(x)\n",
    "    \n",
    "    # Project embeddings to query, key, value spaces\n",
    "    q = w_q(x)\n",
    "    k = w_k(x)\n",
    "    v = w_v(x)\n",
    "    \n",
    "    # Split attention heads\n",
    "    q = split_heads(q, config.n_heads)\n",
    "    k = split_heads(k, config.n_kv_heads)\n",
    "    v = split_heads(v, config.n_kv_heads)\n",
    "\n",
    "    # Encode positions by rotating queries and keys\n",
    "    q = (q * rope_cos) + (llama.rotate_half(q) * rope_sin)\n",
    "    k = (k * rope_cos) + (llama.rotate_half(k) * rope_sin)\n",
    "    \n",
    "    # Expand key/value groups\n",
    "    k = k.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "    v = v.repeat_interleave(config.n_kv_groups, dim=0)\n",
    "    \n",
    "    # Sanity check\n",
    "    assert q.shape == k.shape == v.shape\n",
    "\n",
    "    # Compute masked attention bias M\n",
    "    mask = torch.ones(n, n, dtype=torch.bool, device=device).tril(diagonal=0)\n",
    "    m = torch.zeros(n, n, device=device).masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "    \n",
    "    # Compute attention for all heads in parallel\n",
    "    a = softmax(q @ k.transpose(-2, -1) / np.sqrt(config.d_head) + m, dim=-1) @ v\n",
    "\n",
    "    # Combine attention heads\n",
    "    a = combine_heads(a)\n",
    "    \n",
    "    # Project attention embeddings back to model space\n",
    "    a = attention_outputs(a)\n",
    "    \n",
    "    # Combine attention embeddings with residuals\n",
    "    x = residual + a\n",
    "    \n",
    "    # Normalize FFN inputs\n",
    "    residual = x\n",
    "    x = normalize_ffn(x)\n",
    "\n",
    "    # Apply FFN\n",
    "    f = ffn_outputs(silu(ffn_gates(x)) * ffn_inputs(x))\n",
    "    \n",
    "    # Combine FFN embeddings with residuals\n",
    "    x = residual + f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b23955-575c-4a2e-87e1-850de8fad517",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d674f842-ac9b-4802-b6b2-b568dbcb5879",
   "metadata": {},
   "source": [
    "## Normalize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37ae2c9c-ac17-42c2-9347-b23e9616881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure head normalization\n",
    "normalize_head = RMSNorm(config.d_model, config.rms_norm_eps).to(device)\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(normalize_head, \"normalize_head\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "897aea10-ebdc-4cf1-ad5a-4c7e749bed12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4096])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize head inputs\n",
    "x = normalize_head(x)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e845ea4-31de-4c38-91f4-45292ede9b35",
   "metadata": {},
   "source": [
    "## Predict Next Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee30f96b-038b-4725-a926-dc1e0eb41c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure output projection\n",
    "head_outputs = nn.Linear(\n",
    "    in_features=config.d_model,\n",
    "    out_features=config.vocab_size,\n",
    "    bias=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "llama.load_state(head_outputs, \"head_outputs\", checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df3d9464-0220-47e7-ba15-c7ffb03d1f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use last embedding to represent the entire sequence\n",
    "features = x[-1]\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c3472ba-06fc-4788-936a-9e15b2822f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Boston'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict next token\n",
    "logits = head_outputs(features)\n",
    "token_id = logits.argmax()\n",
    "\n",
    "tokenizer.decode([token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6220419-b6f8-4222-9706-4bbdaa8ff1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the answer is Boston\n",
    "assert token_id == 10406"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744fc6f-4207-4d69-b880-e0019760d48b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
